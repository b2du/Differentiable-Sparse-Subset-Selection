{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just get a quick sparsity overview of the methods so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_PATH_DATA = '../data/'\n",
    "BASE_PATH_DATA = '/scratch/ns3429/sparse-subset/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "lr = 0.002\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "img_size = 28\n",
    "channels = 1\n",
    "\n",
    "log_interval = 20\n",
    "\n",
    "\n",
    "z_size = 40\n",
    "\n",
    "n = 28 * 28\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/CITEseq.mat')\n",
    "data= a['G'].T\n",
    "N,d=data.shape\n",
    "#transformation from integer entries \n",
    "data=np.log(data+np.ones(data.shape))\n",
    "#for i in range(N):\n",
    "for i in range(d):\n",
    "    #data[i,:]=data[i,:]/np.linalg.norm(data[i,:])\n",
    "    data[:,i]= (data[:,i] - np.min(data[:,i])) /  (np.max(data[:,i]) - np.min(data[:, i]))\n",
    "\n",
    "#load labels from file\n",
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/CITEseq-labels.mat')\n",
    "l_aux = a['labels']\n",
    "labels = np.array([i for [i] in l_aux])\n",
    "\n",
    "#load names from file\n",
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/CITEseq_names.mat')\n",
    "names=[a['citeseq_names'][i][0][0] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(data.shape[0]))\n",
    "upto = int(.8 * len(data))\n",
    "\n",
    "train_data = data[slices[:upto]]\n",
    "test_data = data[slices[upto:]]\n",
    "\n",
    "train_data = Tensor(train_data).to(device)\n",
    "test_data = Tensor(test_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1832, device='cuda:0')\n",
      "tensor(0.1824, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.std(dim = 0).mean())\n",
    "print(test_data.std(dim = 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_per_autoencoder(x, mu_x, logvar_x, mu_latent, logvar_latent):\n",
    "    # BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    # BCE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    loss_rec = -torch.sum(\n",
    "            (-0.5 * np.log(2.0 * np.pi))\n",
    "            + (-0.5 * logvar_x)\n",
    "            + ((-0.5 / torch.exp(logvar_x)) * (x - mu_x) ** 2.0))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp())\n",
    "    #print(loss_rec.item(), KLD.item())\n",
    "    return loss_rec + 130640 * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLD of D(P_1||P_2) where P_i are Gaussians, assuming diagonal\n",
    "def kld_joint_autoencoders(mu_1, mu_2, logvar_1, logvar_2):\n",
    "    # equation 6 of Tutorial on Variational Autoencoders by Carl Doersch\n",
    "    # https://arxiv.org/pdf/1606.05908.pdf\n",
    "    mu_12 = mu_1 - mu_2\n",
    "    kld = 0.5 * (-1 - (logvar_1 - logvar_2) + mu_12.pow(2) / logvar_2.exp() + torch.exp(logvar_1 - logvar_2))\n",
    "    #print(kld.shape)\n",
    "    kld = torch.sum(kld, dim = 1)\n",
    "    \n",
    "    return kld.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for joint\n",
    "def loss_function_joint(x, ae_1, ae_2):\n",
    "    # assuming that both autoencoders return recon_x, mu, and logvar\n",
    "    # try to make ae_1 the vanilla vae\n",
    "    # ae_2 should be the L1 penalty VAE\n",
    "    mu_x_1, logvar_x_1, mu_latent_1, logvar_latent_1 = ae_1(x)\n",
    "    mu_x_2, logvar_x_2, mu_latent_2, logvar_latent_2 = ae_2(x)\n",
    "    \n",
    "    loss_vae_1 = loss_function_per_autoencoder(x, mu_x_1, logvar_x_1, mu_latent_1, logvar_latent_1)\n",
    "    loss_vae_2 = loss_function_per_autoencoder(x, mu_x_2, logvar_x_2, mu_latent_2, logvar_latent_2)\n",
    "    joint_kld_loss = kld_joint_autoencoders(mu_latent_1, mu_latent_2, logvar_latent_1, logvar_latent_1)\n",
    "    #print(\"Losses\")\n",
    "    #print(loss_vae_1)\n",
    "    #print(loss_vae_2)\n",
    "    #print(joint_kld_loss)\n",
    "    return loss_vae_1, loss_vae_2, joint_kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does L1 work if we normalize after every step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_l1_diag(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size):\n",
    "        super(VAE_l1_diag, self).__init__()\n",
    "        \n",
    "        self.diag = nn.Parameter(torch.normal(torch.zeros(input_size), \n",
    "                                 torch.ones(input_size)).to(device).requires_grad_(True))\n",
    "        \n",
    "        # self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "        self.fc5 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc3_bn = nn.BatchNorm1d(hidden_layer_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.selection_layer = torch.diag(self.diag)\n",
    "        h0 = torch.mm(x, self.selection_layer)\n",
    "        h1 = self.encoder(h0)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.leaky_relu(self.fc3_bn(self.fc3(z)))\n",
    "        mu_x = F.leaky_relu(self.fc4(h))\n",
    "        #mu_x = self.fc4(h)\n",
    "        logvar_x = self.fc5(h)\n",
    "        return mu_x, logvar_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_latent, logvar_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu_latent, logvar_latent)\n",
    "        mu_x, logvar_x = self.decode(z)\n",
    "        return mu_x, logvar_x, mu_latent, logvar_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l1(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        loss = loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent)\n",
    "        loss += 1000000 * torch.norm(model.diag, p = 1)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.diag.data /= torch.norm(model.diag.data, p = 2)\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data) / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df, model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    inds = np.arange(df.shape[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = df[batch_ind, :]\n",
    "            batch_data = batch_data.to(device)\n",
    "            mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "            test_loss += loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent).item()\n",
    "\n",
    "\n",
    "    test_loss /= len(df)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1_diag = VAE_l1_diag(500, 200, 50)\n",
    "\n",
    "model_l1_diag.to(device)\n",
    "model_l1_optimizer = torch.optim.Adam(model_l1_diag.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 6169834.000000\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 288218.906250\n",
      "====> Epoch: 1 Average loss: 344098.3992\n",
      "====> Test set loss: 8770.4407\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 288131.781250\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 287063.312500\n",
      "====> Epoch: 2 Average loss: 288326.8821\n",
      "====> Test set loss: 8220.0041\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 286978.156250\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 285950.031250\n",
      "====> Epoch: 3 Average loss: 287193.5015\n",
      "====> Test set loss: 7703.1105\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 285867.375000\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 284867.125000\n",
      "====> Epoch: 4 Average loss: 286097.8326\n",
      "====> Test set loss: 7217.9443\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 284785.968750\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 283805.968750\n",
      "====> Epoch: 5 Average loss: 285027.1850\n",
      "====> Test set loss: 6754.0759\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 283733.593750\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 282774.875000\n",
      "====> Epoch: 6 Average loss: 283983.5326\n",
      "====> Test set loss: 6311.8424\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 282703.500000\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 281772.750000\n",
      "====> Epoch: 7 Average loss: 282966.5379\n",
      "====> Test set loss: 5889.4389\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 281697.531250\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 280781.093750\n",
      "====> Epoch: 8 Average loss: 281969.5250\n",
      "====> Test set loss: 5482.3958\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 280714.437500\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 279819.625000\n",
      "====> Epoch: 9 Average loss: 280990.6224\n",
      "====> Test set loss: 5091.9603\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 279745.000000\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 278862.406250\n",
      "====> Epoch: 10 Average loss: 280033.7925\n",
      "====> Test set loss: 4719.2832\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 278802.281250\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 277943.125000\n",
      "====> Epoch: 11 Average loss: 279095.4406\n",
      "====> Test set loss: 4366.2332\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 277879.843750\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 277032.375000\n",
      "====> Epoch: 12 Average loss: 278180.1071\n",
      "====> Test set loss: 4033.1886\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 276977.062500\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 276156.250000\n",
      "====> Epoch: 13 Average loss: 277289.7865\n",
      "====> Test set loss: 3717.8804\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 276095.812500\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 275312.531250\n",
      "====> Epoch: 14 Average loss: 276426.5249\n",
      "====> Test set loss: 3418.4024\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 275242.875000\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 274454.875000\n",
      "====> Epoch: 15 Average loss: 275578.4059\n",
      "====> Test set loss: 3126.0901\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 274405.031250\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 273643.906250\n",
      "====> Epoch: 16 Average loss: 274746.1346\n",
      "====> Test set loss: 2851.4341\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 273587.843750\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 272829.500000\n",
      "====> Epoch: 17 Average loss: 273934.4222\n",
      "====> Test set loss: 2593.7711\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 272780.125000\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 272078.562500\n",
      "====> Epoch: 18 Average loss: 273146.8446\n",
      "====> Test set loss: 2353.5822\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 272002.281250\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 271302.187500\n",
      "====> Epoch: 19 Average loss: 272376.3755\n",
      "====> Test set loss: 2130.2132\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 271255.875000\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 270571.687500\n",
      "====> Epoch: 20 Average loss: 271620.9740\n",
      "====> Test set loss: 1925.1482\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 270502.593750\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 269829.687500\n",
      "====> Epoch: 21 Average loss: 270882.2249\n",
      "====> Test set loss: 1737.8005\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 269776.406250\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 269129.375000\n",
      "====> Epoch: 22 Average loss: 270165.4229\n",
      "====> Test set loss: 1566.8921\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 269088.437500\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 268433.343750\n",
      "====> Epoch: 23 Average loss: 269469.5790\n",
      "====> Test set loss: 1412.6401\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 268392.968750\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 267759.125000\n",
      "====> Epoch: 24 Average loss: 268787.6016\n",
      "====> Test set loss: 1266.9557\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 267706.531250\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 267084.531250\n",
      "====> Epoch: 25 Average loss: 268114.0566\n",
      "====> Test set loss: 1136.5832\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 267041.656250\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 266459.406250\n",
      "====> Epoch: 26 Average loss: 267457.7870\n",
      "====> Test set loss: 1023.3508\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 266384.718750\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 265815.093750\n",
      "====> Epoch: 27 Average loss: 266819.4200\n",
      "====> Test set loss: 924.5256\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 265777.343750\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 265201.000000\n",
      "====> Epoch: 28 Average loss: 266199.6350\n",
      "====> Test set loss: 839.9546\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 265161.218750\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 264611.375000\n",
      "====> Epoch: 29 Average loss: 265591.4948\n",
      "====> Test set loss: 767.2443\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 264558.562500\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 264019.875000\n",
      "====> Epoch: 30 Average loss: 264996.1900\n",
      "====> Test set loss: 705.8836\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 263963.593750\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 263442.000000\n",
      "====> Epoch: 31 Average loss: 264414.3461\n",
      "====> Test set loss: 652.9044\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 263401.875000\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 262882.343750\n",
      "====> Epoch: 32 Average loss: 263843.7696\n",
      "====> Test set loss: 608.1715\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 262822.500000\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 262316.406250\n",
      "====> Epoch: 33 Average loss: 263282.0641\n",
      "====> Test set loss: 570.8929\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 262278.687500\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 261784.593750\n",
      "====> Epoch: 34 Average loss: 262729.2231\n",
      "====> Test set loss: 538.6169\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 261737.484375\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 261237.546875\n",
      "====> Epoch: 35 Average loss: 262189.8819\n",
      "====> Test set loss: 512.3138\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 261196.734375\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 260708.906250\n",
      "====> Epoch: 36 Average loss: 261661.2762\n",
      "====> Test set loss: 489.8872\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 260672.765625\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 260209.859375\n",
      "====> Epoch: 37 Average loss: 261139.0019\n",
      "====> Test set loss: 471.1408\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 260161.750000\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 259675.640625\n",
      "====> Epoch: 38 Average loss: 260619.6276\n",
      "====> Test set loss: 455.1181\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 259641.734375\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 259175.671875\n",
      "====> Epoch: 39 Average loss: 260104.4138\n",
      "====> Test set loss: 442.1072\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 259126.765625\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 258676.609375\n",
      "====> Epoch: 40 Average loss: 259594.6599\n",
      "====> Test set loss: 431.0556\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 258625.765625\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 258153.859375\n",
      "====> Epoch: 41 Average loss: 259083.7750\n",
      "====> Test set loss: 421.6771\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 258111.453125\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 257637.312500\n",
      "====> Epoch: 42 Average loss: 258573.8314\n",
      "====> Test set loss: 413.8759\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 257634.203125\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 257148.484375\n",
      "====> Epoch: 43 Average loss: 258069.0077\n",
      "====> Test set loss: 406.7029\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 257110.812500\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 256644.937500\n",
      "====> Epoch: 44 Average loss: 257568.5025\n",
      "====> Test set loss: 400.4227\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 256611.359375\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 256151.515625\n",
      "====> Epoch: 45 Average loss: 257068.5648\n",
      "====> Test set loss: 395.2034\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 256112.125000\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 255656.531250\n",
      "====> Epoch: 46 Average loss: 256570.1731\n",
      "====> Test set loss: 390.7828\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 255634.734375\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 255167.796875\n",
      "====> Epoch: 47 Average loss: 256079.1039\n",
      "====> Test set loss: 386.2898\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 255129.718750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 254688.812500\n",
      "====> Epoch: 48 Average loss: 255597.7020\n",
      "====> Test set loss: 382.8043\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 254654.687500\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 254221.046875\n",
      "====> Epoch: 49 Average loss: 255124.2917\n",
      "====> Test set loss: 378.4853\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 254183.734375\n",
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 253753.828125\n",
      "====> Epoch: 50 Average loss: 254654.4989\n",
      "====> Test set loss: 375.1201\n",
      "Train Epoch: 51 [0/6893 (0%)]\tLoss: 253722.093750\n",
      "Train Epoch: 51 [6400/6893 (1%)]\tLoss: 253285.156250\n",
      "====> Epoch: 51 Average loss: 254184.2211\n",
      "====> Test set loss: 370.9341\n",
      "Train Epoch: 52 [0/6893 (0%)]\tLoss: 253249.250000\n",
      "Train Epoch: 52 [6400/6893 (1%)]\tLoss: 252805.796875\n",
      "====> Epoch: 52 Average loss: 253712.9378\n",
      "====> Test set loss: 367.9999\n",
      "Train Epoch: 53 [0/6893 (0%)]\tLoss: 252782.937500\n",
      "Train Epoch: 53 [6400/6893 (1%)]\tLoss: 252338.640625\n",
      "====> Epoch: 53 Average loss: 253238.9299\n",
      "====> Test set loss: 364.8888\n",
      "Train Epoch: 54 [0/6893 (0%)]\tLoss: 252311.312500\n",
      "Train Epoch: 54 [6400/6893 (1%)]\tLoss: 251863.203125\n",
      "====> Epoch: 54 Average loss: 252764.1577\n",
      "====> Test set loss: 361.0689\n",
      "Train Epoch: 55 [0/6893 (0%)]\tLoss: 251823.812500\n",
      "Train Epoch: 55 [6400/6893 (1%)]\tLoss: 251386.484375\n",
      "====> Epoch: 55 Average loss: 252286.8191\n",
      "====> Test set loss: 357.8482\n",
      "Train Epoch: 56 [0/6893 (0%)]\tLoss: 251346.000000\n",
      "Train Epoch: 56 [6400/6893 (1%)]\tLoss: 250911.015625\n",
      "====> Epoch: 56 Average loss: 251807.3058\n",
      "====> Test set loss: 354.1015\n",
      "Train Epoch: 57 [0/6893 (0%)]\tLoss: 250876.328125\n",
      "Train Epoch: 57 [6400/6893 (1%)]\tLoss: 250434.687500\n",
      "====> Epoch: 57 Average loss: 251329.5362\n",
      "====> Test set loss: 351.4435\n",
      "Train Epoch: 58 [0/6893 (0%)]\tLoss: 250399.718750\n",
      "Train Epoch: 58 [6400/6893 (1%)]\tLoss: 249953.796875\n",
      "====> Epoch: 58 Average loss: 250849.4201\n",
      "====> Test set loss: 347.9573\n",
      "Train Epoch: 59 [0/6893 (0%)]\tLoss: 249917.093750\n",
      "Train Epoch: 59 [6400/6893 (1%)]\tLoss: 249476.015625\n",
      "====> Epoch: 59 Average loss: 250371.3361\n",
      "====> Test set loss: 344.1052\n",
      "Train Epoch: 60 [0/6893 (0%)]\tLoss: 249440.890625\n",
      "Train Epoch: 60 [6400/6893 (1%)]\tLoss: 249000.718750\n",
      "====> Epoch: 60 Average loss: 249893.9008\n",
      "====> Test set loss: 340.8467\n",
      "Train Epoch: 61 [0/6893 (0%)]\tLoss: 248969.625000\n",
      "Train Epoch: 61 [6400/6893 (1%)]\tLoss: 248527.625000\n",
      "====> Epoch: 61 Average loss: 249416.6240\n",
      "====> Test set loss: 337.6702\n",
      "Train Epoch: 62 [0/6893 (0%)]\tLoss: 248490.953125\n",
      "Train Epoch: 62 [6400/6893 (1%)]\tLoss: 248053.468750\n",
      "====> Epoch: 62 Average loss: 248939.1013\n",
      "====> Test set loss: 335.0100\n",
      "Train Epoch: 63 [0/6893 (0%)]\tLoss: 248013.234375\n",
      "Train Epoch: 63 [6400/6893 (1%)]\tLoss: 247575.062500\n",
      "====> Epoch: 63 Average loss: 248459.9589\n",
      "====> Test set loss: 331.4795\n",
      "Train Epoch: 64 [0/6893 (0%)]\tLoss: 247533.453125\n",
      "Train Epoch: 64 [6400/6893 (1%)]\tLoss: 247097.656250\n",
      "====> Epoch: 64 Average loss: 247983.7284\n",
      "====> Test set loss: 327.2645\n",
      "Train Epoch: 65 [0/6893 (0%)]\tLoss: 247059.828125\n",
      "Train Epoch: 65 [6400/6893 (1%)]\tLoss: 246623.437500\n",
      "====> Epoch: 65 Average loss: 247509.1719\n",
      "====> Test set loss: 324.9560\n",
      "Train Epoch: 66 [0/6893 (0%)]\tLoss: 246588.640625\n",
      "Train Epoch: 66 [6400/6893 (1%)]\tLoss: 246161.406250\n",
      "====> Epoch: 66 Average loss: 247039.1850\n",
      "====> Test set loss: 321.3339\n",
      "Train Epoch: 67 [0/6893 (0%)]\tLoss: 246128.515625\n",
      "Train Epoch: 67 [6400/6893 (1%)]\tLoss: 245690.453125\n",
      "====> Epoch: 67 Average loss: 246568.7349\n",
      "====> Test set loss: 318.4562\n",
      "Train Epoch: 68 [0/6893 (0%)]\tLoss: 245656.265625\n",
      "Train Epoch: 68 [6400/6893 (1%)]\tLoss: 245217.234375\n",
      "====> Epoch: 68 Average loss: 246099.1638\n",
      "====> Test set loss: 315.2164\n",
      "Train Epoch: 69 [0/6893 (0%)]\tLoss: 245181.531250\n",
      "Train Epoch: 69 [6400/6893 (1%)]\tLoss: 244752.687500\n",
      "====> Epoch: 69 Average loss: 245629.0914\n",
      "====> Test set loss: 311.7269\n",
      "Train Epoch: 70 [0/6893 (0%)]\tLoss: 244715.781250\n",
      "Train Epoch: 70 [6400/6893 (1%)]\tLoss: 244280.781250\n",
      "====> Epoch: 70 Average loss: 245157.6885\n",
      "====> Test set loss: 308.9876\n",
      "Train Epoch: 71 [0/6893 (0%)]\tLoss: 244245.406250\n",
      "Train Epoch: 71 [6400/6893 (1%)]\tLoss: 243816.890625\n",
      "====> Epoch: 71 Average loss: 244688.6482\n",
      "====> Test set loss: 305.9142\n",
      "Train Epoch: 72 [0/6893 (0%)]\tLoss: 243782.625000\n",
      "Train Epoch: 72 [6400/6893 (1%)]\tLoss: 243355.125000\n",
      "====> Epoch: 72 Average loss: 244225.9778\n",
      "====> Test set loss: 303.2324\n",
      "Train Epoch: 73 [0/6893 (0%)]\tLoss: 243324.796875\n",
      "Train Epoch: 73 [6400/6893 (1%)]\tLoss: 242905.421875\n",
      "====> Epoch: 73 Average loss: 243767.6031\n",
      "====> Test set loss: 299.8951\n",
      "Train Epoch: 74 [0/6893 (0%)]\tLoss: 242871.078125\n",
      "Train Epoch: 74 [6400/6893 (1%)]\tLoss: 242439.437500\n",
      "====> Epoch: 74 Average loss: 243310.5665\n",
      "====> Test set loss: 297.4256\n",
      "Train Epoch: 75 [0/6893 (0%)]\tLoss: 242409.843750\n",
      "Train Epoch: 75 [6400/6893 (1%)]\tLoss: 241999.734375\n",
      "====> Epoch: 75 Average loss: 242854.3564\n",
      "====> Test set loss: 293.7394\n",
      "Train Epoch: 76 [0/6893 (0%)]\tLoss: 241958.593750\n",
      "Train Epoch: 76 [6400/6893 (1%)]\tLoss: 241549.625000\n",
      "====> Epoch: 76 Average loss: 242404.0238\n",
      "====> Test set loss: 290.7326\n",
      "Train Epoch: 77 [0/6893 (0%)]\tLoss: 241511.421875\n",
      "Train Epoch: 77 [6400/6893 (1%)]\tLoss: 241091.796875\n",
      "====> Epoch: 77 Average loss: 241951.5981\n",
      "====> Test set loss: 287.2845\n",
      "Train Epoch: 78 [0/6893 (0%)]\tLoss: 241057.859375\n",
      "Train Epoch: 78 [6400/6893 (1%)]\tLoss: 240640.453125\n",
      "====> Epoch: 78 Average loss: 241496.4629\n",
      "====> Test set loss: 284.4384\n",
      "Train Epoch: 79 [0/6893 (0%)]\tLoss: 240605.500000\n",
      "Train Epoch: 79 [6400/6893 (1%)]\tLoss: 240190.046875\n",
      "====> Epoch: 79 Average loss: 241043.0115\n",
      "====> Test set loss: 281.8856\n",
      "Train Epoch: 80 [0/6893 (0%)]\tLoss: 240152.984375\n",
      "Train Epoch: 80 [6400/6893 (1%)]\tLoss: 239734.312500\n",
      "====> Epoch: 80 Average loss: 240591.6392\n",
      "====> Test set loss: 278.4306\n",
      "Train Epoch: 81 [0/6893 (0%)]\tLoss: 239697.218750\n",
      "Train Epoch: 81 [6400/6893 (1%)]\tLoss: 239284.312500\n",
      "====> Epoch: 81 Average loss: 240139.8218\n",
      "====> Test set loss: 276.0068\n",
      "Train Epoch: 82 [0/6893 (0%)]\tLoss: 239251.281250\n",
      "Train Epoch: 82 [6400/6893 (1%)]\tLoss: 238835.843750\n",
      "====> Epoch: 82 Average loss: 239688.1455\n",
      "====> Test set loss: 271.3899\n",
      "Train Epoch: 83 [0/6893 (0%)]\tLoss: 238801.000000\n",
      "Train Epoch: 83 [6400/6893 (1%)]\tLoss: 238392.296875\n",
      "====> Epoch: 83 Average loss: 239238.0176\n",
      "====> Test set loss: 268.9835\n",
      "Train Epoch: 84 [0/6893 (0%)]\tLoss: 238350.890625\n",
      "Train Epoch: 84 [6400/6893 (1%)]\tLoss: 237935.828125\n",
      "====> Epoch: 84 Average loss: 238788.9434\n",
      "====> Test set loss: 267.2153\n",
      "Train Epoch: 85 [0/6893 (0%)]\tLoss: 237907.859375\n",
      "Train Epoch: 85 [6400/6893 (1%)]\tLoss: 237491.218750\n",
      "====> Epoch: 85 Average loss: 238337.1457\n",
      "====> Test set loss: 264.3042\n",
      "Train Epoch: 86 [0/6893 (0%)]\tLoss: 237450.515625\n",
      "Train Epoch: 86 [6400/6893 (1%)]\tLoss: 237036.203125\n",
      "====> Epoch: 86 Average loss: 237884.3224\n",
      "====> Test set loss: 261.4386\n",
      "Train Epoch: 87 [0/6893 (0%)]\tLoss: 236999.796875\n",
      "Train Epoch: 87 [6400/6893 (1%)]\tLoss: 236591.687500\n",
      "====> Epoch: 87 Average loss: 237432.9624\n",
      "====> Test set loss: 259.2067\n",
      "Train Epoch: 88 [0/6893 (0%)]\tLoss: 236555.828125\n",
      "Train Epoch: 88 [6400/6893 (1%)]\tLoss: 236142.078125\n",
      "====> Epoch: 88 Average loss: 236984.9795\n",
      "====> Test set loss: 256.8051\n",
      "Train Epoch: 89 [0/6893 (0%)]\tLoss: 236110.140625\n",
      "Train Epoch: 89 [6400/6893 (1%)]\tLoss: 235689.968750\n",
      "====> Epoch: 89 Average loss: 236536.0321\n",
      "====> Test set loss: 251.3535\n",
      "Train Epoch: 90 [0/6893 (0%)]\tLoss: 235663.375000\n",
      "Train Epoch: 90 [6400/6893 (1%)]\tLoss: 235242.734375\n",
      "====> Epoch: 90 Average loss: 236085.9338\n",
      "====> Test set loss: 250.1409\n",
      "Train Epoch: 91 [0/6893 (0%)]\tLoss: 235214.296875\n",
      "Train Epoch: 91 [6400/6893 (1%)]\tLoss: 234795.296875\n",
      "====> Epoch: 91 Average loss: 235634.8150\n",
      "====> Test set loss: 247.3902\n",
      "Train Epoch: 92 [0/6893 (0%)]\tLoss: 234759.953125\n",
      "Train Epoch: 92 [6400/6893 (1%)]\tLoss: 234344.453125\n",
      "====> Epoch: 92 Average loss: 235185.3302\n",
      "====> Test set loss: 243.7188\n",
      "Train Epoch: 93 [0/6893 (0%)]\tLoss: 234312.843750\n",
      "Train Epoch: 93 [6400/6893 (1%)]\tLoss: 233899.015625\n",
      "====> Epoch: 93 Average loss: 234737.0505\n",
      "====> Test set loss: 241.4822\n",
      "Train Epoch: 94 [0/6893 (0%)]\tLoss: 233872.656250\n",
      "Train Epoch: 94 [6400/6893 (1%)]\tLoss: 233457.078125\n",
      "====> Epoch: 94 Average loss: 234289.0486\n",
      "====> Test set loss: 239.2648\n",
      "Train Epoch: 95 [0/6893 (0%)]\tLoss: 233424.234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 95 [6400/6893 (1%)]\tLoss: 233003.468750\n",
      "====> Epoch: 95 Average loss: 233840.7484\n",
      "====> Test set loss: 236.4366\n",
      "Train Epoch: 96 [0/6893 (0%)]\tLoss: 232975.281250\n",
      "Train Epoch: 96 [6400/6893 (1%)]\tLoss: 232556.921875\n",
      "====> Epoch: 96 Average loss: 233390.7688\n",
      "====> Test set loss: 233.1021\n",
      "Train Epoch: 97 [0/6893 (0%)]\tLoss: 232528.250000\n",
      "Train Epoch: 97 [6400/6893 (1%)]\tLoss: 232111.109375\n",
      "====> Epoch: 97 Average loss: 232942.6768\n",
      "====> Test set loss: 229.8905\n",
      "Train Epoch: 98 [0/6893 (0%)]\tLoss: 232078.906250\n",
      "Train Epoch: 98 [6400/6893 (1%)]\tLoss: 231671.156250\n",
      "====> Epoch: 98 Average loss: 232501.0731\n",
      "====> Test set loss: 227.5249\n",
      "Train Epoch: 99 [0/6893 (0%)]\tLoss: 231639.375000\n",
      "Train Epoch: 99 [6400/6893 (1%)]\tLoss: 231229.718750\n",
      "====> Epoch: 99 Average loss: 232059.4801\n",
      "====> Test set loss: 223.9121\n",
      "Train Epoch: 100 [0/6893 (0%)]\tLoss: 231197.218750\n",
      "Train Epoch: 100 [6400/6893 (1%)]\tLoss: 230795.343750\n",
      "====> Epoch: 100 Average loss: 231620.9520\n",
      "====> Test set loss: 222.2917\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_l1(train_data, model_l1_diag, model_l1_optimizer, epoch)\n",
    "        test(test_data, model_l1_diag, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2,  22, 112,  17,   1,   5,  48, 274,  18,   0]),\n",
       " array([1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02,\n",
       "        1.e-01, 1.e+00, 1.e+01]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [10**(-i) for i in range(10)]\n",
    "bins.reverse()\n",
    "bins += [10]\n",
    "np.histogram(model_l1_diag.diag.abs().clone().detach().cpu().numpy(), bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = model_l1_diag(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = model_l1_diag(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5442, 0.6031, 0.4708, 0.5576, 0.3855, 0.5411, 0.5537, 0.4934, 0.5422,\n",
       "        0.5011, 0.5007, 0.3545, 0.5559, 0.3670, 0.1785, 0.5369, 0.1804, 0.4077,\n",
       "        0.4889, 0.6549, 0.4487, 0.3518, 0.3118, 0.6650, 0.3306, 0.3359, 0.5101,\n",
       "        0.3252, 0.5532, 0.1821, 0.3380, 0.4546, 0.2051, 0.5097, 0.5571, 0.3372,\n",
       "        0.3656, 0.5523, 0.3793, 0.3885, 0.4283, 0.5121, 0.5536, 0.5432, 0.3926,\n",
       "        0.3274, 0.4116, 0.3199, 0.2978, 0.4827, 0.3418, 0.5646, 0.4051, 0.1440,\n",
       "        0.3947, 0.2577, 0.4066, 0.3444, 0.3856, 0.4076, 0.3271, 0.4662, 0.1573,\n",
       "        0.4680, 0.3788, 0.3458, 0.3526, 0.4697, 0.4621, 0.4787, 0.3839, 0.3931,\n",
       "        0.3947, 0.3900, 0.5435, 0.3632, 0.3662, 0.3182, 0.1832, 0.3874, 0.4308,\n",
       "        0.0000, 0.5779, 0.3885, 0.3562, 0.0000, 0.2672, 0.4513, 0.4471, 0.1637,\n",
       "        0.4628, 0.0000, 0.1601, 0.1892, 0.4363, 0.1906, 0.3709, 0.1700, 0.0000,\n",
       "        0.0000, 0.2522, 0.0000, 0.3841, 0.2080, 0.1686, 0.3274, 0.1560, 0.0000,\n",
       "        0.0000, 0.4421, 0.0000, 0.2652, 0.4255, 0.2398, 0.0000, 0.5499, 0.0000,\n",
       "        0.1854, 0.0000, 0.1552, 0.3801, 0.2277, 0.0000, 0.2211, 0.2153, 0.1542,\n",
       "        0.4883, 0.0000, 0.3733, 0.3230, 0.3504, 0.3878, 0.2103, 0.3667, 0.2398,\n",
       "        0.2038, 0.2447, 0.3263, 0.0000, 0.0000, 0.0000, 0.4057, 0.2153, 0.2103,\n",
       "        0.2869, 0.3142, 0.0000, 0.2500, 0.3457, 0.3731, 0.0000, 0.0000, 0.2242,\n",
       "        0.1800, 0.4163, 0.0000, 0.2080, 0.1237, 0.0000, 0.1906, 0.4893, 0.3504,\n",
       "        0.0000, 0.0000, 0.0000, 0.1950, 0.0000, 0.0000, 0.2153, 0.3709, 0.0000,\n",
       "        0.0000, 0.0000, 0.2354, 0.0000, 0.0000, 0.1215, 0.4117, 0.3457, 0.0000,\n",
       "        0.0000, 0.3878, 0.4057, 0.2754, 0.4157, 0.2398, 0.0000, 0.3878, 0.0000,\n",
       "        0.1810, 0.4283, 0.2560, 0.2211, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.4057, 0.0000, 0.0000, 0.0000, 0.2626, 0.3878, 0.0000,\n",
       "        0.3297, 0.0000, 0.3801, 0.0000, 0.0000, 0.2005, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2211, 0.2242, 0.4421, 0.4283, 0.0000, 0.0000, 0.4893, 0.2789,\n",
       "        0.1791, 0.0000, 0.2560, 0.3263, 0.4421, 0.0000, 0.2354, 0.2181, 0.0000,\n",
       "        0.1920, 0.1660, 0.0000, 0.0000, 0.0000, 0.3962, 0.2242, 0.2789, 0.0000,\n",
       "        0.2626, 0.0000, 0.3962, 0.2891, 0.2354, 0.3230, 0.0000, 0.0000, 0.2891,\n",
       "        0.2500, 0.2103, 0.2354, 0.0000, 0.2398, 0.0000, 0.0000, 0.2500, 0.2702,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.3784, 0.3333, 0.3010, 0.1982, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5781, 0.2560, 0.4283, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.5579, 0.2277, 0.0000, 0.4421, 0.0000, 0.2038,\n",
       "        0.2626, 0.0000, 0.2398, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0992,\n",
       "        0.0000, 0.4163, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2153, 0.0000, 0.0000, 0.3155, 0.1966, 0.1754, 0.0000, 0.2080, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.4628, 0.0000, 0.0000, 0.2127, 0.0000,\n",
       "        0.2447, 0.3010, 0.3155, 0.0000, 0.0000, 0.3020, 0.0000, 0.0000, 0.0000,\n",
       "        0.2939, 0.0000, 0.0000, 0.2000, 0.2560, 0.3562, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2891, 0.0000, 0.0000, 0.0000,\n",
       "        0.2560, 0.2626, 0.2626, 0.0000, 0.0000, 0.2242, 0.0000, 0.0997, 0.0000,\n",
       "        0.0000, 0.0000, 0.4283, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2789, 0.0000, 0.0000, 0.0000, 0.2891, 0.0000, 0.3155, 0.4283, 0.0000,\n",
       "        0.3010, 0.0000, 0.0000, 0.0000, 0.2354, 0.0000, 0.0000, 0.0000, 0.3504,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.3010, 0.1076, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2103, 0.3801, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.4057, 0.0000, 0.2560, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2891, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.4582, 0.0000, 0.0000, 0.0000, 0.4163, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.6712, 0.2153, 0.0000, 0.2314, 0.0000, 0.2789, 0.3333,\n",
       "        0.3010, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2891, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.5646, 0.0000, 0.2891, 0.0000, 0.1440, 0.2702,\n",
       "        0.0000, 0.0000, 0.0000, 0.3333, 0.2058, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.3758, 0.0000, 0.0000, 0.2354, 0.0000, 0.0000,\n",
       "        0.0000, 0.3878, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2891, 0.0000,\n",
       "        0.0000, 0.0000, 0.3731, 0.0000, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(444, device='cuda:0')\n",
      "tensor(264, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(test_pred[0,:] != 0))\n",
    "print(torch.sum(test_data[0,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1841, device='cuda:0')\n",
      "tensor(0.1865, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try Pretrained VAE and then gumble trick with it\n",
    "\n",
    "Then try joint training VAE and Gumbel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla VAE model\n",
    "# try with gaussian decoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        #self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "        self.fc5 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc3_bn = nn.BatchNorm1d(hidden_layer_size)\n",
    "        \n",
    "        #self.decoder = nn.Sequential()\n",
    "\n",
    "    def encode(self, x):\n",
    "        #h1 = F.relu(self.fc1(x))\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):    \n",
    "        h = F.leaky_relu(self.fc3_bn(self.fc3(z)))\n",
    "        mu_x = F.leaky_relu(self.fc4(h))\n",
    "        #mu_x = self.fc4(h)\n",
    "        logvar_x = self.fc5(h)\n",
    "        return mu_x, logvar_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_latent, logvar_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu_latent, logvar_latent)\n",
    "        mu_x, logvar_x = self.decode(z)\n",
    "        return mu_x, logvar_x, mu_latent, logvar_latent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain VAE First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_vae = VAE(500, 200, 50)\n",
    "\n",
    "pretrain_vae.to(device)\n",
    "pretrain_vae_optimizer = torch.optim.Adam(pretrain_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        loss = loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 11003.015625\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 9763.967773\n",
      "====> Epoch: 1 Average loss: 10388.4728\n",
      "====> Test set loss: 9695.3521\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 9685.396484\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 8611.845703\n",
      "====> Epoch: 2 Average loss: 9100.4957\n",
      "====> Test set loss: 8506.8281\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 8492.738281\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 7503.057617\n",
      "====> Epoch: 3 Average loss: 7956.1889\n",
      "====> Test set loss: 7390.9549\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 7382.337402\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 6329.913574\n",
      "====> Epoch: 4 Average loss: 6848.4586\n",
      "====> Test set loss: 6292.4771\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 6237.245605\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 5289.105469\n",
      "====> Epoch: 5 Average loss: 5771.4324\n",
      "====> Test set loss: 5244.8189\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 5286.896973\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 4327.622070\n",
      "====> Epoch: 6 Average loss: 4764.9922\n",
      "====> Test set loss: 4284.3829\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 4485.995605\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 3430.428223\n",
      "====> Epoch: 7 Average loss: 3868.2860\n",
      "====> Test set loss: 3463.5243\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 3493.008789\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 2735.115967\n",
      "====> Epoch: 8 Average loss: 3145.8371\n",
      "====> Test set loss: 2833.0054\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 2845.357422\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 2306.441406\n",
      "====> Epoch: 9 Average loss: 2603.1683\n",
      "====> Test set loss: 2365.5667\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 2267.023682\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 2160.039551\n",
      "====> Epoch: 10 Average loss: 2200.1831\n",
      "====> Test set loss: 2016.9596\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 1960.368652\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 1736.425537\n",
      "====> Epoch: 11 Average loss: 1897.3790\n",
      "====> Test set loss: 1754.4490\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 1646.700806\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 1430.791016\n",
      "====> Epoch: 12 Average loss: 1668.9429\n",
      "====> Test set loss: 1556.9220\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 1471.307373\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 1489.268188\n",
      "====> Epoch: 13 Average loss: 1496.3345\n",
      "====> Test set loss: 1406.6956\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 1449.583374\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 1384.378540\n",
      "====> Epoch: 14 Average loss: 1364.9109\n",
      "====> Test set loss: 1292.7134\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 1218.262451\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 1429.373291\n",
      "====> Epoch: 15 Average loss: 1264.1858\n",
      "====> Test set loss: 1204.8906\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 1131.665771\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 1160.533691\n",
      "====> Epoch: 16 Average loss: 1185.6865\n",
      "====> Test set loss: 1135.9435\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 1131.636353\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 1269.100708\n",
      "====> Epoch: 17 Average loss: 1123.9912\n",
      "====> Test set loss: 1082.2981\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 947.652344\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 901.532471\n",
      "====> Epoch: 18 Average loss: 1074.6208\n",
      "====> Test set loss: 1038.5340\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 978.028870\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 1192.991455\n",
      "====> Epoch: 19 Average loss: 1033.9363\n",
      "====> Test set loss: 1001.7333\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 1038.588745\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 1034.169067\n",
      "====> Epoch: 20 Average loss: 998.8604\n",
      "====> Test set loss: 969.9279\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 971.697083\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 1012.582947\n",
      "====> Epoch: 21 Average loss: 967.9649\n",
      "====> Test set loss: 941.8027\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 833.950073\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 931.188843\n",
      "====> Epoch: 22 Average loss: 939.9787\n",
      "====> Test set loss: 916.3720\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 1041.359985\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 922.367004\n",
      "====> Epoch: 23 Average loss: 914.4160\n",
      "====> Test set loss: 892.0864\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 881.117065\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 905.742981\n",
      "====> Epoch: 24 Average loss: 890.2324\n",
      "====> Test set loss: 869.9229\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 789.638550\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 999.905518\n",
      "====> Epoch: 25 Average loss: 867.4264\n",
      "====> Test set loss: 847.9982\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 784.091492\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 924.813293\n",
      "====> Epoch: 26 Average loss: 845.7944\n",
      "====> Test set loss: 827.7969\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 745.281494\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 773.464111\n",
      "====> Epoch: 27 Average loss: 824.7995\n",
      "====> Test set loss: 808.1011\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 795.238647\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 824.197021\n",
      "====> Epoch: 28 Average loss: 804.6726\n",
      "====> Test set loss: 790.0426\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 819.121277\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 766.806152\n",
      "====> Epoch: 29 Average loss: 785.8905\n",
      "====> Test set loss: 771.4553\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 782.006836\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 832.642944\n",
      "====> Epoch: 30 Average loss: 767.2337\n",
      "====> Test set loss: 754.2663\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 736.868774\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 771.137573\n",
      "====> Epoch: 31 Average loss: 749.5601\n",
      "====> Test set loss: 737.2907\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 741.303223\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 683.795898\n",
      "====> Epoch: 32 Average loss: 732.4339\n",
      "====> Test set loss: 721.6374\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 683.040466\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 674.882202\n",
      "====> Epoch: 33 Average loss: 716.2126\n",
      "====> Test set loss: 705.9871\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 687.483704\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 743.750366\n",
      "====> Epoch: 34 Average loss: 700.3979\n",
      "====> Test set loss: 691.1895\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 677.305176\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 726.244324\n",
      "====> Epoch: 35 Average loss: 685.1202\n",
      "====> Test set loss: 676.7725\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 681.887695\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 669.828125\n",
      "====> Epoch: 36 Average loss: 670.9596\n",
      "====> Test set loss: 663.2237\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 630.727173\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 648.206787\n",
      "====> Epoch: 37 Average loss: 657.0883\n",
      "====> Test set loss: 649.9426\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 668.708801\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 673.393921\n",
      "====> Epoch: 38 Average loss: 643.4172\n",
      "====> Test set loss: 637.4230\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 579.621521\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 649.287598\n",
      "====> Epoch: 39 Average loss: 630.6634\n",
      "====> Test set loss: 624.7208\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 581.458801\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 610.694580\n",
      "====> Epoch: 40 Average loss: 617.9667\n",
      "====> Test set loss: 613.1361\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 583.965332\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 618.423096\n",
      "====> Epoch: 41 Average loss: 606.2146\n",
      "====> Test set loss: 601.9334\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 589.769165\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 569.648499\n",
      "====> Epoch: 42 Average loss: 594.5788\n",
      "====> Test set loss: 590.9272\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 577.776123\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 587.850159\n",
      "====> Epoch: 43 Average loss: 583.5397\n",
      "====> Test set loss: 580.0231\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 590.279602\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 531.009827\n",
      "====> Epoch: 44 Average loss: 572.9448\n",
      "====> Test set loss: 570.3144\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 576.924194\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 633.908813\n",
      "====> Epoch: 45 Average loss: 562.7958\n",
      "====> Test set loss: 560.0802\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 556.769897\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 547.776001\n",
      "====> Epoch: 46 Average loss: 553.2333\n",
      "====> Test set loss: 551.2185\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 562.148071\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 501.528015\n",
      "====> Epoch: 47 Average loss: 543.2181\n",
      "====> Test set loss: 541.3121\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 558.785217\n",
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 546.955322\n",
      "====> Epoch: 48 Average loss: 534.3776\n",
      "====> Test set loss: 533.3372\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 570.792480\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 518.062744\n",
      "====> Epoch: 49 Average loss: 525.3822\n",
      "====> Test set loss: 524.4897\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 503.577576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 546.936279\n",
      "====> Epoch: 50 Average loss: 516.7711\n",
      "====> Test set loss: 516.3924\n",
      "Train Epoch: 51 [0/6893 (0%)]\tLoss: 506.785828\n",
      "Train Epoch: 51 [6400/6893 (1%)]\tLoss: 507.812775\n",
      "====> Epoch: 51 Average loss: 508.2960\n",
      "====> Test set loss: 508.3170\n",
      "Train Epoch: 52 [0/6893 (0%)]\tLoss: 503.375854\n",
      "Train Epoch: 52 [6400/6893 (1%)]\tLoss: 482.785583\n",
      "====> Epoch: 52 Average loss: 500.4318\n",
      "====> Test set loss: 500.5266\n",
      "Train Epoch: 53 [0/6893 (0%)]\tLoss: 501.746094\n",
      "Train Epoch: 53 [6400/6893 (1%)]\tLoss: 483.465393\n",
      "====> Epoch: 53 Average loss: 492.7492\n",
      "====> Test set loss: 492.4154\n",
      "Train Epoch: 54 [0/6893 (0%)]\tLoss: 467.760437\n",
      "Train Epoch: 54 [6400/6893 (1%)]\tLoss: 483.972351\n",
      "====> Epoch: 54 Average loss: 485.3140\n",
      "====> Test set loss: 485.0994\n",
      "Train Epoch: 55 [0/6893 (0%)]\tLoss: 473.254120\n",
      "Train Epoch: 55 [6400/6893 (1%)]\tLoss: 477.762390\n",
      "====> Epoch: 55 Average loss: 477.9699\n",
      "====> Test set loss: 478.7003\n",
      "Train Epoch: 56 [0/6893 (0%)]\tLoss: 494.800476\n",
      "Train Epoch: 56 [6400/6893 (1%)]\tLoss: 449.481293\n",
      "====> Epoch: 56 Average loss: 471.5090\n",
      "====> Test set loss: 471.6178\n",
      "Train Epoch: 57 [0/6893 (0%)]\tLoss: 456.258362\n",
      "Train Epoch: 57 [6400/6893 (1%)]\tLoss: 468.729248\n",
      "====> Epoch: 57 Average loss: 464.4626\n",
      "====> Test set loss: 465.4716\n",
      "Train Epoch: 58 [0/6893 (0%)]\tLoss: 465.474579\n",
      "Train Epoch: 58 [6400/6893 (1%)]\tLoss: 442.231720\n",
      "====> Epoch: 58 Average loss: 458.2226\n",
      "====> Test set loss: 459.2044\n",
      "Train Epoch: 59 [0/6893 (0%)]\tLoss: 448.565704\n",
      "Train Epoch: 59 [6400/6893 (1%)]\tLoss: 448.542542\n",
      "====> Epoch: 59 Average loss: 451.5729\n",
      "====> Test set loss: 452.4968\n",
      "Train Epoch: 60 [0/6893 (0%)]\tLoss: 447.592224\n",
      "Train Epoch: 60 [6400/6893 (1%)]\tLoss: 458.340088\n",
      "====> Epoch: 60 Average loss: 445.2954\n",
      "====> Test set loss: 446.9042\n",
      "Train Epoch: 61 [0/6893 (0%)]\tLoss: 426.597656\n",
      "Train Epoch: 61 [6400/6893 (1%)]\tLoss: 439.696869\n",
      "====> Epoch: 61 Average loss: 439.3499\n",
      "====> Test set loss: 440.6193\n",
      "Train Epoch: 62 [0/6893 (0%)]\tLoss: 424.852997\n",
      "Train Epoch: 62 [6400/6893 (1%)]\tLoss: 439.310486\n",
      "====> Epoch: 62 Average loss: 434.0384\n",
      "====> Test set loss: 435.3199\n",
      "Train Epoch: 63 [0/6893 (0%)]\tLoss: 420.456299\n",
      "Train Epoch: 63 [6400/6893 (1%)]\tLoss: 416.582764\n",
      "====> Epoch: 63 Average loss: 427.9106\n",
      "====> Test set loss: 429.3920\n",
      "Train Epoch: 64 [0/6893 (0%)]\tLoss: 416.918488\n",
      "Train Epoch: 64 [6400/6893 (1%)]\tLoss: 446.203522\n",
      "====> Epoch: 64 Average loss: 422.4607\n",
      "====> Test set loss: 422.9582\n",
      "Train Epoch: 65 [0/6893 (0%)]\tLoss: 415.902405\n",
      "Train Epoch: 65 [6400/6893 (1%)]\tLoss: 408.861420\n",
      "====> Epoch: 65 Average loss: 416.6335\n",
      "====> Test set loss: 417.8257\n",
      "Train Epoch: 66 [0/6893 (0%)]\tLoss: 403.920105\n",
      "Train Epoch: 66 [6400/6893 (1%)]\tLoss: 398.260742\n",
      "====> Epoch: 66 Average loss: 411.2234\n",
      "====> Test set loss: 412.6539\n",
      "Train Epoch: 67 [0/6893 (0%)]\tLoss: 399.570129\n",
      "Train Epoch: 67 [6400/6893 (1%)]\tLoss: 402.947205\n",
      "====> Epoch: 67 Average loss: 405.9602\n",
      "====> Test set loss: 406.6818\n",
      "Train Epoch: 68 [0/6893 (0%)]\tLoss: 410.598053\n",
      "Train Epoch: 68 [6400/6893 (1%)]\tLoss: 391.539124\n",
      "====> Epoch: 68 Average loss: 400.9530\n",
      "====> Test set loss: 402.4041\n",
      "Train Epoch: 69 [0/6893 (0%)]\tLoss: 383.942719\n",
      "Train Epoch: 69 [6400/6893 (1%)]\tLoss: 389.288208\n",
      "====> Epoch: 69 Average loss: 395.9036\n",
      "====> Test set loss: 396.3354\n",
      "Train Epoch: 70 [0/6893 (0%)]\tLoss: 387.901764\n",
      "Train Epoch: 70 [6400/6893 (1%)]\tLoss: 378.674805\n",
      "====> Epoch: 70 Average loss: 390.4736\n",
      "====> Test set loss: 390.3228\n",
      "Train Epoch: 71 [0/6893 (0%)]\tLoss: 377.927887\n",
      "Train Epoch: 71 [6400/6893 (1%)]\tLoss: 403.264526\n",
      "====> Epoch: 71 Average loss: 385.7687\n",
      "====> Test set loss: 386.5910\n",
      "Train Epoch: 72 [0/6893 (0%)]\tLoss: 401.852783\n",
      "Train Epoch: 72 [6400/6893 (1%)]\tLoss: 373.593170\n",
      "====> Epoch: 72 Average loss: 380.5394\n",
      "====> Test set loss: 381.7913\n",
      "Train Epoch: 73 [0/6893 (0%)]\tLoss: 367.205200\n",
      "Train Epoch: 73 [6400/6893 (1%)]\tLoss: 370.361145\n",
      "====> Epoch: 73 Average loss: 376.0127\n",
      "====> Test set loss: 377.6762\n",
      "Train Epoch: 74 [0/6893 (0%)]\tLoss: 360.997284\n",
      "Train Epoch: 74 [6400/6893 (1%)]\tLoss: 377.608795\n",
      "====> Epoch: 74 Average loss: 371.2851\n",
      "====> Test set loss: 372.0446\n",
      "Train Epoch: 75 [0/6893 (0%)]\tLoss: 366.036469\n",
      "Train Epoch: 75 [6400/6893 (1%)]\tLoss: 359.932495\n",
      "====> Epoch: 75 Average loss: 367.0099\n",
      "====> Test set loss: 367.1405\n",
      "Train Epoch: 76 [0/6893 (0%)]\tLoss: 369.123047\n",
      "Train Epoch: 76 [6400/6893 (1%)]\tLoss: 350.238647\n",
      "====> Epoch: 76 Average loss: 362.1824\n",
      "====> Test set loss: 362.2054\n",
      "Train Epoch: 77 [0/6893 (0%)]\tLoss: 364.459869\n",
      "Train Epoch: 77 [6400/6893 (1%)]\tLoss: 349.332947\n",
      "====> Epoch: 77 Average loss: 357.6289\n",
      "====> Test set loss: 358.3899\n",
      "Train Epoch: 78 [0/6893 (0%)]\tLoss: 353.912994\n",
      "Train Epoch: 78 [6400/6893 (1%)]\tLoss: 351.090759\n",
      "====> Epoch: 78 Average loss: 353.6614\n",
      "====> Test set loss: 353.9902\n",
      "Train Epoch: 79 [0/6893 (0%)]\tLoss: 347.798126\n",
      "Train Epoch: 79 [6400/6893 (1%)]\tLoss: 351.386780\n",
      "====> Epoch: 79 Average loss: 349.1887\n",
      "====> Test set loss: 348.7129\n",
      "Train Epoch: 80 [0/6893 (0%)]\tLoss: 336.566772\n",
      "Train Epoch: 80 [6400/6893 (1%)]\tLoss: 351.789246\n",
      "====> Epoch: 80 Average loss: 344.2828\n",
      "====> Test set loss: 344.9446\n",
      "Train Epoch: 81 [0/6893 (0%)]\tLoss: 343.142212\n",
      "Train Epoch: 81 [6400/6893 (1%)]\tLoss: 342.064087\n",
      "====> Epoch: 81 Average loss: 340.8173\n",
      "====> Test set loss: 340.1538\n",
      "Train Epoch: 82 [0/6893 (0%)]\tLoss: 335.063477\n",
      "Train Epoch: 82 [6400/6893 (1%)]\tLoss: 332.539276\n",
      "====> Epoch: 82 Average loss: 335.5436\n",
      "====> Test set loss: 335.9954\n",
      "Train Epoch: 83 [0/6893 (0%)]\tLoss: 328.632538\n",
      "Train Epoch: 83 [6400/6893 (1%)]\tLoss: 337.551453\n",
      "====> Epoch: 83 Average loss: 332.5300\n",
      "====> Test set loss: 332.2826\n",
      "Train Epoch: 84 [0/6893 (0%)]\tLoss: 333.044769\n",
      "Train Epoch: 84 [6400/6893 (1%)]\tLoss: 335.129456\n",
      "====> Epoch: 84 Average loss: 327.5449\n",
      "====> Test set loss: 327.9127\n",
      "Train Epoch: 85 [0/6893 (0%)]\tLoss: 322.952209\n",
      "Train Epoch: 85 [6400/6893 (1%)]\tLoss: 312.489288\n",
      "====> Epoch: 85 Average loss: 324.0673\n",
      "====> Test set loss: 323.7271\n",
      "Train Epoch: 86 [0/6893 (0%)]\tLoss: 321.194336\n",
      "Train Epoch: 86 [6400/6893 (1%)]\tLoss: 319.969421\n",
      "====> Epoch: 86 Average loss: 320.1613\n",
      "====> Test set loss: 322.0821\n",
      "Train Epoch: 87 [0/6893 (0%)]\tLoss: 324.201294\n",
      "Train Epoch: 87 [6400/6893 (1%)]\tLoss: 312.169678\n",
      "====> Epoch: 87 Average loss: 315.9720\n",
      "====> Test set loss: 315.9599\n",
      "Train Epoch: 88 [0/6893 (0%)]\tLoss: 313.864868\n",
      "Train Epoch: 88 [6400/6893 (1%)]\tLoss: 306.045624\n",
      "====> Epoch: 88 Average loss: 311.9627\n",
      "====> Test set loss: 312.6317\n",
      "Train Epoch: 89 [0/6893 (0%)]\tLoss: 304.941467\n",
      "Train Epoch: 89 [6400/6893 (1%)]\tLoss: 295.170044\n",
      "====> Epoch: 89 Average loss: 308.1914\n",
      "====> Test set loss: 308.5840\n",
      "Train Epoch: 90 [0/6893 (0%)]\tLoss: 302.652863\n",
      "Train Epoch: 90 [6400/6893 (1%)]\tLoss: 293.830536\n",
      "====> Epoch: 90 Average loss: 304.3056\n",
      "====> Test set loss: 305.2940\n",
      "Train Epoch: 91 [0/6893 (0%)]\tLoss: 306.469971\n",
      "Train Epoch: 91 [6400/6893 (1%)]\tLoss: 298.077148\n",
      "====> Epoch: 91 Average loss: 300.9279\n",
      "====> Test set loss: 301.3610\n",
      "Train Epoch: 92 [0/6893 (0%)]\tLoss: 297.274506\n",
      "Train Epoch: 92 [6400/6893 (1%)]\tLoss: 290.790222\n",
      "====> Epoch: 92 Average loss: 296.8315\n",
      "====> Test set loss: 298.1233\n",
      "Train Epoch: 93 [0/6893 (0%)]\tLoss: 298.142242\n",
      "Train Epoch: 93 [6400/6893 (1%)]\tLoss: 289.072632\n",
      "====> Epoch: 93 Average loss: 292.9639\n",
      "====> Test set loss: 294.3479\n",
      "Train Epoch: 94 [0/6893 (0%)]\tLoss: 291.694763\n",
      "Train Epoch: 94 [6400/6893 (1%)]\tLoss: 289.955811\n",
      "====> Epoch: 94 Average loss: 290.0138\n",
      "====> Test set loss: 289.7815\n",
      "Train Epoch: 95 [0/6893 (0%)]\tLoss: 282.383575\n",
      "Train Epoch: 95 [6400/6893 (1%)]\tLoss: 290.433136\n",
      "====> Epoch: 95 Average loss: 286.1231\n",
      "====> Test set loss: 285.0983\n",
      "Train Epoch: 96 [0/6893 (0%)]\tLoss: 281.852264\n",
      "Train Epoch: 96 [6400/6893 (1%)]\tLoss: 274.724701\n",
      "====> Epoch: 96 Average loss: 282.1444\n",
      "====> Test set loss: 281.7785\n",
      "Train Epoch: 97 [0/6893 (0%)]\tLoss: 281.898621\n",
      "Train Epoch: 97 [6400/6893 (1%)]\tLoss: 281.874146\n",
      "====> Epoch: 97 Average loss: 279.1769\n",
      "====> Test set loss: 279.0620\n",
      "Train Epoch: 98 [0/6893 (0%)]\tLoss: 270.130890\n",
      "Train Epoch: 98 [6400/6893 (1%)]\tLoss: 275.695374\n",
      "====> Epoch: 98 Average loss: 275.0008\n",
      "====> Test set loss: 274.7297\n",
      "Train Epoch: 99 [0/6893 (0%)]\tLoss: 278.165802\n",
      "Train Epoch: 99 [6400/6893 (1%)]\tLoss: 273.873016\n",
      "====> Epoch: 99 Average loss: 271.7638\n",
      "====> Test set loss: 272.2176\n",
      "Train Epoch: 100 [0/6893 (0%)]\tLoss: 273.861755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 100 [6400/6893 (1%)]\tLoss: 264.254974\n",
      "====> Epoch: 100 Average loss: 267.6345\n",
      "====> Test set loss: 269.6333\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train(train_data, pretrain_vae, pretrain_vae_optimizer, epoch)\n",
    "        test(test_data, pretrain_vae, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = pretrain_vae(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = pretrain_vae(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1827, device='cuda:0')\n",
      "tensor(0.1834, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(430, device='cuda:0')\n",
      "tensor(264, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(test_pred[0,:] != 0))\n",
    "print(torch.sum(test_data[0,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pretrain_vae.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc21): Linear(in_features=200, out_features=50, bias=True)\n",
       "  (fc22): Linear(in_features=200, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=200, bias=True)\n",
       "  (fc4): Linear(in_features=200, out_features=500, bias=True)\n",
       "  (fc5): Linear(in_features=200, out_features=500, bias=True)\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gumbel with the Pre-Trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pre_trained(df, model, optimizer, epoch, pretrained_model):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        with torch.no_grad():\n",
    "            _, _, mu_latent_2, logvar_latent_2 = pretrained_model(batch_data)\n",
    "        \n",
    "        loss = loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent)\n",
    "        loss += 1000*F.mse_loss(mu_latent, mu_latent_2, reduction = 'sum')\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_keys(w):\n",
    "    # sample some gumbels\n",
    "    uniform = (1.0 - EPSILON) * torch.rand_like(w) + EPSILON\n",
    "    z = torch.log(-torch.log(uniform))\n",
    "    w = w + z\n",
    "    return w\n",
    "\n",
    "\n",
    "#equations 3 and 4 and 5\n",
    "def continuous_topk(w, k, t, separate=False):\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    khot_list = []\n",
    "    onehot_approx = torch.zeros_like(w, dtype = torch.float32)\n",
    "    for i in range(k):\n",
    "        ### conver the following into pytorch\n",
    "        #khot_mask = tf.maximum(1.0 - onehot_approx, EPSILON)\n",
    "        max_mask = 1 - onehot_approx < EPSILON\n",
    "        khot_mask = 1 - onehot_approx\n",
    "        khot_mask[max_mask] = EPSILON\n",
    "        \n",
    "        w += torch.log(khot_mask)\n",
    "        #onehot_approx = tf.nn.softmax(w / t, axis=-1)\n",
    "        onehot_approx = softmax(w/t)\n",
    "        khot_list.append(onehot_approx)\n",
    "    if separate:\n",
    "        return torch.stack(khot_list)\n",
    "    else:\n",
    "        return torch.sum(torch.stack(khot_list), dim = 0) \n",
    "\n",
    "\n",
    "def sample_subset(w, k, t=0.1):\n",
    "    '''\n",
    "    Args:\n",
    "        w (Tensor): Float Tensor of weights for each element. In gumbel mode\n",
    "            these are interpreted as log probabilities\n",
    "        k (int): number of elements in the subset sample\n",
    "        t (float): temperature of the softmax\n",
    "    '''\n",
    "    w = gumbel_keys(w)\n",
    "    return continuous_topk(w, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_Gumbel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size, k, t = 0.1):\n",
    "        super(VAE_Gumbel, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self.t = t\n",
    "        \n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size)\n",
    "        )\n",
    "        \n",
    "        #self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        #self.fcextra = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "        self.fc5 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.BatchNorm1d(hidden_layer_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc3_bn = nn.BatchNorm1d(hidden_layer_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        w = self.weight_creator(x)\n",
    "        subset_indices = sample_subset(w, self.k, self.t)\n",
    "        x = x * subset_indices\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.leaky_relu(self.fc3_bn(self.fc3(z)))\n",
    "        mu_x = F.leaky_relu(self.fc4(h))\n",
    "        #mu_x = self.fc4(h)\n",
    "        logvar_x = self.fc5(h)\n",
    "        return mu_x, logvar_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_latent, logvar_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu_latent, logvar_latent)\n",
    "        mu_x, logvar_x = self.decode(z)\n",
    "        return mu_x, logvar_x, mu_latent, logvar_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel_with_pre = VAE_Gumbel(500, 200, 50, k = 50)\n",
    "vae_gumbel_with_pre.to(device)\n",
    "vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 9236.722656\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 8699.991211\n",
      "====> Epoch: 1 Average loss: 8949.6141\n",
      "====> Test set loss: 8578.5354\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 8662.710938\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 8150.341797\n",
      "====> Epoch: 2 Average loss: 8393.2081\n",
      "====> Test set loss: 8047.3223\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 8125.241699\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 7671.884766\n",
      "====> Epoch: 3 Average loss: 7866.0143\n",
      "====> Test set loss: 7537.4988\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 7594.772949\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 7150.710938\n",
      "====> Epoch: 4 Average loss: 7361.1178\n",
      "====> Test set loss: 7047.1213\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 7114.481445\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 6651.139648\n",
      "====> Epoch: 5 Average loss: 6862.9495\n",
      "====> Test set loss: 6556.3174\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 6635.752441\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 6135.333496\n",
      "====> Epoch: 6 Average loss: 6369.3400\n",
      "====> Test set loss: 6068.1345\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 6092.244141\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 5667.382324\n",
      "====> Epoch: 7 Average loss: 5872.4476\n",
      "====> Test set loss: 5572.2798\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 5603.627441\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 5152.473633\n",
      "====> Epoch: 8 Average loss: 5367.4081\n",
      "====> Test set loss: 5069.0439\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 5095.283203\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 4653.790039\n",
      "====> Epoch: 9 Average loss: 4854.9262\n",
      "====> Test set loss: 4558.7617\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 4610.238770\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 4133.500488\n",
      "====> Epoch: 10 Average loss: 4340.4877\n",
      "====> Test set loss: 4056.6098\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 4177.552734\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 3661.549561\n",
      "====> Epoch: 11 Average loss: 3837.2307\n",
      "====> Test set loss: 3571.2511\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 3561.441406\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 3195.614258\n",
      "====> Epoch: 12 Average loss: 3360.4955\n",
      "====> Test set loss: 3115.5078\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 3095.591309\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 2754.410156\n",
      "====> Epoch: 13 Average loss: 2924.7410\n",
      "====> Test set loss: 2698.7098\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 2745.865479\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 2308.052246\n",
      "====> Epoch: 14 Average loss: 2533.8021\n",
      "====> Test set loss: 2335.2397\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 2401.018555\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 2139.729004\n",
      "====> Epoch: 15 Average loss: 2192.3057\n",
      "====> Test set loss: 2025.0420\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 1971.119507\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 1842.398193\n",
      "====> Epoch: 16 Average loss: 1896.5893\n",
      "====> Test set loss: 1762.7727\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 1751.447021\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 1559.999512\n",
      "====> Epoch: 17 Average loss: 1658.1082\n",
      "====> Test set loss: 1542.8641\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 1533.500732\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 1402.732910\n",
      "====> Epoch: 18 Average loss: 1468.2204\n",
      "====> Test set loss: 1369.9674\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 1364.693237\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 1320.590088\n",
      "====> Epoch: 19 Average loss: 1311.8092\n",
      "====> Test set loss: 1234.2280\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 1305.702026\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 1189.985352\n",
      "====> Epoch: 20 Average loss: 1189.1055\n",
      "====> Test set loss: 1123.4783\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 1162.239014\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 1086.629028\n",
      "====> Epoch: 21 Average loss: 1090.4819\n",
      "====> Test set loss: 1030.8918\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 1061.358032\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 945.610291\n",
      "====> Epoch: 22 Average loss: 1005.0789\n",
      "====> Test set loss: 960.5021\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 925.302795\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 874.072510\n",
      "====> Epoch: 23 Average loss: 934.6537\n",
      "====> Test set loss: 894.8967\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 950.562256\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 826.175598\n",
      "====> Epoch: 24 Average loss: 882.2750\n",
      "====> Test set loss: 841.4981\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 927.838074\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 753.232483\n",
      "====> Epoch: 25 Average loss: 834.9518\n",
      "====> Test set loss: 803.5092\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 849.882019\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 761.969116\n",
      "====> Epoch: 26 Average loss: 796.6424\n",
      "====> Test set loss: 766.8219\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 817.682922\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 861.953064\n",
      "====> Epoch: 27 Average loss: 762.0767\n",
      "====> Test set loss: 736.0131\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 739.775391\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 703.682373\n",
      "====> Epoch: 28 Average loss: 733.6797\n",
      "====> Test set loss: 709.4704\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 687.378784\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 678.719666\n",
      "====> Epoch: 29 Average loss: 708.6430\n",
      "====> Test set loss: 685.8862\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 753.798096\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 707.476257\n",
      "====> Epoch: 30 Average loss: 685.2955\n",
      "====> Test set loss: 665.1499\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 651.642883\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 628.127136\n",
      "====> Epoch: 31 Average loss: 665.2229\n",
      "====> Test set loss: 646.6061\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 682.194763\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 633.397522\n",
      "====> Epoch: 32 Average loss: 649.0805\n",
      "====> Test set loss: 630.6164\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 614.528137\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 594.251770\n",
      "====> Epoch: 33 Average loss: 628.3728\n",
      "====> Test set loss: 619.0193\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 594.630066\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 614.817749\n",
      "====> Epoch: 34 Average loss: 614.9543\n",
      "====> Test set loss: 598.7818\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 593.508911\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 610.974915\n",
      "====> Epoch: 35 Average loss: 598.5552\n",
      "====> Test set loss: 588.1828\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 572.738464\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 586.926575\n",
      "====> Epoch: 36 Average loss: 583.4385\n",
      "====> Test set loss: 571.8618\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 569.727478\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 530.520508\n",
      "====> Epoch: 37 Average loss: 571.9065\n",
      "====> Test set loss: 560.5513\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 514.307190\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 556.584412\n",
      "====> Epoch: 38 Average loss: 558.9586\n",
      "====> Test set loss: 547.9372\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 566.293884\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 537.658875\n",
      "====> Epoch: 39 Average loss: 545.4963\n",
      "====> Test set loss: 534.0954\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 554.309265\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 506.041077\n",
      "====> Epoch: 40 Average loss: 534.5161\n",
      "====> Test set loss: 521.6821\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 568.812744\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 502.279175\n",
      "====> Epoch: 41 Average loss: 525.1735\n",
      "====> Test set loss: 514.0536\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 499.927948\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 475.344696\n",
      "====> Epoch: 42 Average loss: 513.4614\n",
      "====> Test set loss: 504.3782\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 484.669281\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 488.507996\n",
      "====> Epoch: 43 Average loss: 502.0057\n",
      "====> Test set loss: 493.4399\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 511.081879\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 472.936127\n",
      "====> Epoch: 44 Average loss: 492.7185\n",
      "====> Test set loss: 483.7392\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 474.835419\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 492.294586\n",
      "====> Epoch: 45 Average loss: 482.6282\n",
      "====> Test set loss: 474.4162\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 478.521667\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 458.895538\n",
      "====> Epoch: 46 Average loss: 474.5129\n",
      "====> Test set loss: 467.1363\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 463.990967\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 484.750946\n",
      "====> Epoch: 47 Average loss: 466.8114\n",
      "====> Test set loss: 459.4703\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 454.735687\n",
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 438.072052\n",
      "====> Epoch: 48 Average loss: 457.2722\n",
      "====> Test set loss: 450.8339\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 445.030823\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 437.796295\n",
      "====> Epoch: 49 Average loss: 449.4658\n",
      "====> Test set loss: 443.6577\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 434.546112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 443.265930\n",
      "====> Epoch: 50 Average loss: 441.7853\n",
      "====> Test set loss: 435.8204\n",
      "Train Epoch: 51 [0/6893 (0%)]\tLoss: 431.146576\n",
      "Train Epoch: 51 [6400/6893 (1%)]\tLoss: 435.620270\n",
      "====> Epoch: 51 Average loss: 434.8804\n",
      "====> Test set loss: 429.6375\n",
      "Train Epoch: 52 [0/6893 (0%)]\tLoss: 435.378540\n",
      "Train Epoch: 52 [6400/6893 (1%)]\tLoss: 414.201660\n",
      "====> Epoch: 52 Average loss: 427.8811\n",
      "====> Test set loss: 424.9954\n",
      "Train Epoch: 53 [0/6893 (0%)]\tLoss: 433.152069\n",
      "Train Epoch: 53 [6400/6893 (1%)]\tLoss: 426.823181\n",
      "====> Epoch: 53 Average loss: 422.2638\n",
      "====> Test set loss: 416.5564\n",
      "Train Epoch: 54 [0/6893 (0%)]\tLoss: 421.890350\n",
      "Train Epoch: 54 [6400/6893 (1%)]\tLoss: 405.016052\n",
      "====> Epoch: 54 Average loss: 415.3515\n",
      "====> Test set loss: 411.7351\n",
      "Train Epoch: 55 [0/6893 (0%)]\tLoss: 398.312805\n",
      "Train Epoch: 55 [6400/6893 (1%)]\tLoss: 420.354401\n",
      "====> Epoch: 55 Average loss: 408.9861\n",
      "====> Test set loss: 404.1925\n",
      "Train Epoch: 56 [0/6893 (0%)]\tLoss: 415.611481\n",
      "Train Epoch: 56 [6400/6893 (1%)]\tLoss: 394.129120\n",
      "====> Epoch: 56 Average loss: 403.2312\n",
      "====> Test set loss: 398.5372\n",
      "Train Epoch: 57 [0/6893 (0%)]\tLoss: 402.402069\n",
      "Train Epoch: 57 [6400/6893 (1%)]\tLoss: 399.550446\n",
      "====> Epoch: 57 Average loss: 396.7650\n",
      "====> Test set loss: 393.5547\n",
      "Train Epoch: 58 [0/6893 (0%)]\tLoss: 391.330322\n",
      "Train Epoch: 58 [6400/6893 (1%)]\tLoss: 379.702362\n",
      "====> Epoch: 58 Average loss: 391.8525\n",
      "====> Test set loss: 389.1188\n",
      "Train Epoch: 59 [0/6893 (0%)]\tLoss: 391.178406\n",
      "Train Epoch: 59 [6400/6893 (1%)]\tLoss: 393.494446\n",
      "====> Epoch: 59 Average loss: 386.6170\n",
      "====> Test set loss: 383.1335\n",
      "Train Epoch: 60 [0/6893 (0%)]\tLoss: 381.860779\n",
      "Train Epoch: 60 [6400/6893 (1%)]\tLoss: 380.248169\n",
      "====> Epoch: 60 Average loss: 381.4310\n",
      "====> Test set loss: 377.3724\n",
      "Train Epoch: 61 [0/6893 (0%)]\tLoss: 374.378448\n",
      "Train Epoch: 61 [6400/6893 (1%)]\tLoss: 371.119080\n",
      "====> Epoch: 61 Average loss: 376.4233\n",
      "====> Test set loss: 373.0421\n",
      "Train Epoch: 62 [0/6893 (0%)]\tLoss: 375.325836\n",
      "Train Epoch: 62 [6400/6893 (1%)]\tLoss: 372.075409\n",
      "====> Epoch: 62 Average loss: 372.6790\n",
      "====> Test set loss: 369.5151\n",
      "Train Epoch: 63 [0/6893 (0%)]\tLoss: 366.791870\n",
      "Train Epoch: 63 [6400/6893 (1%)]\tLoss: 363.606903\n",
      "====> Epoch: 63 Average loss: 367.2098\n",
      "====> Test set loss: 363.2451\n",
      "Train Epoch: 64 [0/6893 (0%)]\tLoss: 361.242218\n",
      "Train Epoch: 64 [6400/6893 (1%)]\tLoss: 349.115692\n",
      "====> Epoch: 64 Average loss: 362.4819\n",
      "====> Test set loss: 359.6894\n",
      "Train Epoch: 65 [0/6893 (0%)]\tLoss: 366.414398\n",
      "Train Epoch: 65 [6400/6893 (1%)]\tLoss: 361.577301\n",
      "====> Epoch: 65 Average loss: 357.9125\n",
      "====> Test set loss: 354.1604\n",
      "Train Epoch: 66 [0/6893 (0%)]\tLoss: 365.003510\n",
      "Train Epoch: 66 [6400/6893 (1%)]\tLoss: 356.425385\n",
      "====> Epoch: 66 Average loss: 354.0344\n",
      "====> Test set loss: 350.5131\n",
      "Train Epoch: 67 [0/6893 (0%)]\tLoss: 344.691010\n",
      "Train Epoch: 67 [6400/6893 (1%)]\tLoss: 351.214539\n",
      "====> Epoch: 67 Average loss: 349.5508\n",
      "====> Test set loss: 345.3764\n",
      "Train Epoch: 68 [0/6893 (0%)]\tLoss: 344.619110\n",
      "Train Epoch: 68 [6400/6893 (1%)]\tLoss: 352.099762\n",
      "====> Epoch: 68 Average loss: 345.9721\n",
      "====> Test set loss: 342.0522\n",
      "Train Epoch: 69 [0/6893 (0%)]\tLoss: 351.886505\n",
      "Train Epoch: 69 [6400/6893 (1%)]\tLoss: 346.608032\n",
      "====> Epoch: 69 Average loss: 341.7020\n",
      "====> Test set loss: 338.6974\n",
      "Train Epoch: 70 [0/6893 (0%)]\tLoss: 336.749298\n",
      "Train Epoch: 70 [6400/6893 (1%)]\tLoss: 334.119568\n",
      "====> Epoch: 70 Average loss: 337.0312\n",
      "====> Test set loss: 334.4395\n",
      "Train Epoch: 71 [0/6893 (0%)]\tLoss: 334.671600\n",
      "Train Epoch: 71 [6400/6893 (1%)]\tLoss: 327.512177\n",
      "====> Epoch: 71 Average loss: 333.7908\n",
      "====> Test set loss: 330.7067\n",
      "Train Epoch: 72 [0/6893 (0%)]\tLoss: 327.921875\n",
      "Train Epoch: 72 [6400/6893 (1%)]\tLoss: 327.751007\n",
      "====> Epoch: 72 Average loss: 329.0960\n",
      "====> Test set loss: 327.1661\n",
      "Train Epoch: 73 [0/6893 (0%)]\tLoss: 319.615326\n",
      "Train Epoch: 73 [6400/6893 (1%)]\tLoss: 321.495056\n",
      "====> Epoch: 73 Average loss: 325.6720\n",
      "====> Test set loss: 323.0796\n",
      "Train Epoch: 74 [0/6893 (0%)]\tLoss: 326.878265\n",
      "Train Epoch: 74 [6400/6893 (1%)]\tLoss: 320.053650\n",
      "====> Epoch: 74 Average loss: 321.5704\n",
      "====> Test set loss: 320.1389\n",
      "Train Epoch: 75 [0/6893 (0%)]\tLoss: 318.359863\n",
      "Train Epoch: 75 [6400/6893 (1%)]\tLoss: 316.164429\n",
      "====> Epoch: 75 Average loss: 318.2929\n",
      "====> Test set loss: 316.3026\n",
      "Train Epoch: 76 [0/6893 (0%)]\tLoss: 309.933502\n",
      "Train Epoch: 76 [6400/6893 (1%)]\tLoss: 315.200684\n",
      "====> Epoch: 76 Average loss: 314.5060\n",
      "====> Test set loss: 312.4083\n",
      "Train Epoch: 77 [0/6893 (0%)]\tLoss: 308.373810\n",
      "Train Epoch: 77 [6400/6893 (1%)]\tLoss: 309.857758\n",
      "====> Epoch: 77 Average loss: 310.5813\n",
      "====> Test set loss: 307.4319\n",
      "Train Epoch: 78 [0/6893 (0%)]\tLoss: 310.259460\n",
      "Train Epoch: 78 [6400/6893 (1%)]\tLoss: 307.205688\n",
      "====> Epoch: 78 Average loss: 307.2747\n",
      "====> Test set loss: 305.3901\n",
      "Train Epoch: 79 [0/6893 (0%)]\tLoss: 308.420074\n",
      "Train Epoch: 79 [6400/6893 (1%)]\tLoss: 298.112885\n",
      "====> Epoch: 79 Average loss: 303.6694\n",
      "====> Test set loss: 301.3333\n",
      "Train Epoch: 80 [0/6893 (0%)]\tLoss: 298.021484\n",
      "Train Epoch: 80 [6400/6893 (1%)]\tLoss: 299.365417\n",
      "====> Epoch: 80 Average loss: 300.1228\n",
      "====> Test set loss: 297.5968\n",
      "Train Epoch: 81 [0/6893 (0%)]\tLoss: 296.418549\n",
      "Train Epoch: 81 [6400/6893 (1%)]\tLoss: 288.016388\n",
      "====> Epoch: 81 Average loss: 296.6271\n",
      "====> Test set loss: 294.1223\n",
      "Train Epoch: 82 [0/6893 (0%)]\tLoss: 294.887390\n",
      "Train Epoch: 82 [6400/6893 (1%)]\tLoss: 295.642029\n",
      "====> Epoch: 82 Average loss: 293.2297\n",
      "====> Test set loss: 290.4408\n",
      "Train Epoch: 83 [0/6893 (0%)]\tLoss: 287.875580\n",
      "Train Epoch: 83 [6400/6893 (1%)]\tLoss: 288.937653\n",
      "====> Epoch: 83 Average loss: 290.0756\n",
      "====> Test set loss: 287.0743\n",
      "Train Epoch: 84 [0/6893 (0%)]\tLoss: 282.996185\n",
      "Train Epoch: 84 [6400/6893 (1%)]\tLoss: 283.797180\n",
      "====> Epoch: 84 Average loss: 286.4418\n",
      "====> Test set loss: 283.3765\n",
      "Train Epoch: 85 [0/6893 (0%)]\tLoss: 285.606598\n",
      "Train Epoch: 85 [6400/6893 (1%)]\tLoss: 288.792023\n",
      "====> Epoch: 85 Average loss: 282.6942\n",
      "====> Test set loss: 279.5603\n",
      "Train Epoch: 86 [0/6893 (0%)]\tLoss: 285.777954\n",
      "Train Epoch: 86 [6400/6893 (1%)]\tLoss: 286.049927\n",
      "====> Epoch: 86 Average loss: 278.9182\n",
      "====> Test set loss: 276.5087\n",
      "Train Epoch: 87 [0/6893 (0%)]\tLoss: 280.153625\n",
      "Train Epoch: 87 [6400/6893 (1%)]\tLoss: 272.051849\n",
      "====> Epoch: 87 Average loss: 275.2857\n",
      "====> Test set loss: 272.6302\n",
      "Train Epoch: 88 [0/6893 (0%)]\tLoss: 269.666687\n",
      "Train Epoch: 88 [6400/6893 (1%)]\tLoss: 266.953339\n",
      "====> Epoch: 88 Average loss: 273.0552\n",
      "====> Test set loss: 270.1505\n",
      "Train Epoch: 89 [0/6893 (0%)]\tLoss: 273.753937\n",
      "Train Epoch: 89 [6400/6893 (1%)]\tLoss: 266.436646\n",
      "====> Epoch: 89 Average loss: 269.4458\n",
      "====> Test set loss: 266.5458\n",
      "Train Epoch: 90 [0/6893 (0%)]\tLoss: 270.947021\n",
      "Train Epoch: 90 [6400/6893 (1%)]\tLoss: 271.348938\n",
      "====> Epoch: 90 Average loss: 266.0720\n",
      "====> Test set loss: 263.8820\n",
      "Train Epoch: 91 [0/6893 (0%)]\tLoss: 272.144806\n",
      "Train Epoch: 91 [6400/6893 (1%)]\tLoss: 260.682404\n",
      "====> Epoch: 91 Average loss: 262.3258\n",
      "====> Test set loss: 260.4247\n",
      "Train Epoch: 92 [0/6893 (0%)]\tLoss: 260.172241\n",
      "Train Epoch: 92 [6400/6893 (1%)]\tLoss: 257.229797\n",
      "====> Epoch: 92 Average loss: 259.3971\n",
      "====> Test set loss: 256.5731\n",
      "Train Epoch: 93 [0/6893 (0%)]\tLoss: 259.040039\n",
      "Train Epoch: 93 [6400/6893 (1%)]\tLoss: 259.158569\n",
      "====> Epoch: 93 Average loss: 256.0461\n",
      "====> Test set loss: 253.6656\n",
      "Train Epoch: 94 [0/6893 (0%)]\tLoss: 254.600494\n",
      "Train Epoch: 94 [6400/6893 (1%)]\tLoss: 254.306046\n",
      "====> Epoch: 94 Average loss: 252.3742\n",
      "====> Test set loss: 250.8753\n",
      "Train Epoch: 95 [0/6893 (0%)]\tLoss: 248.161850\n",
      "Train Epoch: 95 [6400/6893 (1%)]\tLoss: 245.014267\n",
      "====> Epoch: 95 Average loss: 249.4208\n",
      "====> Test set loss: 247.2538\n",
      "Train Epoch: 96 [0/6893 (0%)]\tLoss: 251.108093\n",
      "Train Epoch: 96 [6400/6893 (1%)]\tLoss: 251.815689\n",
      "====> Epoch: 96 Average loss: 246.5253\n",
      "====> Test set loss: 243.6103\n",
      "Train Epoch: 97 [0/6893 (0%)]\tLoss: 249.989609\n",
      "Train Epoch: 97 [6400/6893 (1%)]\tLoss: 236.452911\n",
      "====> Epoch: 97 Average loss: 243.0381\n",
      "====> Test set loss: 240.9635\n",
      "Train Epoch: 98 [0/6893 (0%)]\tLoss: 242.088242\n",
      "Train Epoch: 98 [6400/6893 (1%)]\tLoss: 235.972977\n",
      "====> Epoch: 98 Average loss: 240.1539\n",
      "====> Test set loss: 236.7496\n",
      "Train Epoch: 99 [0/6893 (0%)]\tLoss: 238.971771\n",
      "Train Epoch: 99 [6400/6893 (1%)]\tLoss: 236.394852\n",
      "====> Epoch: 99 Average loss: 236.6289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 234.0084\n",
      "Train Epoch: 100 [0/6893 (0%)]\tLoss: 239.565933\n",
      "Train Epoch: 100 [6400/6893 (1%)]\tLoss: 230.973328\n",
      "====> Epoch: 100 Average loss: 233.7206\n",
      "====> Test set loss: 231.7543\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, epoch, pretrain_vae)\n",
    "        test(test_data, vae_gumbel_with_pre, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = vae_gumbel_with_pre(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = vae_gumbel_with_pre(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1859, device='cuda:0')\n",
      "tensor(0.1849, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(446, device='cuda:0')\n",
      "tensor(264, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(test_pred[0,:] != 0))\n",
    "print(torch.sum(test_data[0,:] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 should be vanilla\n",
    "\n",
    "def train_joint(df, model1, model2, optimizer, epoch):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        loss_vae_1, loss_vae_2, joint_kld_loss = loss_function_joint(batch_data, model1, model2)\n",
    "        loss = (loss_vae_1 + loss_vae_2 + 1000 * joint_kld_loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i * len(batch_data)/ len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_joint(df, model1, model2, epoch):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    inds = np.arange(df.shape[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = df[batch_ind, :]\n",
    "            batch_data = batch_data.to(device)\n",
    "            loss_vae_1, loss_vae_2, joint_kld_loss = loss_function_joint(batch_data, model1, model2)\n",
    "        \n",
    "            test_loss += (loss_vae_1 + loss_vae_2 + 1000 * joint_kld_loss).item()\n",
    "\n",
    "\n",
    "    test_loss /= len(df)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(500, 200, 50)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(500, 200, 50, k = 50)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 23476.283203\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 21387.679688\n",
      "====> Epoch: 1 Average loss: 22362.1970\n",
      "====> Test set loss: 21281.3498\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 21306.634766\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 19388.736328\n",
      "====> Epoch: 2 Average loss: 20301.2954\n",
      "====> Test set loss: 19339.7691\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 19285.892578\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 17649.837891\n",
      "====> Epoch: 3 Average loss: 18424.3846\n",
      "====> Test set loss: 17506.8465\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 17461.537109\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 15772.488281\n",
      "====> Epoch: 4 Average loss: 16598.7503\n",
      "====> Test set loss: 15695.4505\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 15659.019531\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 13970.343750\n",
      "====> Epoch: 5 Average loss: 14825.5477\n",
      "====> Test set loss: 13966.9555\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 13997.325195\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 12492.328125\n",
      "====> Epoch: 6 Average loss: 13148.8052\n",
      "====> Test set loss: 12347.7899\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 12234.872070\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 11055.357422\n",
      "====> Epoch: 7 Average loss: 11601.8917\n",
      "====> Test set loss: 10873.2632\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 10895.202148\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 9755.891602\n",
      "====> Epoch: 8 Average loss: 10204.9915\n",
      "====> Test set loss: 9547.8565\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 9497.896484\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 8361.344727\n",
      "====> Epoch: 9 Average loss: 8943.7172\n",
      "====> Test set loss: 8340.8995\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 8417.198242\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 7432.772461\n",
      "====> Epoch: 10 Average loss: 7790.8208\n",
      "====> Test set loss: 7233.0518\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 7365.410645\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 6549.535645\n",
      "====> Epoch: 11 Average loss: 6731.4181\n",
      "====> Test set loss: 6234.0101\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 6142.554688\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 5388.414551\n",
      "====> Epoch: 12 Average loss: 5791.1938\n",
      "====> Test set loss: 5356.3984\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 5411.180176\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 4729.906250\n",
      "====> Epoch: 13 Average loss: 4963.0323\n",
      "====> Test set loss: 4574.1548\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 4376.315430\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 4017.093506\n",
      "====> Epoch: 14 Average loss: 4258.3540\n",
      "====> Test set loss: 3932.9132\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 4204.677246\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 3465.057617\n",
      "====> Epoch: 15 Average loss: 3678.7664\n",
      "====> Test set loss: 3415.0893\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 3616.656250\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 2944.933838\n",
      "====> Epoch: 16 Average loss: 3211.6276\n",
      "====> Test set loss: 2995.9720\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 2851.483154\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 2572.220703\n",
      "====> Epoch: 17 Average loss: 2848.3912\n",
      "====> Test set loss: 2682.3932\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 2768.791748\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 2467.304688\n",
      "====> Epoch: 18 Average loss: 2570.7865\n",
      "====> Test set loss: 2431.8491\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 2520.031738\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 2116.393555\n",
      "====> Epoch: 19 Average loss: 2353.2186\n",
      "====> Test set loss: 2244.5372\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 2116.435059\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 1983.282471\n",
      "====> Epoch: 20 Average loss: 2190.4247\n",
      "====> Test set loss: 2096.0636\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 2330.153809\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 2155.096191\n",
      "====> Epoch: 21 Average loss: 2056.7296\n",
      "====> Test set loss: 1980.8431\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 1975.753540\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 1781.695435\n",
      "====> Epoch: 22 Average loss: 1948.1515\n",
      "====> Test set loss: 1888.9618\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 1939.779419\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 2279.981689\n",
      "====> Epoch: 23 Average loss: 1864.8411\n",
      "====> Test set loss: 1806.9093\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 1901.773315\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 1630.781006\n",
      "====> Epoch: 24 Average loss: 1790.7294\n",
      "====> Test set loss: 1741.1288\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 1544.300415\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 1640.491089\n",
      "====> Epoch: 25 Average loss: 1724.4001\n",
      "====> Test set loss: 1677.6561\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 1681.370239\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 1618.253174\n",
      "====> Epoch: 26 Average loss: 1670.1004\n",
      "====> Test set loss: 1630.3352\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 1648.914551\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 1648.339478\n",
      "====> Epoch: 27 Average loss: 1622.9925\n",
      "====> Test set loss: 1584.7128\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 1604.426270\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 1717.684448\n",
      "====> Epoch: 28 Average loss: 1576.6980\n",
      "====> Test set loss: 1539.7280\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 1450.374268\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 1696.047363\n",
      "====> Epoch: 29 Average loss: 1533.4951\n",
      "====> Test set loss: 1499.9265\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 1455.906738\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 1409.011597\n",
      "====> Epoch: 30 Average loss: 1492.1517\n",
      "====> Test set loss: 1463.6601\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 1350.882690\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 1399.370850\n",
      "====> Epoch: 31 Average loss: 1455.4118\n",
      "====> Test set loss: 1425.7620\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 1425.876953\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 1447.407471\n",
      "====> Epoch: 32 Average loss: 1420.2368\n",
      "====> Test set loss: 1392.8853\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 1349.203125\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 1353.967896\n",
      "====> Epoch: 33 Average loss: 1386.4622\n",
      "====> Test set loss: 1355.2290\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 1410.896973\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 1277.687622\n",
      "====> Epoch: 34 Average loss: 1353.0306\n",
      "====> Test set loss: 1330.7204\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 1320.790405\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 1277.826660\n",
      "====> Epoch: 35 Average loss: 1323.0714\n",
      "====> Test set loss: 1299.8387\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 1254.052490\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 1244.227539\n",
      "====> Epoch: 36 Average loss: 1292.7013\n",
      "====> Test set loss: 1267.6524\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 1253.188599\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 1256.567017\n",
      "====> Epoch: 37 Average loss: 1265.4415\n",
      "====> Test set loss: 1245.8603\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 1188.300293\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 1244.112305\n",
      "====> Epoch: 38 Average loss: 1235.0128\n",
      "====> Test set loss: 1215.1703\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 1141.802490\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 1218.437622\n",
      "====> Epoch: 39 Average loss: 1209.8548\n",
      "====> Test set loss: 1190.2040\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 1168.032471\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 1151.818604\n",
      "====> Epoch: 40 Average loss: 1182.4055\n",
      "====> Test set loss: 1167.3123\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 1139.740479\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 1043.482788\n",
      "====> Epoch: 41 Average loss: 1158.6041\n",
      "====> Test set loss: 1143.6739\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 1205.277832\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 1110.964233\n",
      "====> Epoch: 42 Average loss: 1134.8618\n",
      "====> Test set loss: 1122.1023\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 1202.867920\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 1074.108521\n",
      "====> Epoch: 43 Average loss: 1113.7225\n",
      "====> Test set loss: 1100.9234\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 1102.527344\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 1062.997925\n",
      "====> Epoch: 44 Average loss: 1091.5460\n",
      "====> Test set loss: 1079.3651\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 1118.158203\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 1002.141785\n",
      "====> Epoch: 45 Average loss: 1072.0935\n",
      "====> Test set loss: 1060.5933\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 1030.668579\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 1026.247070\n",
      "====> Epoch: 46 Average loss: 1050.4343\n",
      "====> Test set loss: 1040.0180\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 1017.030701\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 995.528564\n",
      "====> Epoch: 47 Average loss: 1031.7255\n",
      "====> Test set loss: 1022.8723\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 1040.089600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-de6366f6e2bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_vanilla_vae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_vae_gumbel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_vanilla_vae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_vae_gumbel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-edd2fc9b7966>\u001b[0m in \u001b[0;36mtrain_joint\u001b[0;34m(df, model1, model2, optimizer, epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss_vae_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_vae_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_kld_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function_joint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_vae_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_vae_2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjoint_kld_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d470e9fafa97>\u001b[0m in \u001b[0;36mloss_function_joint\u001b[0;34m(x, ae_1, ae_2)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# ae_2 should be the L1 penalty VAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmu_x_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_x_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_latent_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmu_x_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_x_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_latent_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss_vae_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function_per_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_x_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_x_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_latent_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-354b19f93c62>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mmu_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmu_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-354b19f93c62>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0msubset_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msubset_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-e5a7780ff259>\u001b[0m in \u001b[0;36msample_subset\u001b[0;34m(w, k, t)\u001b[0m\n\u001b[1;32m     38\u001b[0m     '''\n\u001b[1;32m     39\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgumbel_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontinuous_topk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-e5a7780ff259>\u001b[0m in \u001b[0;36mcontinuous_topk\u001b[0;34m(w, k, t, separate)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmax_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0monehot_approx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mkhot_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0monehot_approx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mkhot_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkhot_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = joint_vae_gumbel(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = joint_vae_gumbel(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1934, device='cuda:0')\n",
      "tensor(0.1961, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(433, device='cuda:0')\n",
      "tensor(264, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(test_pred[0,:] != 0))\n",
    "print(torch.sum(test_data[0,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's actually Garph this.\n",
    "\n",
    "### Try it out at Gumbel sparsity of k = 10, 25, 50, 100, 250\n",
    "\n",
    "### Graph Test MSE Loss\n",
    "\n",
    "## Graph the mean activations at k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_activations(test_data, model, title, file):\n",
    "    preds, _, _, _ = model(test_data)\n",
    "    \n",
    "    pred_activations = preds.mean(dim = 0)\n",
    "    \n",
    "    test_activations = test_data.mean(dim = 0)\n",
    "    \n",
    "    x = np.arange(500) + 1\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, pred_activations.clone().detach().cpu().numpy(), label = 'Average Predictions')\n",
    "    plt.plot(x, test_activations.clone().detach().cpu().numpy(), label = 'Average Test Data')\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_activations(test_data, model_l1_diag, 'Joint Gumbel vs Test Means', \n",
    "                  '/scratch/ns3429/sparse-subset/vae_l1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/ns3429/sparse-subset/joint_gumbel.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-1633eff4adcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m graph_activations(test_data, joint_vae_gumbel, 'Joint Gumbel vs Test Means', \n\u001b[0;32m----> 2\u001b[0;31m                   '/scratch/ns3429/sparse-subset/joint_gumbel.png')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-55d1f8299e2d>\u001b[0m in \u001b[0;36mgraph_activations\u001b[0;34m(test_data, model, title, file)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2092\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                     \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[1;32m    532\u001b[0m                                self.figure.dpi, metadata=metadata)\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/ns3429/sparse-subset/joint_gumbel.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_activations(test_data, joint_vae_gumbel, 'Joint Gumbel vs Test Means', \n",
    "                  '/scratch/ns3429/sparse-subset/joint_gumbel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/ns3429/sparse-subset/pretrained_gumbel.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-61df8f17b2ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m graph_activations(test_data, vae_gumbel_with_pre, 'Gumbel Matching Pretrained VAE vs Test Means', \n\u001b[0;32m----> 2\u001b[0;31m                   '/scratch/ns3429/sparse-subset/pretrained_gumbel.png')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-55d1f8299e2d>\u001b[0m in \u001b[0;36mgraph_activations\u001b[0;34m(test_data, model, title, file)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2092\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m                     \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[1;32m    532\u001b[0m                                self.figure.dpi, metadata=metadata)\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/ns3429/sparse-subset/pretrained_gumbel.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_activations(test_data, vae_gumbel_with_pre, 'Gumbel Matching Pretrained VAE vs Test Means', \n",
    "                  '/scratch/ns3429/sparse-subset/pretrained_gumbel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_all = [5, 10, 15, 20, 25, 30, 40, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375,\n",
    "         400, 425, 450]\n",
    "n_trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_pre = []\n",
    "losses_joint = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_all:\n",
    "    current_k_pre_losses = []\n",
    "    current_k_joint_losses = []\n",
    "    for trial_i in range(n_trials):\n",
    "        print(\"RUNNING for K {} Trial {}\".format(k, trial_i), flush=True)\n",
    "        vae_gumbel_with_pre = VAE_Gumbel(500, 200, 50, k = k)\n",
    "        vae_gumbel_with_pre.to(device)\n",
    "        vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                        lr=lr, \n",
    "                                                        betas = (b1,b2))\n",
    "    \n",
    "        joint_vanilla_vae = VAE(500, 200, 50)\n",
    "        joint_vanilla_vae.to(device)\n",
    "\n",
    "        joint_vae_gumbel = VAE_Gumbel(500, 200, 50, k = k)\n",
    "        joint_vae_gumbel.to(device)\n",
    "\n",
    "\n",
    "        joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + \n",
    "                                           list(joint_vae_gumbel.parameters()),\n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))\n",
    "    \n",
    "        for epoch in (1, n_epochs + 1):\n",
    "            train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, epoch, pretrain_vae)\n",
    "            train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch)\n",
    "    \n",
    "        test_pred_pre = vae_gumbel_with_pre(test_data)[0]\n",
    "        test_pred_pre[test_pred_pre < 0.001] = 0 \n",
    "    \n",
    "        test_pred_joint = joint_vanilla_vae(test_data)[0]\n",
    "        test_pred_joint[test_pred_joint < 0.001] = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            mae_pre = torch.sum((test_pred_pre - test_data).abs()) / len(test_data) / 500\n",
    "            mae_joint = torch.sum((test_pred_joint - test_data).abs()) / len(test_data) / 500\n",
    "        \n",
    "        current_k_pre_losses.append(mae_pre.cpu().item())\n",
    "        current_k_joint_losses.append(mae_joint.cpu().item())\n",
    "        \n",
    "        # for freeing memory faster\n",
    "        # but not too fast\n",
    "        if (trial+1) % 2 == 0:\n",
    "            del vae_gumbel_with_pre\n",
    "            del vae_gumbel_with_pre_optimizer\n",
    "            del joint_vanilla_vae\n",
    "            del joint_vae_gumbel\n",
    "            del joint_optimizer\n",
    "            del test_pred_pre\n",
    "            del test_pred_joint\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    \n",
    "    losses_pre.append(np.mean(current_k_pre_losses))\n",
    "    losses_joint.append(np.mean(current_k_joint_losses))\n",
    "    \n",
    "    \n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(k_all, losses_pre, label = 'Average MAE Losses with Gumbel Matching Pretrained')\n",
    "plt.plot(k_all, losses_joint, label = 'Average MAE Losses with Gumbel Joint Training')\n",
    "\n",
    "plt.title(\"Effect on Sparsity on MAE Loss\")\n",
    "plt.xlabel('Sparsity Level (Number of Non-Zero Features)')\n",
    "plt.ylabel('Per Neuron Average MAE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('/scratch/ns3429/sparse-subset/comparing_across_sparsity.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
