{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just get a quick sparsity overview of the methods so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = '../data/'\n",
    "#BASE_PATH_DATA = '/scratch/ns3429/sparse-subset/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "lr = 0.000002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "img_size = 28\n",
    "channels = 1\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "z_size = 40\n",
    "\n",
    "n = 28 * 28\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/CITEseq.mat')\n",
    "data= a['G'].T\n",
    "N,d=data.shape\n",
    "#transformation from integer entries \n",
    "data=np.log(data+np.ones(data.shape))\n",
    "#for i in range(N):\n",
    "for i in range(d):\n",
    "    #data[i,:]=data[i,:]/np.linalg.norm(data[i,:])\n",
    "    data[:,i]= (data[:,i] - np.min(data[:,i])) /  (np.max(data[:,i]) - np.min(data[:, i]))\n",
    "\n",
    "#load labels from file\n",
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/CITEseq-labels.mat')\n",
    "l_aux = a['labels']\n",
    "labels = np.array([i for [i] in l_aux])\n",
    "\n",
    "#load names from file\n",
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/CITEseq_names.mat')\n",
    "names=[a['citeseq_names'][i][0][0] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(data.shape[0]))\n",
    "upto = int(.8 * len(data))\n",
    "\n",
    "train_data = data[slices[:upto]]\n",
    "test_data = data[slices[upto:]]\n",
    "\n",
    "train_data = Tensor(train_data).to(device)\n",
    "test_data = Tensor(test_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1830, device='cuda:0')\n",
      "tensor(0.1831, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.std(dim = 0).mean())\n",
    "print(test_data.std(dim = 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_per_autoencoder(x, mu_x, logvar_x, mu_latent, logvar_latent):\n",
    "    # BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    # BCE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    loss_rec = -torch.sum(\n",
    "            (-0.5 * np.log(2.0 * np.pi))\n",
    "            + (-0.5 * logvar_x)\n",
    "            + ((-0.5 / torch.exp(logvar_x)) * (x - mu_x) ** 2.0))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp())\n",
    "    #print(loss_rec.item(), KLD.item())\n",
    "    return loss_rec + 130640 * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLD of D(P_1||P_2) where P_i are Gaussians, assuming diagonal\n",
    "def kld_joint_autoencoders(mu_1, mu_2, logvar_1, logvar_2):\n",
    "    # equation 6 of Tutorial on Variational Autoencoders by Carl Doersch\n",
    "    # https://arxiv.org/pdf/1606.05908.pdf\n",
    "    mu_12 = mu_1 - mu_2\n",
    "    kld = 0.5 * (-1 - (logvar_1 - logvar_2) + mu_12.pow(2) / logvar_2.exp() + torch.exp(logvar_1 - logvar_2))\n",
    "    #print(kld.shape)\n",
    "    kld = torch.sum(kld, dim = 1)\n",
    "    \n",
    "    return kld.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for joint\n",
    "def loss_function_joint(x, ae_1, ae_2):\n",
    "    # assuming that both autoencoders return recon_x, mu, and logvar\n",
    "    # try to make ae_1 the vanilla vae\n",
    "    # ae_2 should be the L1 penalty VAE\n",
    "    mu_x_1, logvar_x_1, mu_latent_1, logvar_latent_1 = ae_1(x)\n",
    "    mu_x_2, logvar_x_2, mu_latent_2, logvar_latent_2 = ae_2(x)\n",
    "    \n",
    "    loss_vae_1 = loss_function_per_autoencoder(x, mu_x_1, logvar_x_1, mu_latent_1, logvar_latent_1)\n",
    "    loss_vae_2 = loss_function_per_autoencoder(x, mu_x_2, logvar_x_2, mu_latent_2, logvar_latent_2)\n",
    "    joint_kld_loss = kld_joint_autoencoders(mu_latent_1, mu_latent_2, logvar_latent_1, logvar_latent_1)\n",
    "    #print(\"Losses\")\n",
    "    #print(loss_vae_1)\n",
    "    #print(loss_vae_2)\n",
    "    #print(joint_kld_loss)\n",
    "    return loss_vae_1, loss_vae_2, joint_kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does L1 work if we normalize after every step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_l1_diag(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size):\n",
    "        super(VAE_l1_diag, self).__init__()\n",
    "        \n",
    "        self.diag = nn.Parameter(torch.normal(torch.zeros(input_size), \n",
    "                                 torch.ones(input_size)).to(device).requires_grad_(True))\n",
    "        \n",
    "        # self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "        self.fc5 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.selection_layer = torch.diag(self.diag)\n",
    "        h0 = torch.mm(x, self.selection_layer)\n",
    "        h1 = self.encoder(h0)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.leaky_relu(self.fc3(z))\n",
    "        mu_x = F.leaky_relu(self.fc4(h))\n",
    "        #mu_x = self.fc4(h)\n",
    "        logvar_x = self.fc5(h)\n",
    "        return mu_x, logvar_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_latent, logvar_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu_latent, logvar_latent)\n",
    "        mu_x, logvar_x = self.decode(z)\n",
    "        return mu_x, logvar_x, mu_latent, logvar_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l1(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        loss = loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent)\n",
    "        loss += 1000000 * torch.norm(model.diag, p = 1)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.diag.data /= torch.norm(model.diag.data, p = 2)\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df, model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    inds = np.arange(df.shape[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = df[batch_ind, :]\n",
    "            batch_data = batch_data.to(device)\n",
    "            mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "            test_loss += loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent).item()\n",
    "\n",
    "\n",
    "    test_loss /= len(df)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1_diag = VAE_l1_diag(500, 200, 50)\n",
    "\n",
    "model_l1_diag.to(device)\n",
    "model_l1_optimizer = torch.optim.Adam(model_l1_diag.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 6556737.000000\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 294779.000000\n",
      "====> Epoch: 1 Average loss: 354253.9434\n",
      "====> Test set loss: 10521.8996\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 294684.687500\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 293521.468750\n",
      "====> Epoch: 2 Average loss: 294844.8237\n",
      "====> Test set loss: 9836.2487\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 293431.906250\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 292304.031250\n",
      "====> Epoch: 3 Average loss: 293609.2259\n",
      "====> Test set loss: 9194.2115\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 292215.875000\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 291127.218750\n",
      "====> Epoch: 4 Average loss: 292411.6008\n",
      "====> Test set loss: 8590.8992\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 291040.062500\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 289979.156250\n",
      "====> Epoch: 5 Average loss: 291248.3557\n",
      "====> Test set loss: 8019.4116\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 289895.343750\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 288857.687500\n",
      "====> Epoch: 6 Average loss: 290117.5131\n",
      "====> Test set loss: 7468.7560\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 288777.875000\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 287749.343750\n",
      "====> Epoch: 7 Average loss: 289002.4959\n",
      "====> Test set loss: 6935.6027\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 287672.937500\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 286668.156250\n",
      "====> Epoch: 8 Average loss: 287906.6775\n",
      "====> Test set loss: 6430.5501\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 286584.000000\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 285604.687500\n",
      "====> Epoch: 9 Average loss: 286832.1660\n",
      "====> Test set loss: 5944.0853\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 285530.093750\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 284576.906250\n",
      "====> Epoch: 10 Average loss: 285783.0411\n",
      "====> Test set loss: 5485.0828\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 284494.718750\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 283587.093750\n",
      "====> Epoch: 11 Average loss: 284771.3106\n",
      "====> Test set loss: 5051.5446\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 283496.937500\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 282598.750000\n",
      "====> Epoch: 12 Average loss: 283788.4204\n",
      "====> Test set loss: 4631.0704\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 282528.687500\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 281627.406250\n",
      "====> Epoch: 13 Average loss: 282813.7110\n",
      "====> Test set loss: 4223.3990\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 281568.562500\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 280709.218750\n",
      "====> Epoch: 14 Average loss: 281871.5337\n",
      "====> Test set loss: 3849.5075\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 280644.468750\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 279824.468750\n",
      "====> Epoch: 15 Average loss: 280958.7715\n",
      "====> Test set loss: 3506.8554\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 279760.562500\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 278926.843750\n",
      "====> Epoch: 16 Average loss: 280074.7329\n",
      "====> Test set loss: 3191.6983\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 278887.562500\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 278107.125000\n",
      "====> Epoch: 17 Average loss: 279222.5620\n",
      "====> Test set loss: 2902.4373\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 278042.906250\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 277297.281250\n",
      "====> Epoch: 18 Average loss: 278395.5361\n",
      "====> Test set loss: 2636.4346\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 277226.218750\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 276465.968750\n",
      "====> Epoch: 19 Average loss: 277575.7548\n",
      "====> Test set loss: 2367.4111\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 276391.000000\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 275669.312500\n",
      "====> Epoch: 20 Average loss: 276760.4796\n",
      "====> Test set loss: 2112.3560\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 275582.687500\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 274845.500000\n",
      "====> Epoch: 21 Average loss: 275966.0389\n",
      "====> Test set loss: 1878.0671\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 274829.843750\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 274114.312500\n",
      "====> Epoch: 22 Average loss: 275195.5053\n",
      "====> Test set loss: 1669.6166\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 274067.000000\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 273373.281250\n",
      "====> Epoch: 23 Average loss: 274448.4637\n",
      "====> Test set loss: 1482.9566\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 273316.968750\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 272677.718750\n",
      "====> Epoch: 24 Average loss: 273722.3032\n",
      "====> Test set loss: 1313.7409\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 272618.156250\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 271978.156250\n",
      "====> Epoch: 25 Average loss: 273016.9804\n",
      "====> Test set loss: 1165.9878\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 271911.156250\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 271314.812500\n",
      "====> Epoch: 26 Average loss: 272337.3631\n",
      "====> Test set loss: 1036.7352\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 271264.718750\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 270679.593750\n",
      "====> Epoch: 27 Average loss: 271692.0168\n",
      "====> Test set loss: 926.0989\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 270629.781250\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 270068.906250\n",
      "====> Epoch: 28 Average loss: 271071.6379\n",
      "====> Test set loss: 830.7226\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 270026.000000\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 269465.562500\n",
      "====> Epoch: 29 Average loss: 270466.9111\n",
      "====> Test set loss: 749.3937\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 269425.812500\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 268880.562500\n",
      "====> Epoch: 30 Average loss: 269874.5263\n",
      "====> Test set loss: 682.1632\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 268846.312500\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 268319.500000\n",
      "====> Epoch: 31 Average loss: 269301.4760\n",
      "====> Test set loss: 624.1573\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 268282.468750\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 267760.312500\n",
      "====> Epoch: 32 Average loss: 268740.6827\n",
      "====> Test set loss: 577.5678\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 267727.937500\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 267232.437500\n",
      "====> Epoch: 33 Average loss: 268196.0151\n",
      "====> Test set loss: 538.4759\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 267188.437500\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 266720.562500\n",
      "====> Epoch: 34 Average loss: 267668.9871\n",
      "====> Test set loss: 507.9861\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 266668.437500\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 266194.718750\n",
      "====> Epoch: 35 Average loss: 267150.2881\n",
      "====> Test set loss: 482.4330\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 266154.781250\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 265700.625000\n",
      "====> Epoch: 36 Average loss: 266637.6794\n",
      "====> Test set loss: 463.2444\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 265648.343750\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 265192.281250\n",
      "====> Epoch: 37 Average loss: 266132.4233\n",
      "====> Test set loss: 446.0347\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 265148.718750\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 264680.718750\n",
      "====> Epoch: 38 Average loss: 265629.3712\n",
      "====> Test set loss: 432.0144\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 264644.968750\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 264180.656250\n",
      "====> Epoch: 39 Average loss: 265128.8519\n",
      "====> Test set loss: 420.6013\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 264152.125000\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 263695.781250\n",
      "====> Epoch: 40 Average loss: 264633.2552\n",
      "====> Test set loss: 411.2238\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 263651.562500\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 263199.843750\n",
      "====> Epoch: 41 Average loss: 264136.4692\n",
      "====> Test set loss: 403.9614\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 263165.000000\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 262702.250000\n",
      "====> Epoch: 42 Average loss: 263640.4094\n",
      "====> Test set loss: 397.6882\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 262667.218750\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 262205.281250\n",
      "====> Epoch: 43 Average loss: 263142.9929\n",
      "====> Test set loss: 392.4237\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 262168.375000\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 261699.984375\n",
      "====> Epoch: 44 Average loss: 262642.7619\n",
      "====> Test set loss: 387.5252\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 261668.390625\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 261206.812500\n",
      "====> Epoch: 45 Average loss: 262141.3134\n",
      "====> Test set loss: 383.4315\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 261166.765625\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 260704.984375\n",
      "====> Epoch: 46 Average loss: 261640.2849\n",
      "====> Test set loss: 380.2002\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 260663.078125\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 260208.312500\n",
      "====> Epoch: 47 Average loss: 261138.9259\n",
      "====> Test set loss: 375.1862\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 260170.843750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 259707.218750\n",
      "====> Epoch: 48 Average loss: 260639.6187\n",
      "====> Test set loss: 371.9362\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 259675.703125\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 259217.968750\n",
      "====> Epoch: 49 Average loss: 260142.5449\n",
      "====> Test set loss: 368.6944\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 259172.015625\n",
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 258720.218750\n",
      "====> Epoch: 50 Average loss: 259647.8102\n",
      "====> Test set loss: 365.6455\n",
      "Train Epoch: 51 [0/6893 (0%)]\tLoss: 258691.390625\n",
      "Train Epoch: 51 [6400/6893 (1%)]\tLoss: 258240.359375\n",
      "====> Epoch: 51 Average loss: 259155.0447\n",
      "====> Test set loss: 361.7382\n",
      "Train Epoch: 52 [0/6893 (0%)]\tLoss: 258196.781250\n",
      "Train Epoch: 52 [6400/6893 (1%)]\tLoss: 257745.359375\n",
      "====> Epoch: 52 Average loss: 258665.6122\n",
      "====> Test set loss: 358.3322\n",
      "Train Epoch: 53 [0/6893 (0%)]\tLoss: 257709.796875\n",
      "Train Epoch: 53 [6400/6893 (1%)]\tLoss: 257261.218750\n",
      "====> Epoch: 53 Average loss: 258177.3496\n",
      "====> Test set loss: 355.6372\n",
      "Train Epoch: 54 [0/6893 (0%)]\tLoss: 257217.718750\n",
      "Train Epoch: 54 [6400/6893 (1%)]\tLoss: 256769.578125\n",
      "====> Epoch: 54 Average loss: 257687.3292\n",
      "====> Test set loss: 351.6836\n",
      "Train Epoch: 55 [0/6893 (0%)]\tLoss: 256731.953125\n",
      "Train Epoch: 55 [6400/6893 (1%)]\tLoss: 256286.625000\n",
      "====> Epoch: 55 Average loss: 257196.1934\n",
      "====> Test set loss: 349.0735\n",
      "Train Epoch: 56 [0/6893 (0%)]\tLoss: 256250.734375\n",
      "Train Epoch: 56 [6400/6893 (1%)]\tLoss: 255794.296875\n",
      "====> Epoch: 56 Average loss: 256707.7280\n",
      "====> Test set loss: 345.7457\n",
      "Train Epoch: 57 [0/6893 (0%)]\tLoss: 255761.109375\n",
      "Train Epoch: 57 [6400/6893 (1%)]\tLoss: 255304.593750\n",
      "====> Epoch: 57 Average loss: 256217.2886\n",
      "====> Test set loss: 342.3300\n",
      "Train Epoch: 58 [0/6893 (0%)]\tLoss: 255274.859375\n",
      "Train Epoch: 58 [6400/6893 (1%)]\tLoss: 254810.156250\n",
      "====> Epoch: 58 Average loss: 255724.5533\n",
      "====> Test set loss: 339.3044\n",
      "Train Epoch: 59 [0/6893 (0%)]\tLoss: 254779.546875\n",
      "Train Epoch: 59 [6400/6893 (1%)]\tLoss: 254311.218750\n",
      "====> Epoch: 59 Average loss: 255230.8274\n",
      "====> Test set loss: 336.8480\n",
      "Train Epoch: 60 [0/6893 (0%)]\tLoss: 254276.281250\n",
      "Train Epoch: 60 [6400/6893 (1%)]\tLoss: 253824.328125\n",
      "====> Epoch: 60 Average loss: 254735.3811\n",
      "====> Test set loss: 332.8362\n",
      "Train Epoch: 61 [0/6893 (0%)]\tLoss: 253788.843750\n",
      "Train Epoch: 61 [6400/6893 (1%)]\tLoss: 253351.125000\n",
      "====> Epoch: 61 Average loss: 254248.1660\n",
      "====> Test set loss: 330.0283\n",
      "Train Epoch: 62 [0/6893 (0%)]\tLoss: 253306.250000\n",
      "Train Epoch: 62 [6400/6893 (1%)]\tLoss: 252858.578125\n",
      "====> Epoch: 62 Average loss: 253767.0123\n",
      "====> Test set loss: 326.5204\n",
      "Train Epoch: 63 [0/6893 (0%)]\tLoss: 252822.656250\n",
      "Train Epoch: 63 [6400/6893 (1%)]\tLoss: 252383.015625\n",
      "====> Epoch: 63 Average loss: 253284.0316\n",
      "====> Test set loss: 323.1960\n",
      "Train Epoch: 64 [0/6893 (0%)]\tLoss: 252353.359375\n",
      "Train Epoch: 64 [6400/6893 (1%)]\tLoss: 251904.093750\n",
      "====> Epoch: 64 Average loss: 252800.7612\n",
      "====> Test set loss: 320.3265\n",
      "Train Epoch: 65 [0/6893 (0%)]\tLoss: 251857.781250\n",
      "Train Epoch: 65 [6400/6893 (1%)]\tLoss: 251419.015625\n",
      "====> Epoch: 65 Average loss: 252320.6733\n",
      "====> Test set loss: 317.4474\n",
      "Train Epoch: 66 [0/6893 (0%)]\tLoss: 251387.093750\n",
      "Train Epoch: 66 [6400/6893 (1%)]\tLoss: 250959.078125\n",
      "====> Epoch: 66 Average loss: 251847.5214\n",
      "====> Test set loss: 314.9326\n",
      "Train Epoch: 67 [0/6893 (0%)]\tLoss: 250916.968750\n",
      "Train Epoch: 67 [6400/6893 (1%)]\tLoss: 250484.671875\n",
      "====> Epoch: 67 Average loss: 251378.7168\n",
      "====> Test set loss: 311.3616\n",
      "Train Epoch: 68 [0/6893 (0%)]\tLoss: 250452.296875\n",
      "Train Epoch: 68 [6400/6893 (1%)]\tLoss: 250024.593750\n",
      "====> Epoch: 68 Average loss: 250911.4583\n",
      "====> Test set loss: 307.2372\n",
      "Train Epoch: 69 [0/6893 (0%)]\tLoss: 249988.484375\n",
      "Train Epoch: 69 [6400/6893 (1%)]\tLoss: 249556.265625\n",
      "====> Epoch: 69 Average loss: 250445.8095\n",
      "====> Test set loss: 304.8422\n",
      "Train Epoch: 70 [0/6893 (0%)]\tLoss: 249525.265625\n",
      "Train Epoch: 70 [6400/6893 (1%)]\tLoss: 249089.421875\n",
      "====> Epoch: 70 Average loss: 249979.5317\n",
      "====> Test set loss: 302.3410\n",
      "Train Epoch: 71 [0/6893 (0%)]\tLoss: 249053.375000\n",
      "Train Epoch: 71 [6400/6893 (1%)]\tLoss: 248628.437500\n",
      "====> Epoch: 71 Average loss: 249511.7566\n",
      "====> Test set loss: 299.1706\n",
      "Train Epoch: 72 [0/6893 (0%)]\tLoss: 248591.718750\n",
      "Train Epoch: 72 [6400/6893 (1%)]\tLoss: 248166.171875\n",
      "====> Epoch: 72 Average loss: 249047.3109\n",
      "====> Test set loss: 296.9762\n",
      "Train Epoch: 73 [0/6893 (0%)]\tLoss: 248132.531250\n",
      "Train Epoch: 73 [6400/6893 (1%)]\tLoss: 247698.578125\n",
      "====> Epoch: 73 Average loss: 248582.2109\n",
      "====> Test set loss: 292.7602\n",
      "Train Epoch: 74 [0/6893 (0%)]\tLoss: 247661.687500\n",
      "Train Epoch: 74 [6400/6893 (1%)]\tLoss: 247232.421875\n",
      "====> Epoch: 74 Average loss: 248113.7152\n",
      "====> Test set loss: 289.9405\n",
      "Train Epoch: 75 [0/6893 (0%)]\tLoss: 247195.453125\n",
      "Train Epoch: 75 [6400/6893 (1%)]\tLoss: 246760.250000\n",
      "====> Epoch: 75 Average loss: 247642.8153\n",
      "====> Test set loss: 286.9949\n",
      "Train Epoch: 76 [0/6893 (0%)]\tLoss: 246723.218750\n",
      "Train Epoch: 76 [6400/6893 (1%)]\tLoss: 246285.421875\n",
      "====> Epoch: 76 Average loss: 247168.4011\n",
      "====> Test set loss: 283.3910\n",
      "Train Epoch: 77 [0/6893 (0%)]\tLoss: 246249.171875\n",
      "Train Epoch: 77 [6400/6893 (1%)]\tLoss: 245813.796875\n",
      "====> Epoch: 77 Average loss: 246690.7854\n",
      "====> Test set loss: 281.3822\n",
      "Train Epoch: 78 [0/6893 (0%)]\tLoss: 245774.421875\n",
      "Train Epoch: 78 [6400/6893 (1%)]\tLoss: 245328.765625\n",
      "====> Epoch: 78 Average loss: 246211.0707\n",
      "====> Test set loss: 277.0224\n",
      "Train Epoch: 79 [0/6893 (0%)]\tLoss: 245295.093750\n",
      "Train Epoch: 79 [6400/6893 (1%)]\tLoss: 244848.546875\n",
      "====> Epoch: 79 Average loss: 245730.3293\n",
      "====> Test set loss: 275.6832\n",
      "Train Epoch: 80 [0/6893 (0%)]\tLoss: 244811.984375\n",
      "Train Epoch: 80 [6400/6893 (1%)]\tLoss: 244375.687500\n",
      "====> Epoch: 80 Average loss: 245248.3179\n",
      "====> Test set loss: 272.1286\n",
      "Train Epoch: 81 [0/6893 (0%)]\tLoss: 244330.515625\n",
      "Train Epoch: 81 [6400/6893 (1%)]\tLoss: 243890.968750\n",
      "====> Epoch: 81 Average loss: 244770.2630\n",
      "====> Test set loss: 268.9938\n",
      "Train Epoch: 82 [0/6893 (0%)]\tLoss: 243857.187500\n",
      "Train Epoch: 82 [6400/6893 (1%)]\tLoss: 243421.171875\n",
      "====> Epoch: 82 Average loss: 244295.4333\n",
      "====> Test set loss: 266.7829\n",
      "Train Epoch: 83 [0/6893 (0%)]\tLoss: 243384.781250\n",
      "Train Epoch: 83 [6400/6893 (1%)]\tLoss: 242961.406250\n",
      "====> Epoch: 83 Average loss: 243825.8733\n",
      "====> Test set loss: 262.4974\n",
      "Train Epoch: 84 [0/6893 (0%)]\tLoss: 242927.546875\n",
      "Train Epoch: 84 [6400/6893 (1%)]\tLoss: 242487.781250\n",
      "====> Epoch: 84 Average loss: 243358.7130\n",
      "====> Test set loss: 261.8634\n",
      "Train Epoch: 85 [0/6893 (0%)]\tLoss: 242457.671875\n",
      "Train Epoch: 85 [6400/6893 (1%)]\tLoss: 242022.156250\n",
      "====> Epoch: 85 Average loss: 242891.3547\n",
      "====> Test set loss: 257.0340\n",
      "Train Epoch: 86 [0/6893 (0%)]\tLoss: 241989.906250\n",
      "Train Epoch: 86 [6400/6893 (1%)]\tLoss: 241565.406250\n",
      "====> Epoch: 86 Average loss: 242426.8176\n",
      "====> Test set loss: 254.2448\n",
      "Train Epoch: 87 [0/6893 (0%)]\tLoss: 241523.421875\n",
      "Train Epoch: 87 [6400/6893 (1%)]\tLoss: 241100.968750\n",
      "====> Epoch: 87 Average loss: 241966.0413\n",
      "====> Test set loss: 251.7616\n",
      "Train Epoch: 88 [0/6893 (0%)]\tLoss: 241065.015625\n",
      "Train Epoch: 88 [6400/6893 (1%)]\tLoss: 240651.109375\n",
      "====> Epoch: 88 Average loss: 241510.0856\n",
      "====> Test set loss: 249.1956\n",
      "Train Epoch: 89 [0/6893 (0%)]\tLoss: 240615.109375\n",
      "Train Epoch: 89 [6400/6893 (1%)]\tLoss: 240212.453125\n",
      "====> Epoch: 89 Average loss: 241062.5829\n",
      "====> Test set loss: 247.1605\n",
      "Train Epoch: 90 [0/6893 (0%)]\tLoss: 240180.750000\n",
      "Train Epoch: 90 [6400/6893 (1%)]\tLoss: 239759.062500\n",
      "====> Epoch: 90 Average loss: 240615.7170\n",
      "====> Test set loss: 243.6369\n",
      "Train Epoch: 91 [0/6893 (0%)]\tLoss: 239728.828125\n",
      "Train Epoch: 91 [6400/6893 (1%)]\tLoss: 239323.937500\n",
      "====> Epoch: 91 Average loss: 240169.8838\n",
      "====> Test set loss: 240.3930\n",
      "Train Epoch: 92 [0/6893 (0%)]\tLoss: 239291.843750\n",
      "Train Epoch: 92 [6400/6893 (1%)]\tLoss: 238875.953125\n",
      "====> Epoch: 92 Average loss: 239727.3910\n",
      "====> Test set loss: 238.6355\n",
      "Train Epoch: 93 [0/6893 (0%)]\tLoss: 238844.375000\n",
      "Train Epoch: 93 [6400/6893 (1%)]\tLoss: 238439.093750\n",
      "====> Epoch: 93 Average loss: 239284.5413\n",
      "====> Test set loss: 235.5494\n",
      "Train Epoch: 94 [0/6893 (0%)]\tLoss: 238404.875000\n",
      "Train Epoch: 94 [6400/6893 (1%)]\tLoss: 237998.453125\n",
      "====> Epoch: 94 Average loss: 238841.4355\n",
      "====> Test set loss: 232.9097\n",
      "Train Epoch: 95 [0/6893 (0%)]\tLoss: 237963.468750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 95 [6400/6893 (1%)]\tLoss: 237558.609375\n",
      "====> Epoch: 95 Average loss: 238401.1867\n",
      "====> Test set loss: 230.5750\n",
      "Train Epoch: 96 [0/6893 (0%)]\tLoss: 237523.968750\n",
      "Train Epoch: 96 [6400/6893 (1%)]\tLoss: 237109.718750\n",
      "====> Epoch: 96 Average loss: 237960.2289\n",
      "====> Test set loss: 227.8121\n",
      "Train Epoch: 97 [0/6893 (0%)]\tLoss: 237081.062500\n",
      "Train Epoch: 97 [6400/6893 (1%)]\tLoss: 236675.171875\n",
      "====> Epoch: 97 Average loss: 237515.8207\n",
      "====> Test set loss: 222.8560\n",
      "Train Epoch: 98 [0/6893 (0%)]\tLoss: 236642.859375\n",
      "Train Epoch: 98 [6400/6893 (1%)]\tLoss: 236232.578125\n",
      "====> Epoch: 98 Average loss: 237070.5374\n",
      "====> Test set loss: 221.5925\n",
      "Train Epoch: 99 [0/6893 (0%)]\tLoss: 236200.437500\n",
      "Train Epoch: 99 [6400/6893 (1%)]\tLoss: 235779.703125\n",
      "====> Epoch: 99 Average loss: 236628.4626\n",
      "====> Test set loss: 218.0541\n",
      "Train Epoch: 100 [0/6893 (0%)]\tLoss: 235758.437500\n",
      "Train Epoch: 100 [6400/6893 (1%)]\tLoss: 235353.437500\n",
      "====> Epoch: 100 Average loss: 236189.0789\n",
      "====> Test set loss: 216.1482\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_l1(train_data, model_l1_diag, model_l1_optimizer, epoch)\n",
    "        test(test_data, model_l1_diag, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  4,  16, 113,  11,   0,   9,  46, 282,  19,   0]),\n",
       " array([1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02,\n",
       "        1.e-01, 1.e+00, 1.e+01]))"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [10**(-i) for i in range(10)]\n",
    "bins.reverse()\n",
    "bins += [10]\n",
    "np.histogram(model_l1_diag.diag.abs().clone().detach().cpu().numpy(), bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = model_l1_diag(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = model_l1_diag(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4580, 0.3896, 0.3820, 0.4025, 0.1013, 0.5150, 0.3432, 0.4227, 0.3681,\n",
       "        0.4334, 0.1252, 0.2932, 0.2572, 0.4064, 0.2911, 0.2485, 0.3780, 0.1577,\n",
       "        0.2568, 0.2896, 0.1496, 0.2722, 0.1206, 0.0000, 0.2027, 0.0000, 0.3424,\n",
       "        0.2577, 0.2906, 0.3642, 0.2307, 0.1515, 0.2051, 0.1422, 0.1610, 0.4359,\n",
       "        0.0000, 0.2985, 0.2702, 0.1673, 0.0000, 0.2264, 0.0000, 0.2401, 0.2407,\n",
       "        0.2595, 0.2932, 0.4037, 0.3758, 0.5026, 0.1322, 0.3051, 0.1443, 0.0000,\n",
       "        0.1700, 0.0000, 0.1573, 0.1483, 0.1660, 0.4076, 0.2233, 0.1660, 0.5441,\n",
       "        0.0000, 0.0000, 0.0000, 0.2407, 0.1673, 0.0000, 0.0000, 0.1920, 0.3115,\n",
       "        0.0000, 0.2662, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.3559, 0.0000, 0.0000, 0.1781, 0.0000, 0.0000, 0.3492, 0.0000, 0.0000,\n",
       "        0.3667, 0.2613, 0.1601, 0.4393, 0.3758, 0.0000, 0.1854, 0.3400, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.3297, 0.3372, 0.0000, 0.1560, 0.4893,\n",
       "        0.1380, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2127, 0.0000, 0.3285,\n",
       "        0.1854, 0.0000, 0.3104, 0.3801, 0.2277, 0.0000, 0.0000, 0.0000, 0.9062,\n",
       "        0.2103, 0.0000, 0.3733, 0.3230, 0.0000, 0.2447, 0.0000, 0.0000, 0.2398,\n",
       "        0.3230, 0.0000, 0.0000, 0.4362, 0.1800, 0.3170, 0.0000, 0.3413, 0.2103,\n",
       "        0.2869, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2211, 0.0000,\n",
       "        0.0000, 0.2626, 0.2702, 0.0000, 0.1961, 0.3146, 0.0000, 0.0000, 0.3504,\n",
       "        0.2058, 0.0000, 0.2080, 0.0000, 0.0000, 0.0000, 0.0000, 0.1854, 0.0000,\n",
       "        0.3801, 0.1843, 0.2354, 0.0000, 0.0000, 0.4035, 0.0000, 0.0000, 0.0000,\n",
       "        0.2441, 0.0000, 0.0000, 0.3475, 0.0000, 0.3801, 0.0000, 0.0000, 0.0000,\n",
       "        0.1810, 0.2702, 0.0000, 0.0000, 0.4421, 0.4163, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.3414, 0.2560, 0.0000, 0.0000, 0.1326, 0.0000, 0.2447, 0.0000,\n",
       "        0.0000, 0.1722, 0.0000, 0.0000, 0.7985, 0.3551, 0.2560, 0.0000, 0.3693,\n",
       "        0.0000, 0.2211, 0.0000, 0.0000, 0.0000, 0.2211, 0.3372, 0.0000, 0.0000,\n",
       "        0.1791, 0.0000, 0.0000, 0.0000, 0.2789, 0.2626, 0.0000, 0.2181, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.3962, 0.2891, 0.2354, 0.0000, 0.1892, 0.2789, 0.2891,\n",
       "        0.0000, 0.0000, 0.2354, 0.0000, 0.2398, 0.0000, 0.2000, 0.0000, 0.0000,\n",
       "        0.0000, 0.1387, 0.3931, 0.0000, 0.0000, 0.0000, 0.1286, 0.0000, 0.0000,\n",
       "        0.1317, 0.0000, 0.0000, 0.0000, 0.4771, 0.0000, 0.0000, 0.0000, 0.2626,\n",
       "        0.2838, 0.3667, 0.3010, 0.0000, 0.2729, 0.2891, 0.0000, 0.2702, 0.0000,\n",
       "        0.0000, 0.3155, 0.0000, 0.0000, 0.0000, 0.2500, 0.0000, 0.2626, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.3962, 0.0000, 0.0000, 0.2702, 0.8150,\n",
       "        0.2354, 0.0000, 0.0000, 0.0000, 0.2398, 0.2242, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2277, 0.0000, 0.0000, 0.0000, 0.0000, 0.2626, 0.0000, 0.0000,\n",
       "        0.2604, 0.2702, 0.0000, 0.2891, 0.0000, 0.0000, 0.2447, 0.2127, 0.0000,\n",
       "        0.2447, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2626, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5646, 0.0000, 0.0000, 0.2211,\n",
       "        0.0000, 0.2447, 0.0000, 0.0000, 0.0000, 0.2891, 0.0000, 0.2447, 0.0000,\n",
       "        0.0000, 0.2626, 0.0000, 0.0000, 0.0000, 0.2242, 0.0000, 0.8314, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1492, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2579, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2789, 0.0000, 0.0000, 0.0000, 0.2789, 0.0000,\n",
       "        0.0000, 0.0000, 0.2702, 0.2314, 0.0000, 0.8635, 0.2560, 0.0000, 0.0000,\n",
       "        0.2560, 0.0000, 0.3066, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.3155, 0.0000, 0.0000, 0.0000, 0.0000, 0.8630, 0.0000, 0.0000, 0.4796,\n",
       "        0.0000, 0.3155, 0.0000, 0.0000, 0.0000, 0.0000, 0.2560, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.2891, 0.4307, 0.4307, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.4421, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.1854, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.3010, 0.2127, 0.0000, 0.0000, 0.2058, 0.0000, 0.4771, 0.0000, 0.0000,\n",
       "        0.7823, 0.0000, 0.2626, 0.0000, 0.3562, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2398, 0.3562, 0.0000, 0.2421, 0.2891, 0.2626,\n",
       "        0.0000, 0.0000, 0.2354, 0.2314, 0.2277], device='cuda:0')"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(313, device='cuda:0')"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.sum(test_pred[0,:] != 0))\n",
    "print(torch.sum(test_data[0,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(255, device='cuda:0')"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1744, device='cuda:0')\n",
      "tensor(0.1733, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try Pretrained VAE and then gumble trick with it\n",
    "\n",
    "Then try joint training VAE and Gumbel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla VAE model\n",
    "# try with gaussian decoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        #self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "        self.fc5 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        #self.decoder = nn.Sequential()\n",
    "\n",
    "    def encode(self, x):\n",
    "        #h1 = F.relu(self.fc1(x))\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):    \n",
    "        h = F.leaky_relu(self.fc3(z))\n",
    "        mu_x = F.leaky_relu(self.fc4(h))\n",
    "        #mu_x = self.fc4(h)\n",
    "        logvar_x = self.fc5(h)\n",
    "        return mu_x, logvar_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_latent, logvar_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu_latent, logvar_latent)\n",
    "        mu_x, logvar_x = self.decode(z)\n",
    "        return mu_x, logvar_x, mu_latent, logvar_latent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain VAE First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_vae = VAE(500, 200, 50)\n",
    "\n",
    "pretrain_vae.to(device)\n",
    "pretrain_vae_optimizer = torch.optim.Adam(pretrain_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        loss = loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 10350.126953\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 9280.757812\n",
      "====> Epoch: 1 Average loss: 9723.0692\n",
      "====> Test set loss: 9111.0543\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 9090.483398\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 8183.021973\n",
      "====> Epoch: 2 Average loss: 8584.3976\n",
      "====> Test set loss: 8067.8318\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 8080.266602\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 7185.016602\n",
      "====> Epoch: 3 Average loss: 7592.4997\n",
      "====> Test set loss: 7119.4775\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 7125.715820\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 6290.743164\n",
      "====> Epoch: 4 Average loss: 6670.1339\n",
      "====> Test set loss: 6220.7737\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 6292.028320\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 5365.541992\n",
      "====> Epoch: 5 Average loss: 5783.4901\n",
      "====> Test set loss: 5342.3373\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 5352.334961\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 4548.109863\n",
      "====> Epoch: 6 Average loss: 4917.3787\n",
      "====> Test set loss: 4499.3165\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 4485.455566\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 3748.694824\n",
      "====> Epoch: 7 Average loss: 4130.0728\n",
      "====> Test set loss: 3775.3139\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 3858.518066\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 3161.677734\n",
      "====> Epoch: 8 Average loss: 3457.4207\n",
      "====> Test set loss: 3143.2050\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 3062.237305\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 2612.469482\n",
      "====> Epoch: 9 Average loss: 2866.6327\n",
      "====> Test set loss: 2605.6959\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 2516.396973\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 2285.197021\n",
      "====> Epoch: 10 Average loss: 2393.1966\n",
      "====> Test set loss: 2195.6983\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 2117.356201\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 2062.501221\n",
      "====> Epoch: 11 Average loss: 2034.2954\n",
      "====> Test set loss: 1884.6161\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 1808.647583\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 1584.627197\n",
      "====> Epoch: 12 Average loss: 1763.4769\n",
      "====> Test set loss: 1652.5486\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 1637.470825\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 1499.136230\n",
      "====> Epoch: 13 Average loss: 1561.5275\n",
      "====> Test set loss: 1478.5119\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 1477.951172\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 1341.366455\n",
      "====> Epoch: 14 Average loss: 1409.4771\n",
      "====> Test set loss: 1347.3108\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 1575.022827\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 1244.660522\n",
      "====> Epoch: 15 Average loss: 1295.3369\n",
      "====> Test set loss: 1248.7182\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 1203.601685\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 1204.798828\n",
      "====> Epoch: 16 Average loss: 1209.3417\n",
      "====> Test set loss: 1174.1136\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 1195.206787\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 923.318237\n",
      "====> Epoch: 17 Average loss: 1143.1628\n",
      "====> Test set loss: 1116.3747\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 997.736328\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 1040.515137\n",
      "====> Epoch: 18 Average loss: 1091.2651\n",
      "====> Test set loss: 1069.3627\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 1155.659790\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 909.160645\n",
      "====> Epoch: 19 Average loss: 1048.2617\n",
      "====> Test set loss: 1030.3656\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 1226.425903\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 1090.373657\n",
      "====> Epoch: 20 Average loss: 1011.4537\n",
      "====> Test set loss: 995.5940\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 1112.186768\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 1051.722656\n",
      "====> Epoch: 21 Average loss: 979.0316\n",
      "====> Test set loss: 965.1258\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 998.949036\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 783.230469\n",
      "====> Epoch: 22 Average loss: 949.2341\n",
      "====> Test set loss: 936.7615\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 956.131104\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 854.618469\n",
      "====> Epoch: 23 Average loss: 921.9791\n",
      "====> Test set loss: 910.3967\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 872.217407\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 791.689575\n",
      "====> Epoch: 24 Average loss: 896.3490\n",
      "====> Test set loss: 885.5289\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 812.799316\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 925.897339\n",
      "====> Epoch: 25 Average loss: 872.1233\n",
      "====> Test set loss: 862.1426\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 883.873535\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 827.972290\n",
      "====> Epoch: 26 Average loss: 849.0450\n",
      "====> Test set loss: 839.8046\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 802.639709\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 745.546997\n",
      "====> Epoch: 27 Average loss: 827.2515\n",
      "====> Test set loss: 818.4976\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 870.415039\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 857.203735\n",
      "====> Epoch: 28 Average loss: 806.2847\n",
      "====> Test set loss: 797.9417\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 717.080383\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 711.068604\n",
      "====> Epoch: 29 Average loss: 786.1808\n",
      "====> Test set loss: 778.6640\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 865.380005\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 790.118774\n",
      "====> Epoch: 30 Average loss: 766.9515\n",
      "====> Test set loss: 759.7852\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 749.072388\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 692.740723\n",
      "====> Epoch: 31 Average loss: 748.8164\n",
      "====> Test set loss: 742.3269\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 808.441833\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 735.923950\n",
      "====> Epoch: 32 Average loss: 731.2683\n",
      "====> Test set loss: 724.4411\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 802.561401\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 715.906006\n",
      "====> Epoch: 33 Average loss: 713.9584\n",
      "====> Test set loss: 707.8053\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 738.001343\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 663.865112\n",
      "====> Epoch: 34 Average loss: 697.7439\n",
      "====> Test set loss: 692.4202\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 668.430237\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 729.530151\n",
      "====> Epoch: 35 Average loss: 681.8710\n",
      "====> Test set loss: 677.1624\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 653.427246\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 655.438049\n",
      "====> Epoch: 36 Average loss: 667.2305\n",
      "====> Test set loss: 662.4222\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 579.408447\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 790.097107\n",
      "====> Epoch: 37 Average loss: 652.7964\n",
      "====> Test set loss: 648.2873\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 624.096558\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 696.724365\n",
      "====> Epoch: 38 Average loss: 638.7479\n",
      "====> Test set loss: 635.4479\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 615.603455\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 562.430725\n",
      "====> Epoch: 39 Average loss: 625.7603\n",
      "====> Test set loss: 621.3476\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 656.860229\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 574.798340\n",
      "====> Epoch: 40 Average loss: 612.8632\n",
      "====> Test set loss: 609.5124\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 547.015503\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 599.822815\n",
      "====> Epoch: 41 Average loss: 600.4946\n",
      "====> Test set loss: 597.2259\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 558.034912\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 652.729370\n",
      "====> Epoch: 42 Average loss: 588.6282\n",
      "====> Test set loss: 586.2483\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 583.631531\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 587.418701\n",
      "====> Epoch: 43 Average loss: 576.8489\n",
      "====> Test set loss: 574.7683\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 583.241516\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 589.760193\n",
      "====> Epoch: 44 Average loss: 566.2619\n",
      "====> Test set loss: 563.8058\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 529.639404\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 549.579407\n",
      "====> Epoch: 45 Average loss: 555.4912\n",
      "====> Test set loss: 553.4968\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 513.298401\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 575.549316\n",
      "====> Epoch: 46 Average loss: 544.9471\n",
      "====> Test set loss: 544.0488\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 554.353638\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 528.184509\n",
      "====> Epoch: 47 Average loss: 535.4006\n",
      "====> Test set loss: 533.6215\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 561.067383\n",
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 523.231995\n",
      "====> Epoch: 48 Average loss: 525.8263\n",
      "====> Test set loss: 524.3032\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 582.115051\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 534.344788\n",
      "====> Epoch: 49 Average loss: 516.4354\n",
      "====> Test set loss: 515.2152\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 532.092163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 494.331543\n",
      "====> Epoch: 50 Average loss: 507.5651\n",
      "====> Test set loss: 506.6712\n",
      "Train Epoch: 51 [0/6893 (0%)]\tLoss: 541.547852\n",
      "Train Epoch: 51 [6400/6893 (1%)]\tLoss: 469.752441\n",
      "====> Epoch: 51 Average loss: 498.8962\n",
      "====> Test set loss: 497.8975\n",
      "Train Epoch: 52 [0/6893 (0%)]\tLoss: 478.211456\n",
      "Train Epoch: 52 [6400/6893 (1%)]\tLoss: 475.971069\n",
      "====> Epoch: 52 Average loss: 490.2321\n",
      "====> Test set loss: 489.6773\n",
      "Train Epoch: 53 [0/6893 (0%)]\tLoss: 477.898804\n",
      "Train Epoch: 53 [6400/6893 (1%)]\tLoss: 502.425720\n",
      "====> Epoch: 53 Average loss: 482.2407\n",
      "====> Test set loss: 481.7344\n",
      "Train Epoch: 54 [0/6893 (0%)]\tLoss: 502.726562\n",
      "Train Epoch: 54 [6400/6893 (1%)]\tLoss: 463.151154\n",
      "====> Epoch: 54 Average loss: 474.1671\n",
      "====> Test set loss: 473.3923\n",
      "Train Epoch: 55 [0/6893 (0%)]\tLoss: 471.325439\n",
      "Train Epoch: 55 [6400/6893 (1%)]\tLoss: 481.523254\n",
      "====> Epoch: 55 Average loss: 466.6507\n",
      "====> Test set loss: 466.1312\n",
      "Train Epoch: 56 [0/6893 (0%)]\tLoss: 465.814026\n",
      "Train Epoch: 56 [6400/6893 (1%)]\tLoss: 443.639404\n",
      "====> Epoch: 56 Average loss: 459.4624\n",
      "====> Test set loss: 459.1282\n",
      "Train Epoch: 57 [0/6893 (0%)]\tLoss: 498.499481\n",
      "Train Epoch: 57 [6400/6893 (1%)]\tLoss: 441.701630\n",
      "====> Epoch: 57 Average loss: 452.0656\n",
      "====> Test set loss: 452.2197\n",
      "Train Epoch: 58 [0/6893 (0%)]\tLoss: 422.567322\n",
      "Train Epoch: 58 [6400/6893 (1%)]\tLoss: 432.009888\n",
      "====> Epoch: 58 Average loss: 445.3837\n",
      "====> Test set loss: 445.2364\n",
      "Train Epoch: 59 [0/6893 (0%)]\tLoss: 447.293640\n",
      "Train Epoch: 59 [6400/6893 (1%)]\tLoss: 431.931885\n",
      "====> Epoch: 59 Average loss: 438.5483\n",
      "====> Test set loss: 438.1660\n",
      "Train Epoch: 60 [0/6893 (0%)]\tLoss: 431.591400\n",
      "Train Epoch: 60 [6400/6893 (1%)]\tLoss: 424.023743\n",
      "====> Epoch: 60 Average loss: 432.0784\n",
      "====> Test set loss: 432.5068\n",
      "Train Epoch: 61 [0/6893 (0%)]\tLoss: 422.394745\n",
      "Train Epoch: 61 [6400/6893 (1%)]\tLoss: 412.114014\n",
      "====> Epoch: 61 Average loss: 425.7205\n",
      "====> Test set loss: 426.1577\n",
      "Train Epoch: 62 [0/6893 (0%)]\tLoss: 444.525238\n",
      "Train Epoch: 62 [6400/6893 (1%)]\tLoss: 435.829437\n",
      "====> Epoch: 62 Average loss: 419.1913\n",
      "====> Test set loss: 418.2857\n",
      "Train Epoch: 63 [0/6893 (0%)]\tLoss: 468.766724\n",
      "Train Epoch: 63 [6400/6893 (1%)]\tLoss: 398.836304\n",
      "====> Epoch: 63 Average loss: 413.0253\n",
      "====> Test set loss: 413.8060\n",
      "Train Epoch: 64 [0/6893 (0%)]\tLoss: 398.084717\n",
      "Train Epoch: 64 [6400/6893 (1%)]\tLoss: 410.897736\n",
      "====> Epoch: 64 Average loss: 407.2486\n",
      "====> Test set loss: 407.7740\n",
      "Train Epoch: 65 [0/6893 (0%)]\tLoss: 406.043243\n",
      "Train Epoch: 65 [6400/6893 (1%)]\tLoss: 395.419678\n",
      "====> Epoch: 65 Average loss: 401.1540\n",
      "====> Test set loss: 401.5207\n",
      "Train Epoch: 66 [0/6893 (0%)]\tLoss: 400.561157\n",
      "Train Epoch: 66 [6400/6893 (1%)]\tLoss: 391.645782\n",
      "====> Epoch: 66 Average loss: 395.9214\n",
      "====> Test set loss: 395.8838\n",
      "Train Epoch: 67 [0/6893 (0%)]\tLoss: 393.533386\n",
      "Train Epoch: 67 [6400/6893 (1%)]\tLoss: 387.117889\n",
      "====> Epoch: 67 Average loss: 390.1464\n",
      "====> Test set loss: 390.8329\n",
      "Train Epoch: 68 [0/6893 (0%)]\tLoss: 386.107483\n",
      "Train Epoch: 68 [6400/6893 (1%)]\tLoss: 385.529175\n",
      "====> Epoch: 68 Average loss: 384.3768\n",
      "====> Test set loss: 384.9969\n",
      "Train Epoch: 69 [0/6893 (0%)]\tLoss: 385.580719\n",
      "Train Epoch: 69 [6400/6893 (1%)]\tLoss: 396.247955\n",
      "====> Epoch: 69 Average loss: 379.3147\n",
      "====> Test set loss: 378.7874\n",
      "Train Epoch: 70 [0/6893 (0%)]\tLoss: 366.947327\n",
      "Train Epoch: 70 [6400/6893 (1%)]\tLoss: 377.918427\n",
      "====> Epoch: 70 Average loss: 373.5724\n",
      "====> Test set loss: 374.2696\n",
      "Train Epoch: 71 [0/6893 (0%)]\tLoss: 385.706787\n",
      "Train Epoch: 71 [6400/6893 (1%)]\tLoss: 363.398468\n",
      "====> Epoch: 71 Average loss: 368.9824\n",
      "====> Test set loss: 368.8638\n",
      "Train Epoch: 72 [0/6893 (0%)]\tLoss: 369.466766\n",
      "Train Epoch: 72 [6400/6893 (1%)]\tLoss: 370.454437\n",
      "====> Epoch: 72 Average loss: 363.9588\n",
      "====> Test set loss: 364.3980\n",
      "Train Epoch: 73 [0/6893 (0%)]\tLoss: 384.628296\n",
      "Train Epoch: 73 [6400/6893 (1%)]\tLoss: 354.995850\n",
      "====> Epoch: 73 Average loss: 358.3601\n",
      "====> Test set loss: 358.5388\n",
      "Train Epoch: 74 [0/6893 (0%)]\tLoss: 342.716553\n",
      "Train Epoch: 74 [6400/6893 (1%)]\tLoss: 348.315887\n",
      "====> Epoch: 74 Average loss: 353.8821\n",
      "====> Test set loss: 353.9902\n",
      "Train Epoch: 75 [0/6893 (0%)]\tLoss: 356.985168\n",
      "Train Epoch: 75 [6400/6893 (1%)]\tLoss: 339.866882\n",
      "====> Epoch: 75 Average loss: 348.9307\n",
      "====> Test set loss: 349.8495\n",
      "Train Epoch: 76 [0/6893 (0%)]\tLoss: 361.556519\n",
      "Train Epoch: 76 [6400/6893 (1%)]\tLoss: 341.133606\n",
      "====> Epoch: 76 Average loss: 343.9592\n",
      "====> Test set loss: 343.7965\n",
      "Train Epoch: 77 [0/6893 (0%)]\tLoss: 334.781677\n",
      "Train Epoch: 77 [6400/6893 (1%)]\tLoss: 327.452148\n",
      "====> Epoch: 77 Average loss: 339.4389\n",
      "====> Test set loss: 340.1077\n",
      "Train Epoch: 78 [0/6893 (0%)]\tLoss: 335.880920\n",
      "Train Epoch: 78 [6400/6893 (1%)]\tLoss: 330.660797\n",
      "====> Epoch: 78 Average loss: 334.8134\n",
      "====> Test set loss: 335.8967\n",
      "Train Epoch: 79 [0/6893 (0%)]\tLoss: 343.149170\n",
      "Train Epoch: 79 [6400/6893 (1%)]\tLoss: 328.550476\n",
      "====> Epoch: 79 Average loss: 329.9432\n",
      "====> Test set loss: 330.8667\n",
      "Train Epoch: 80 [0/6893 (0%)]\tLoss: 335.642822\n",
      "Train Epoch: 80 [6400/6893 (1%)]\tLoss: 332.732452\n",
      "====> Epoch: 80 Average loss: 326.3132\n",
      "====> Test set loss: 325.5591\n",
      "Train Epoch: 81 [0/6893 (0%)]\tLoss: 326.927429\n",
      "Train Epoch: 81 [6400/6893 (1%)]\tLoss: 306.403870\n",
      "====> Epoch: 81 Average loss: 321.2955\n",
      "====> Test set loss: 321.6618\n",
      "Train Epoch: 82 [0/6893 (0%)]\tLoss: 317.457611\n",
      "Train Epoch: 82 [6400/6893 (1%)]\tLoss: 324.768890\n",
      "====> Epoch: 82 Average loss: 316.5205\n",
      "====> Test set loss: 316.6209\n",
      "Train Epoch: 83 [0/6893 (0%)]\tLoss: 316.121246\n",
      "Train Epoch: 83 [6400/6893 (1%)]\tLoss: 308.497864\n",
      "====> Epoch: 83 Average loss: 312.5279\n",
      "====> Test set loss: 312.7899\n",
      "Train Epoch: 84 [0/6893 (0%)]\tLoss: 307.595428\n",
      "Train Epoch: 84 [6400/6893 (1%)]\tLoss: 300.733734\n",
      "====> Epoch: 84 Average loss: 308.2232\n",
      "====> Test set loss: 308.7819\n",
      "Train Epoch: 85 [0/6893 (0%)]\tLoss: 302.286926\n",
      "Train Epoch: 85 [6400/6893 (1%)]\tLoss: 303.007629\n",
      "====> Epoch: 85 Average loss: 304.6256\n",
      "====> Test set loss: 304.9729\n",
      "Train Epoch: 86 [0/6893 (0%)]\tLoss: 301.792053\n",
      "Train Epoch: 86 [6400/6893 (1%)]\tLoss: 293.358093\n",
      "====> Epoch: 86 Average loss: 299.9935\n",
      "====> Test set loss: 299.2211\n",
      "Train Epoch: 87 [0/6893 (0%)]\tLoss: 291.791962\n",
      "Train Epoch: 87 [6400/6893 (1%)]\tLoss: 282.073975\n",
      "====> Epoch: 87 Average loss: 295.4990\n",
      "====> Test set loss: 296.3750\n",
      "Train Epoch: 88 [0/6893 (0%)]\tLoss: 295.880615\n",
      "Train Epoch: 88 [6400/6893 (1%)]\tLoss: 290.468811\n",
      "====> Epoch: 88 Average loss: 291.5291\n",
      "====> Test set loss: 291.5882\n",
      "Train Epoch: 89 [0/6893 (0%)]\tLoss: 281.903595\n",
      "Train Epoch: 89 [6400/6893 (1%)]\tLoss: 272.705322\n",
      "====> Epoch: 89 Average loss: 287.6278\n",
      "====> Test set loss: 287.6081\n",
      "Train Epoch: 90 [0/6893 (0%)]\tLoss: 288.033905\n",
      "Train Epoch: 90 [6400/6893 (1%)]\tLoss: 288.453522\n",
      "====> Epoch: 90 Average loss: 283.4773\n",
      "====> Test set loss: 285.5738\n",
      "Train Epoch: 91 [0/6893 (0%)]\tLoss: 278.090637\n",
      "Train Epoch: 91 [6400/6893 (1%)]\tLoss: 284.833923\n",
      "====> Epoch: 91 Average loss: 279.7317\n",
      "====> Test set loss: 280.4115\n",
      "Train Epoch: 92 [0/6893 (0%)]\tLoss: 271.595215\n",
      "Train Epoch: 92 [6400/6893 (1%)]\tLoss: 270.542725\n",
      "====> Epoch: 92 Average loss: 275.8079\n",
      "====> Test set loss: 275.6618\n",
      "Train Epoch: 93 [0/6893 (0%)]\tLoss: 280.254242\n",
      "Train Epoch: 93 [6400/6893 (1%)]\tLoss: 275.123779\n",
      "====> Epoch: 93 Average loss: 272.4985\n",
      "====> Test set loss: 273.0813\n",
      "Train Epoch: 94 [0/6893 (0%)]\tLoss: 271.425018\n",
      "Train Epoch: 94 [6400/6893 (1%)]\tLoss: 269.044617\n",
      "====> Epoch: 94 Average loss: 268.5199\n",
      "====> Test set loss: 268.8386\n",
      "Train Epoch: 95 [0/6893 (0%)]\tLoss: 272.216003\n",
      "Train Epoch: 95 [6400/6893 (1%)]\tLoss: 274.381775\n",
      "====> Epoch: 95 Average loss: 265.0366\n",
      "====> Test set loss: 264.6904\n",
      "Train Epoch: 96 [0/6893 (0%)]\tLoss: 249.604813\n",
      "Train Epoch: 96 [6400/6893 (1%)]\tLoss: 268.689850\n",
      "====> Epoch: 96 Average loss: 260.0408\n",
      "====> Test set loss: 260.6504\n",
      "Train Epoch: 97 [0/6893 (0%)]\tLoss: 255.306961\n",
      "Train Epoch: 97 [6400/6893 (1%)]\tLoss: 246.696030\n",
      "====> Epoch: 97 Average loss: 256.5814\n",
      "====> Test set loss: 256.3121\n",
      "Train Epoch: 98 [0/6893 (0%)]\tLoss: 252.721008\n",
      "Train Epoch: 98 [6400/6893 (1%)]\tLoss: 252.556198\n",
      "====> Epoch: 98 Average loss: 252.8328\n",
      "====> Test set loss: 252.3071\n",
      "Train Epoch: 99 [0/6893 (0%)]\tLoss: 253.225220\n",
      "Train Epoch: 99 [6400/6893 (1%)]\tLoss: 249.960785\n",
      "====> Epoch: 99 Average loss: 249.5326\n",
      "====> Test set loss: 250.0644\n",
      "Train Epoch: 100 [0/6893 (0%)]\tLoss: 253.287659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 100 [6400/6893 (1%)]\tLoss: 246.614105\n",
      "====> Epoch: 100 Average loss: 245.3262\n",
      "====> Test set loss: 244.7185\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train(train_data, pretrain_vae, pretrain_vae_optimizer, epoch)\n",
    "        test(test_data, pretrain_vae, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = pretrain_vae(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = pretrain_vae(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1763, device='cuda:0')\n",
      "tensor(0.1770, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(329, device='cuda:0')\n",
      "tensor(214, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(test_pred[0,:] != 0))\n",
    "print(torch.sum(test_data[0,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pretrain_vae.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc21): Linear(in_features=200, out_features=50, bias=True)\n",
       "  (fc22): Linear(in_features=200, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=200, bias=True)\n",
       "  (fc4): Linear(in_features=200, out_features=500, bias=True)\n",
       "  (fc5): Linear(in_features=200, out_features=500, bias=True)\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=200, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gumbel with the Pre-Trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pre_trained(df, model, optimizer, epoch, pretrained_model):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        with torch.no_grad():\n",
    "            _, _, mu_latent_2, logvar_latent_2 = pretrained_model(batch_data)\n",
    "        \n",
    "        loss = loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent)\n",
    "        loss += 1000*F.mse_loss(mu_latent, mu_latent_2, reduction = 'sum')\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_keys(w):\n",
    "    # sample some gumbels\n",
    "    uniform = (1.0 - EPSILON) * torch.rand_like(w) + EPSILON\n",
    "    z = torch.log(-torch.log(uniform))\n",
    "    w = w + z\n",
    "    return w\n",
    "\n",
    "\n",
    "#equations 3 and 4 and 5\n",
    "def continuous_topk(w, k, t, separate=False):\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    khot_list = []\n",
    "    onehot_approx = torch.zeros_like(w, dtype = torch.float32)\n",
    "    for i in range(k):\n",
    "        ### conver the following into pytorch\n",
    "        #khot_mask = tf.maximum(1.0 - onehot_approx, EPSILON)\n",
    "        max_mask = 1 - onehot_approx < EPSILON\n",
    "        khot_mask = 1 - onehot_approx\n",
    "        khot_mask[max_mask] = EPSILON\n",
    "        \n",
    "        w += torch.log(khot_mask)\n",
    "        #onehot_approx = tf.nn.softmax(w / t, axis=-1)\n",
    "        onehot_approx = softmax(w/t)\n",
    "        khot_list.append(onehot_approx)\n",
    "    if separate:\n",
    "        return torch.stack(khot_list)\n",
    "    else:\n",
    "        return torch.sum(torch.stack(khot_list), dim = 0) \n",
    "\n",
    "\n",
    "def sample_subset(w, k, t=0.1):\n",
    "    '''\n",
    "    Args:\n",
    "        w (Tensor): Float Tensor of weights for each element. In gumbel mode\n",
    "            these are interpreted as log probabilities\n",
    "        k (int): number of elements in the subset sample\n",
    "        t (float): temperature of the softmax\n",
    "    '''\n",
    "    w = gumbel_keys(w)\n",
    "    return continuous_topk(w, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_Gumbel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size, k, t = 0.1):\n",
    "        super(VAE_Gumbel, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self.t = t\n",
    "        \n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size)\n",
    "        )\n",
    "        \n",
    "        #self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        #self.fcextra = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "        self.fc5 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        w = self.weight_creator(x)\n",
    "        subset_indices = sample_subset(w, self.k, self.t)\n",
    "        x = x * subset_indices\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.leaky_relu(self.fc3(z))\n",
    "        mu_x = F.leaky_relu(self.fc4(h))\n",
    "        #mu_x = self.fc4(h)\n",
    "        logvar_x = self.fc5(h)\n",
    "        return mu_x, logvar_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_latent, logvar_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu_latent, logvar_latent)\n",
    "        mu_x, logvar_x = self.decode(z)\n",
    "        return mu_x, logvar_x, mu_latent, logvar_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel_with_pre = VAE_Gumbel(500, 200, 50, k = 50)\n",
    "vae_gumbel_with_pre.to(device)\n",
    "vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 11354.818359\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 10679.607422\n",
      "====> Epoch: 1 Average loss: 10988.9253\n",
      "====> Test set loss: 10539.3864\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 10648.009766\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 10089.518555\n",
      "====> Epoch: 2 Average loss: 10321.4054\n",
      "====> Test set loss: 9897.3480\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 9972.563477\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 9462.444336\n",
      "====> Epoch: 3 Average loss: 9690.9522\n",
      "====> Test set loss: 9291.3823\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 9395.164062\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 8823.656250\n",
      "====> Epoch: 4 Average loss: 9082.4363\n",
      "====> Test set loss: 8701.4519\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 8797.026367\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 8240.162109\n",
      "====> Epoch: 5 Average loss: 8503.0004\n",
      "====> Test set loss: 8133.9604\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 8199.973633\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 7699.266113\n",
      "====> Epoch: 6 Average loss: 7927.2575\n",
      "====> Test set loss: 7572.9604\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 7655.047363\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 7084.588379\n",
      "====> Epoch: 7 Average loss: 7360.6253\n",
      "====> Test set loss: 7011.5464\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 7017.580078\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 6563.251953\n",
      "====> Epoch: 8 Average loss: 6785.8344\n",
      "====> Test set loss: 6437.3820\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 6508.196777\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 5911.937988\n",
      "====> Epoch: 9 Average loss: 6207.3478\n",
      "====> Test set loss: 5861.2114\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 5892.676270\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 5377.848633\n",
      "====> Epoch: 10 Average loss: 5614.8868\n",
      "====> Test set loss: 5274.5205\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 5318.241699\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 4733.785645\n",
      "====> Epoch: 11 Average loss: 5027.8044\n",
      "====> Test set loss: 4693.2358\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 4715.667969\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 4147.531738\n",
      "====> Epoch: 12 Average loss: 4438.2938\n",
      "====> Test set loss: 4111.6735\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 4218.318359\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 3523.312012\n",
      "====> Epoch: 13 Average loss: 3854.8749\n",
      "====> Test set loss: 3538.2595\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 3577.692871\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 3036.831299\n",
      "====> Epoch: 14 Average loss: 3284.8482\n",
      "====> Test set loss: 2995.3490\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 2982.811035\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 2517.084229\n",
      "====> Epoch: 15 Average loss: 2768.3485\n",
      "====> Test set loss: 2521.6484\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 2517.461670\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 2100.202881\n",
      "====> Epoch: 16 Average loss: 2343.3032\n",
      "====> Test set loss: 2151.6657\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 2115.535645\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 1929.814819\n",
      "====> Epoch: 17 Average loss: 1996.3145\n",
      "====> Test set loss: 1840.5047\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 1778.701172\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 1628.089111\n",
      "====> Epoch: 18 Average loss: 1740.5041\n",
      "====> Test set loss: 1627.0244\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 1586.479370\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 1475.685303\n",
      "====> Epoch: 19 Average loss: 1535.2132\n",
      "====> Test set loss: 1439.7360\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 1380.122803\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 1261.638428\n",
      "====> Epoch: 20 Average loss: 1375.0940\n",
      "====> Test set loss: 1311.1133\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 1211.549316\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 1403.481445\n",
      "====> Epoch: 21 Average loss: 1257.8569\n",
      "====> Test set loss: 1201.9915\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 1263.998779\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 1106.680176\n",
      "====> Epoch: 22 Average loss: 1159.5440\n",
      "====> Test set loss: 1117.5878\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 1156.787354\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 1040.473999\n",
      "====> Epoch: 23 Average loss: 1078.1954\n",
      "====> Test set loss: 1030.1496\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 1121.273926\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 953.274475\n",
      "====> Epoch: 24 Average loss: 1006.6809\n",
      "====> Test set loss: 973.7888\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 875.063171\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 861.271301\n",
      "====> Epoch: 25 Average loss: 952.1639\n",
      "====> Test set loss: 914.3127\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 911.738159\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 920.677795\n",
      "====> Epoch: 26 Average loss: 899.8217\n",
      "====> Test set loss: 872.8582\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 966.710571\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 859.252075\n",
      "====> Epoch: 27 Average loss: 854.7321\n",
      "====> Test set loss: 824.9407\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 970.548462\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 769.931396\n",
      "====> Epoch: 28 Average loss: 816.6487\n",
      "====> Test set loss: 789.7252\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 821.966431\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 730.380066\n",
      "====> Epoch: 29 Average loss: 778.1186\n",
      "====> Test set loss: 758.5328\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 691.046448\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 697.075623\n",
      "====> Epoch: 30 Average loss: 751.5103\n",
      "====> Test set loss: 729.0489\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 700.596802\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 661.512390\n",
      "====> Epoch: 31 Average loss: 722.7354\n",
      "====> Test set loss: 704.0448\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 648.603882\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 637.183167\n",
      "====> Epoch: 32 Average loss: 696.4474\n",
      "====> Test set loss: 680.1562\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 651.125793\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 641.947815\n",
      "====> Epoch: 33 Average loss: 675.5065\n",
      "====> Test set loss: 660.0509\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 684.009644\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 603.189270\n",
      "====> Epoch: 34 Average loss: 653.8407\n",
      "====> Test set loss: 639.6844\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 626.775757\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 597.229858\n",
      "====> Epoch: 35 Average loss: 632.2045\n",
      "====> Test set loss: 619.2748\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 642.422485\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 599.153992\n",
      "====> Epoch: 36 Average loss: 617.3638\n",
      "====> Test set loss: 599.7939\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 609.710876\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 554.042175\n",
      "====> Epoch: 37 Average loss: 598.9615\n",
      "====> Test set loss: 586.1648\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 576.527710\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 567.907593\n",
      "====> Epoch: 38 Average loss: 583.0916\n",
      "====> Test set loss: 573.1190\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 561.522522\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 553.070374\n",
      "====> Epoch: 39 Average loss: 568.0369\n",
      "====> Test set loss: 556.4399\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 523.353333\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 541.297974\n",
      "====> Epoch: 40 Average loss: 551.1059\n",
      "====> Test set loss: 543.0150\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 510.885071\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 545.683777\n",
      "====> Epoch: 41 Average loss: 541.6676\n",
      "====> Test set loss: 529.3073\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 560.243225\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 524.292847\n",
      "====> Epoch: 42 Average loss: 526.6232\n",
      "====> Test set loss: 518.2156\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 479.967621\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 487.697815\n",
      "====> Epoch: 43 Average loss: 516.0109\n",
      "====> Test set loss: 506.5798\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 509.291626\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 477.692108\n",
      "====> Epoch: 44 Average loss: 505.4099\n",
      "====> Test set loss: 496.5579\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 496.765472\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 471.329437\n",
      "====> Epoch: 45 Average loss: 492.9287\n",
      "====> Test set loss: 486.2554\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 501.294983\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 486.736816\n",
      "====> Epoch: 46 Average loss: 483.6883\n",
      "====> Test set loss: 477.2904\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 449.005432\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 477.764954\n",
      "====> Epoch: 47 Average loss: 472.4104\n",
      "====> Test set loss: 467.7418\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 460.939606\n",
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 476.110107\n",
      "====> Epoch: 48 Average loss: 464.3616\n",
      "====> Test set loss: 457.1444\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 435.183655\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 430.976562\n",
      "====> Epoch: 49 Average loss: 456.1155\n",
      "====> Test set loss: 449.1922\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 449.199524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 448.529510\n",
      "====> Epoch: 50 Average loss: 446.4347\n",
      "====> Test set loss: 444.3412\n",
      "Train Epoch: 51 [0/6893 (0%)]\tLoss: 441.517548\n",
      "Train Epoch: 51 [6400/6893 (1%)]\tLoss: 452.476532\n",
      "====> Epoch: 51 Average loss: 439.6078\n",
      "====> Test set loss: 432.2943\n",
      "Train Epoch: 52 [0/6893 (0%)]\tLoss: 414.686371\n",
      "Train Epoch: 52 [6400/6893 (1%)]\tLoss: 439.610321\n",
      "====> Epoch: 52 Average loss: 432.2142\n",
      "====> Test set loss: 426.2065\n",
      "Train Epoch: 53 [0/6893 (0%)]\tLoss: 414.951141\n",
      "Train Epoch: 53 [6400/6893 (1%)]\tLoss: 426.015228\n",
      "====> Epoch: 53 Average loss: 424.2184\n",
      "====> Test set loss: 418.5072\n",
      "Train Epoch: 54 [0/6893 (0%)]\tLoss: 404.754425\n",
      "Train Epoch: 54 [6400/6893 (1%)]\tLoss: 418.968933\n",
      "====> Epoch: 54 Average loss: 417.3463\n",
      "====> Test set loss: 413.8991\n",
      "Train Epoch: 55 [0/6893 (0%)]\tLoss: 422.851471\n",
      "Train Epoch: 55 [6400/6893 (1%)]\tLoss: 394.760620\n",
      "====> Epoch: 55 Average loss: 412.2916\n",
      "====> Test set loss: 407.0802\n",
      "Train Epoch: 56 [0/6893 (0%)]\tLoss: 401.387543\n",
      "Train Epoch: 56 [6400/6893 (1%)]\tLoss: 405.154266\n",
      "====> Epoch: 56 Average loss: 404.9478\n",
      "====> Test set loss: 397.5436\n",
      "Train Epoch: 57 [0/6893 (0%)]\tLoss: 413.951569\n",
      "Train Epoch: 57 [6400/6893 (1%)]\tLoss: 387.867096\n",
      "====> Epoch: 57 Average loss: 399.1595\n",
      "====> Test set loss: 395.6951\n",
      "Train Epoch: 58 [0/6893 (0%)]\tLoss: 400.581085\n",
      "Train Epoch: 58 [6400/6893 (1%)]\tLoss: 386.320465\n",
      "====> Epoch: 58 Average loss: 393.2725\n",
      "====> Test set loss: 389.5716\n",
      "Train Epoch: 59 [0/6893 (0%)]\tLoss: 390.744232\n",
      "Train Epoch: 59 [6400/6893 (1%)]\tLoss: 384.285034\n",
      "====> Epoch: 59 Average loss: 387.8145\n",
      "====> Test set loss: 384.1241\n",
      "Train Epoch: 60 [0/6893 (0%)]\tLoss: 390.282379\n",
      "Train Epoch: 60 [6400/6893 (1%)]\tLoss: 375.657928\n",
      "====> Epoch: 60 Average loss: 382.8097\n",
      "====> Test set loss: 378.6601\n",
      "Train Epoch: 61 [0/6893 (0%)]\tLoss: 388.760986\n",
      "Train Epoch: 61 [6400/6893 (1%)]\tLoss: 374.835480\n",
      "====> Epoch: 61 Average loss: 377.0467\n",
      "====> Test set loss: 373.9463\n",
      "Train Epoch: 62 [0/6893 (0%)]\tLoss: 371.485626\n",
      "Train Epoch: 62 [6400/6893 (1%)]\tLoss: 356.848755\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, epoch, pretrain_vae)\n",
    "        test(test_data, vae_gumbel_with_pre, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = vae_gumbel_with_pre(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = vae_gumbel_with_pre(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(test_pred[0,:] != 0))\n",
    "print(torch.sum(test_data[0,:] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 should be vanilla\n",
    "\n",
    "def train_joint(df, model1, model2, optimizer, epoch):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        loss_vae_1, loss_vae_2, joint_kld_loss = loss_function_joint(batch_data, model1, model2)\n",
    "        loss = (loss_vae_1 + loss_vae_2 + 1000 * joint_kld_loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_joint(df, model1, model2, epoch):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    inds = np.arange(df.shape[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = df[batch_ind, :]\n",
    "            batch_data = batch_data.to(device)\n",
    "            loss_vae_1, loss_vae_2, joint_kld_loss = loss_function_joint(batch_data, model1, model2)\n",
    "        \n",
    "            test_loss += (loss_vae_1 + loss_vae_2 + 1000 * joint_kld_loss).item()\n",
    "\n",
    "\n",
    "    test_loss /= len(df)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(500, 200, 50)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(500, 200, 50, k = 50)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = joint_vae_gumbel(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = joint_vae_gumbel(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(test_pred[0,:] != 0))\n",
    "print(torch.sum(test_data[0,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's actually Garph this.\n",
    "\n",
    "### Try it out at Gumbel sparsity of k = 10, 25, 50, 100, 250\n",
    "\n",
    "### Graph Test MSE Loss\n",
    "\n",
    "## Graph the mean activations at k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_activations(test_data, model, title, file):\n",
    "    preds, _, _, _ = model(test_data)\n",
    "    \n",
    "    pred_activations = preds.mean(dim = 0)\n",
    "    \n",
    "    test_activations = test_data.mean(dim = 0)\n",
    "    \n",
    "    x = np.arange(500) + 1\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, pred_activations.clone().detach().cpu().numpy(), label = 'Average Predictions')\n",
    "    plt.plot(x, test_activations.clone().detach().cpu().numpy(), label = 'Average Test Data')\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_activations(test_data, joint_vae_gumbel, 'Joint Gumbel vs Test Means', \n",
    "                  '/scratch/ns3429/sparse-subset/joint_gumbel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_activations(test_data, vae_gumbel_with_pre, 'Gumbel Matching Pretrained VAE vs Test Means', \n",
    "                  '/scratch/ns3429/sparse-subset/pretrained_gumbel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_all = [10, 25, 50, 100, 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_pre = []\n",
    "losses_joint = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_all:\n",
    "    vae_gumbel_with_pre = VAE_Gumbel(500, 200, 50, k = k)\n",
    "    vae_gumbel_with_pre.to(device)\n",
    "    vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))\n",
    "    \n",
    "    joint_vanilla_vae = VAE(500, 200, 50)\n",
    "    joint_vanilla_vae.to(device)\n",
    "\n",
    "    joint_vae_gumbel = VAE_Gumbel(500, 200, 50, k = k)\n",
    "    joint_vae_gumbel.to(device)\n",
    "\n",
    "\n",
    "    joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))\n",
    "    \n",
    "    for epoch in (1, n_epochs + 1):\n",
    "        train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, epoch, pretrain_vae)\n",
    "        train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch)\n",
    "    \n",
    "    test_pred_pre = vae_gumbel_with_pre(test_data)[0]\n",
    "    test_pred_pre[test_pred_pre < 0.001] = 0 \n",
    "    \n",
    "    test_pred_joint = joint_vanilla_vae(test_data)[0]\n",
    "    test_pred_joint[test_pred_joint < 0.001] = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mae_pre = torch.sum((test_pred_pre - test_data).abs()) / len(test_data) / 500\n",
    "        mae_joint = torch.sum((test_pred_joint - test_data).abs()) / len(test_data) / 500\n",
    "    \n",
    "    losses_pre.append(mae_pre.cpu().item())\n",
    "    losses_joint.append(mae_joint.cpu().item())\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(k_all, losses_pre, label = 'Average MAE Losses with Gumbel Matching Pretrained')\n",
    "plt.plot(k_all, losses_joint, label = 'Average MAE Losses with Gumbel Joint Training')\n",
    "\n",
    "plt.title(\"Effect on Sparsity on MAE Loss\")\n",
    "plt.xlabel('Sparsity Level (Number of Non-Zero Features)')\n",
    "plt.ylabel('Per Neuron Average MAE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('/scratch/ns3429/sparse-subset/comparing_across_sparsity.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu]",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
