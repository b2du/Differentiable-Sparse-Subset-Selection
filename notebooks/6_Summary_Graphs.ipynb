{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just get a quick sparsity overview of the methods so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = '../data/'\n",
    "#BASE_PATH_DATA = '/scratch/ns3429/sparse-subset/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "lr = 0.000002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "img_size = 28\n",
    "channels = 1\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "z_size = 40\n",
    "\n",
    "n = 28 * 28\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/CITEseq.mat')\n",
    "data= a['G'].T\n",
    "N,d=data.shape\n",
    "#transformation from integer entries \n",
    "data=np.log(data+np.ones(data.shape))\n",
    "#for i in range(N):\n",
    "for i in range(d):\n",
    "    #data[i,:]=data[i,:]/np.linalg.norm(data[i,:])\n",
    "    data[:,i]= (data[:,i] - np.min(data[:,i])) /  (np.max(data[:,i]) - np.min(data[:, i]))\n",
    "\n",
    "#load labels from file\n",
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/CITEseq-labels.mat')\n",
    "l_aux = a['labels']\n",
    "labels = np.array([i for [i] in l_aux])\n",
    "\n",
    "#load names from file\n",
    "a = sio.loadmat(BASE_PATH_DATA + 'zeisel/CITEseq_names.mat')\n",
    "names=[a['citeseq_names'][i][0][0] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(data.shape[0]))\n",
    "upto = int(.8 * len(data))\n",
    "\n",
    "train_data = data[slices[:upto]]\n",
    "test_data = data[slices[upto:]]\n",
    "\n",
    "train_data = Tensor(train_data).to(device)\n",
    "test_data = Tensor(test_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1829, device='cuda:0')\n",
      "tensor(0.1833, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.std(dim = 0).mean())\n",
    "print(test_data.std(dim = 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_per_autoencoder(x, mu_x, logvar_x, mu_latent, logvar_latent):\n",
    "    # BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    # BCE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    loss_rec = -torch.sum(\n",
    "            (-0.5 * np.log(2.0 * np.pi))\n",
    "            + (-0.5 * logvar_x)\n",
    "            + ((-0.5 / torch.exp(logvar_x)) * (x - mu_x) ** 2.0))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar_latent - mu_latent.pow(2) - logvar_latent.exp())\n",
    "    #print(loss_rec.item(), KLD.item())\n",
    "    return loss_rec + 130640 * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLD of D(P_1||P_2) where P_i are Gaussians, assuming diagonal\n",
    "def kld_joint_autoencoders(mu_1, mu_2, logvar_1, logvar_2):\n",
    "    # equation 6 of Tutorial on Variational Autoencoders by Carl Doersch\n",
    "    # https://arxiv.org/pdf/1606.05908.pdf\n",
    "    mu_12 = mu_1 - mu_2\n",
    "    kld = 0.5 * (-1 - (logvar_1 - logvar_2) + mu_12.pow(2) / logvar_2.exp() + torch.exp(logvar_1 - logvar_2))\n",
    "    #print(kld.shape)\n",
    "    kld = torch.sum(kld, dim = 1)\n",
    "    \n",
    "    return kld.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for joint\n",
    "def loss_function_joint(x, ae_1, ae_2):\n",
    "    # assuming that both autoencoders return recon_x, mu, and logvar\n",
    "    # try to make ae_1 the vanilla vae\n",
    "    # ae_2 should be the L1 penalty VAE\n",
    "    mu_x_1, logvar_x_1, mu_latent_1, logvar_latent_1 = ae_1(x)\n",
    "    mu_x_2, logvar_x_2, mu_latent_2, logvar_latent_2 = ae_2(x)\n",
    "    \n",
    "    loss_vae_1 = loss_function_per_autoencoder(x, mu_x_1, logvar_x_1, mu_latent_1, logvar_latent_1)\n",
    "    loss_vae_2 = loss_function_per_autoencoder(x, mu_x_2, logvar_x_2, mu_latent_2, logvar_latent_2)\n",
    "    joint_kld_loss = kld_joint_autoencoders(mu_latent_1, mu_latent_2, logvar_latent_1, logvar_latent_1)\n",
    "    #print(\"Losses\")\n",
    "    #print(loss_vae_1)\n",
    "    #print(loss_vae_2)\n",
    "    #print(joint_kld_loss)\n",
    "    return loss_vae_1, loss_vae_2, joint_kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does L1 work if we normalize after every step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_l1_diag(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size):\n",
    "        super(VAE_l1_diag, self).__init__()\n",
    "        \n",
    "        self.diag = nn.Parameter(torch.normal(torch.zeros(input_size), \n",
    "                                 torch.ones(input_size)).to(device).requires_grad_(True))\n",
    "        \n",
    "        # self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "        self.fc5 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.selection_layer = torch.diag(self.diag)\n",
    "        h0 = torch.mm(x, self.selection_layer)\n",
    "        h1 = self.encoder(h0)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.leaky_relu(self.fc3(z))\n",
    "        mu_x = F.leaky_relu(self.fc4(h))\n",
    "        #mu_x = self.fc4(h)\n",
    "        logvar_x = self.fc5(h)\n",
    "        return mu_x, logvar_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_latent, logvar_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu_latent, logvar_latent)\n",
    "        mu_x, logvar_x = self.decode(z)\n",
    "        return mu_x, logvar_x, mu_latent, logvar_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l1(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        loss = loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent)\n",
    "        loss += 1000 * torch.norm(model.diag, p = 1)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.diag.data /= torch.norm(model.diag.data, p = 2)\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df, model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    inds = np.arange(df.shape[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = df[batch_ind, :]\n",
    "            batch_data = batch_data.to(device)\n",
    "            mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "            test_loss += loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent).item()\n",
    "\n",
    "\n",
    "    test_loss /= len(df)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1_diag = VAE_l1_diag(500, 200, 50)\n",
    "\n",
    "model_l1_diag.to(device)\n",
    "model_l1_optimizer = torch.optim.Adam(model_l1_diag.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 18919.675781\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 11274.064453\n",
      "====> Epoch: 1 Average loss: 11596.6926\n",
      "====> Test set loss: 10957.5660\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 11232.386719\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 10706.171875\n",
      "====> Epoch: 2 Average loss: 10951.0320\n",
      "====> Test set loss: 10393.1456\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 10667.792969\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 10168.211914\n",
      "====> Epoch: 3 Average loss: 10400.3867\n",
      "====> Test set loss: 9855.0571\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 10129.848633\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 9647.754883\n",
      "====> Epoch: 4 Average loss: 9872.6227\n",
      "====> Test set loss: 9338.9796\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 9611.535156\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 9153.852539\n",
      "====> Epoch: 5 Average loss: 9368.6614\n",
      "====> Test set loss: 8846.8443\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 9121.530273\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 8688.260742\n",
      "====> Epoch: 6 Average loss: 8886.1605\n",
      "====> Test set loss: 8376.7646\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 8653.893555\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 8233.898438\n",
      "====> Epoch: 7 Average loss: 8427.5819\n",
      "====> Test set loss: 7928.5075\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 8200.798828\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 7792.778809\n",
      "====> Epoch: 8 Average loss: 7985.9181\n",
      "====> Test set loss: 7493.0183\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 7762.757812\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 7366.047852\n",
      "====> Epoch: 9 Average loss: 7554.9092\n",
      "====> Test set loss: 7067.5420\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 7343.163086\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 6954.638672\n",
      "====> Epoch: 10 Average loss: 7134.0426\n",
      "====> Test set loss: 6651.8969\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 6929.253906\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 6542.894531\n",
      "====> Epoch: 11 Average loss: 6720.7171\n",
      "====> Test set loss: 6239.2947\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 6519.810547\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 6125.117188\n",
      "====> Epoch: 12 Average loss: 6310.9239\n",
      "====> Test set loss: 5833.7208\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 6115.680176\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 5737.380859\n",
      "====> Epoch: 13 Average loss: 5908.7679\n",
      "====> Test set loss: 5437.0540\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 5711.627930\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 5356.277344\n",
      "====> Epoch: 14 Average loss: 5519.0811\n",
      "====> Test set loss: 5054.3537\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 5320.918457\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 4979.865723\n",
      "====> Epoch: 15 Average loss: 5140.2972\n",
      "====> Test set loss: 4676.0464\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 4949.297852\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 4588.100586\n",
      "====> Epoch: 16 Average loss: 4760.9286\n",
      "====> Test set loss: 4301.3200\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 4573.226562\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 4233.050781\n",
      "====> Epoch: 17 Average loss: 4394.7795\n",
      "====> Test set loss: 3944.0636\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 4229.223633\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 3908.377930\n",
      "====> Epoch: 18 Average loss: 4045.6038\n",
      "====> Test set loss: 3602.2485\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 3862.723389\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 3569.105957\n",
      "====> Epoch: 19 Average loss: 3711.4861\n",
      "====> Test set loss: 3277.0076\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 3560.250244\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 3265.858154\n",
      "====> Epoch: 20 Average loss: 3395.3433\n",
      "====> Test set loss: 2969.4520\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 3212.251953\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 2950.198975\n",
      "====> Epoch: 21 Average loss: 3095.4371\n",
      "====> Test set loss: 2677.4784\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 2957.535400\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 2682.095947\n",
      "====> Epoch: 22 Average loss: 2811.7130\n",
      "====> Test set loss: 2402.4759\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 2662.220947\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 2434.274658\n",
      "====> Epoch: 23 Average loss: 2543.0614\n",
      "====> Test set loss: 2139.8092\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 2399.563477\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 2187.286621\n",
      "====> Epoch: 24 Average loss: 2288.3169\n",
      "====> Test set loss: 1893.0653\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 2174.834229\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 1943.797607\n",
      "====> Epoch: 25 Average loss: 2053.6199\n",
      "====> Test set loss: 1670.5862\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 1954.818481\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 1758.655884\n",
      "====> Epoch: 26 Average loss: 1838.7307\n",
      "====> Test set loss: 1466.2875\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 1739.781250\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 1547.893799\n",
      "====> Epoch: 27 Average loss: 1649.1764\n",
      "====> Test set loss: 1290.7800\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 1576.369019\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 1404.840942\n",
      "====> Epoch: 28 Average loss: 1485.0838\n",
      "====> Test set loss: 1138.2825\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 1412.775513\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 1286.602295\n",
      "====> Epoch: 29 Average loss: 1343.4636\n",
      "====> Test set loss: 1009.1544\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 1285.687256\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 1180.726074\n",
      "====> Epoch: 30 Average loss: 1224.8500\n",
      "====> Test set loss: 901.4924\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 1170.975830\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 1106.946655\n",
      "====> Epoch: 31 Average loss: 1124.9184\n",
      "====> Test set loss: 809.5774\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 1081.535645\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 992.985291\n",
      "====> Epoch: 32 Average loss: 1039.7263\n",
      "====> Test set loss: 732.6254\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 986.819031\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 942.855164\n",
      "====> Epoch: 33 Average loss: 966.7204\n",
      "====> Test set loss: 666.7261\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 948.651245\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 885.819458\n",
      "====> Epoch: 34 Average loss: 907.1841\n",
      "====> Test set loss: 614.2374\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 877.409973\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 836.008423\n",
      "====> Epoch: 35 Average loss: 858.9530\n",
      "====> Test set loss: 571.6937\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 835.694824\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 811.246094\n",
      "====> Epoch: 36 Average loss: 819.6982\n",
      "====> Test set loss: 536.5884\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 791.486328\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 768.560303\n",
      "====> Epoch: 37 Average loss: 787.6418\n",
      "====> Test set loss: 507.4938\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 768.041748\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 750.599487\n",
      "====> Epoch: 38 Average loss: 760.5117\n",
      "====> Test set loss: 484.6628\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 749.215332\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 737.823364\n",
      "====> Epoch: 39 Average loss: 738.5941\n",
      "====> Test set loss: 465.3964\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 718.949585\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 715.833069\n",
      "====> Epoch: 40 Average loss: 720.6987\n",
      "====> Test set loss: 450.5959\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 709.374329\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 694.530518\n",
      "====> Epoch: 41 Average loss: 705.8158\n",
      "====> Test set loss: 437.4047\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 705.810669\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 683.175232\n",
      "====> Epoch: 42 Average loss: 693.5972\n",
      "====> Test set loss: 426.5249\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 702.806946\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 702.495361\n",
      "====> Epoch: 43 Average loss: 683.1233\n",
      "====> Test set loss: 417.8591\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 674.398132\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 667.760559\n",
      "====> Epoch: 44 Average loss: 674.0166\n",
      "====> Test set loss: 410.2863\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 653.293579\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_l1(train_data, model_l1_diag, model_l1_optimizer, epoch)\n",
    "        test(test_data, model_l1_diag, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [10**(-i) for i in range(10)]\n",
    "bins.reverse()\n",
    "bins += [10]\n",
    "np.histogram(model_l1_diag.diag.abs().clone().detach().cpu().numpy(), bins = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = model_l1_diag(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = model_l1_diag(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(test_pred[9,:] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(test_data[9,:] != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try Pretrained VAE and then gumble trick with it\n",
    "\n",
    "Then try joint training VAE and Gumbel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla VAE model\n",
    "# try with gaussian decoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        #self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "        self.fc5 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        #self.decoder = nn.Sequential()\n",
    "\n",
    "    def encode(self, x):\n",
    "        #h1 = F.relu(self.fc1(x))\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):    \n",
    "        h = F.leaky_relu(self.fc3(z))\n",
    "        mu_x = F.leaky_relu(self.fc4(h))\n",
    "        #mu_x = self.fc4(h)\n",
    "        logvar_x = self.fc5(h)\n",
    "        return mu_x, logvar_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_latent, logvar_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu_latent, logvar_latent)\n",
    "        mu_x, logvar_x = self.decode(z)\n",
    "        return mu_x, logvar_x, mu_latent, logvar_latent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain VAE First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_vae = VAE(500, 200, 50)\n",
    "\n",
    "pretrain_vae.to(device)\n",
    "pretrain_vae_optimizer = torch.optim.Adam(pretrain_vae.parameters(), \n",
    "                                            lr=lr,\n",
    "                                            betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        loss = loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train(train_data, pretrain_vae, pretrain_vae_optimizer, epoch)\n",
    "        test(test_data, pretrain_vae, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = pretrain_vae(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = pretrain_vae(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(test_pred[0,:] != 0))\n",
    "print(torch.sum(test_data[0,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pretrain_vae.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gumbel with the Pre-Trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pre_trained(df, model, optimizer, epoch, pretrained_model):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mu_x, logvar_x, mu_latent, logvar_latent = model(batch_data)\n",
    "        with torch.no_grad():\n",
    "            _, _, mu_latent_2, logvar_latent_2 = pretrained_model(batch_data)\n",
    "        \n",
    "        loss = loss_function_per_autoencoder(batch_data, mu_x, logvar_x, mu_latent, logvar_latent)\n",
    "        loss += 1000*F.mse_loss(mu_latent, mu_latent_2, reduction = 'sum')\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_keys(w):\n",
    "    # sample some gumbels\n",
    "    uniform = (1.0 - EPSILON) * torch.rand_like(w) + EPSILON\n",
    "    z = torch.log(-torch.log(uniform))\n",
    "    w = w + z\n",
    "    return w\n",
    "\n",
    "\n",
    "#equations 3 and 4 and 5\n",
    "def continuous_topk(w, k, t, separate=False):\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    khot_list = []\n",
    "    onehot_approx = torch.zeros_like(w, dtype = torch.float32)\n",
    "    for i in range(k):\n",
    "        ### conver the following into pytorch\n",
    "        #khot_mask = tf.maximum(1.0 - onehot_approx, EPSILON)\n",
    "        max_mask = 1 - onehot_approx < EPSILON\n",
    "        khot_mask = 1 - onehot_approx\n",
    "        khot_mask[max_mask] = EPSILON\n",
    "        \n",
    "        w += torch.log(khot_mask)\n",
    "        #onehot_approx = tf.nn.softmax(w / t, axis=-1)\n",
    "        onehot_approx = softmax(w/t)\n",
    "        khot_list.append(onehot_approx)\n",
    "    if separate:\n",
    "        return torch.stack(khot_list)\n",
    "    else:\n",
    "        return torch.sum(torch.stack(khot_list), dim = 0) \n",
    "\n",
    "\n",
    "def sample_subset(w, k, t=0.1):\n",
    "    '''\n",
    "    Args:\n",
    "        w (Tensor): Float Tensor of weights for each element. In gumbel mode\n",
    "            these are interpreted as log probabilities\n",
    "        k (int): number of elements in the subset sample\n",
    "        t (float): temperature of the softmax\n",
    "    '''\n",
    "    w = gumbel_keys(w)\n",
    "    return continuous_topk(w, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_Gumbel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size, k, t = 0.1):\n",
    "        super(VAE_Gumbel, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self.t = t\n",
    "        \n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size)\n",
    "        )\n",
    "        \n",
    "        #self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        #self.fcextra = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "        self.fc5 = nn.Linear(hidden_layer_size, input_size)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        w = self.weight_creator(x)\n",
    "        subset_indices = sample_subset(w, self.k, self.t)\n",
    "        x = x * subset_indices\n",
    "        h1 = self.encoder(x)\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.leaky_relu(self.fc3(z))\n",
    "        mu_x = F.leaky_relu(self.fc4(h))\n",
    "        #mu_x = self.fc4(h)\n",
    "        logvar_x = self.fc5(h)\n",
    "        return mu_x, logvar_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu_latent, logvar_latent = self.encode(x)\n",
    "        z = self.reparameterize(mu_latent, logvar_latent)\n",
    "        mu_x, logvar_x = self.decode(z)\n",
    "        return mu_x, logvar_x, mu_latent, logvar_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel_with_pre = VAE_Gumbel(500, 200, 50, k = 50)\n",
    "vae_gumbel_with_pre.to(device)\n",
    "vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, epoch, pretrain_vae)\n",
    "        test(test_data, vae_gumbel_with_pre, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = vae_gumbel_with_pre(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = vae_gumbel_with_pre(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(test_pred[4,:] != 0))\n",
    "print(torch.sum(test_data[4,:] != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 should be vanilla\n",
    "\n",
    "def train_joint(df, model1, model2, optimizer, epoch):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        loss_vae_1, loss_vae_2, joint_kld_loss = loss_function_joint(batch_data, model1, model2)\n",
    "        loss = (loss_vae_1 + loss_vae_2 + 1000 * joint_kld_loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_joint(df, model1, model2, epoch):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    inds = np.arange(df.shape[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = df[batch_ind, :]\n",
    "            batch_data = batch_data.to(device)\n",
    "            loss_vae_1, loss_vae_2, joint_kld_loss = loss_function_joint(batch_data, model1, model2)\n",
    "        \n",
    "            test_loss += (loss_vae_1 + loss_vae_2 + 1000 * joint_kld_loss).item()\n",
    "\n",
    "\n",
    "    test_loss /= len(df)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_vanilla_vae = VAE(500, 200, 50)\n",
    "joint_vanilla_vae.to(device)\n",
    "\n",
    "joint_vae_gumbel = VAE_Gumbel(500, 200, 50, k = 50)\n",
    "joint_vae_gumbel.to(device)\n",
    "\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch)\n",
    "    test_joint(test_data, joint_vanilla_vae, joint_vae_gumbel, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = joint_vae_gumbel(train_data[0:64, :])[0]\n",
    "    train_pred[train_pred < 0.001] = 0 \n",
    "\n",
    "    test_pred = joint_vae_gumbel(test_data[0:64,:])[0]\n",
    "    test_pred[test_pred < 0.001] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((train_pred - train_data[0:64, :]).abs()) / 64/500)\n",
    "    print(torch.sum((test_pred - test_data[0:64, :]).abs()) / 64/500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(test_pred[4,:] != 0))\n",
    "print(torch.sum(test_data[4,:] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's actually Garph this.\n",
    "\n",
    "### Try it out at Gumbel sparsity of k = 10, 25, 50, 100, 250\n",
    "\n",
    "### Graph Test MSE Loss\n",
    "\n",
    "## Graph the mean activations at k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_activations(test_data, model, title, file):\n",
    "    preds, _, _, _ = model(test_data)\n",
    "    \n",
    "    pred_activations = preds.mean(dim = 0)\n",
    "    \n",
    "    test_activations = test_data.mean(dim = 0)\n",
    "    \n",
    "    x = np.arange(500) + 1\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(x, pred_activations.clone().detach().cpu().numpy(), label = 'Average Predictions')\n",
    "    plt.plot(x, test_activations.clone().detach().cpu().numpy(), label = 'Average Test Data')\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_activations(test_data, joint_vae_gumbel, 'Joint Gumbel vs Test Means', \n",
    "                  '/scratch/ns3429/sparse-subset/joint_gumbel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_activations(test_data, vae_gumbel_with_pre, 'Gumbel Matching Pretrained VAE vs Test Means', \n",
    "                  '/scratch/ns3429/sparse-subset/pretrained_gumbel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_all = [10, 25, 50, 100, 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_pre = []\n",
    "losses_joint = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_all:\n",
    "    vae_gumbel_with_pre = VAE_Gumbel(500, 200, 50, k = k)\n",
    "    vae_gumbel_with_pre.to(device)\n",
    "    vae_gumbel_with_pre_optimizer = torch.optim.Adam(vae_gumbel_with_pre.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))\n",
    "    \n",
    "    joint_vanilla_vae = VAE(500, 200, 50)\n",
    "    joint_vanilla_vae.to(device)\n",
    "\n",
    "    joint_vae_gumbel = VAE_Gumbel(500, 200, 50, k = k)\n",
    "    joint_vae_gumbel.to(device)\n",
    "\n",
    "\n",
    "    joint_optimizer = torch.optim.Adam(list(joint_vanilla_vae.parameters()) + list(joint_vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))\n",
    "    \n",
    "    for epoch in (1, n_epochs + 1):\n",
    "        train_pre_trained(train_data, vae_gumbel_with_pre, vae_gumbel_with_pre_optimizer, epoch, pretrain_vae)\n",
    "        train_joint(train_data, joint_vanilla_vae, joint_vae_gumbel, joint_optimizer, epoch)\n",
    "    \n",
    "    test_pred_pre = vae_gumbel_with_pre(test_data)[0]\n",
    "    test_pred_pre[test_pred_pre < 0.001] = 0 \n",
    "    \n",
    "    test_pred_joint = joint_vanilla_vae(test_data)[0]\n",
    "    test_pred_joint[test_pred_joint < 0.001] = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mae_pre = torch.sum((test_pred_pre - test_data).abs()) / len(test_data) / 500\n",
    "        mae_joint = torch.sum((test_pred_joint - test_data).abs()) / len(test_data) / 500\n",
    "    \n",
    "    losses_pre.append(mae_pre.cpu().item())\n",
    "    losses_joint.append(mae_joint.cpu().item())\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(k_all, losses_pre, label = 'Average MAE Losses with Gumbel Matching Pretrained')\n",
    "plt.plot(k_all, losses_joint, label = 'Average MAE Losses with Gumbel Joint Training')\n",
    "\n",
    "plt.title(\"Effect on Sparsity on MAE Loss\")\n",
    "plt.xlabel('Sparsity Level (Number of Non-Zero Features)')\n",
    "plt.ylabel('Per Neuron Average MAE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('/scratch/ns3429/sparse-subset/comparing_across_sparsity.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu]",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
