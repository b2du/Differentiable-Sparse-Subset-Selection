{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have tried L1 normalization on the weight layer that selects features. Have trained when trying to match a pretrained VAE. Can we train a vanilla model and a L1 model together? How does it compare when each done solo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "img_size = 28\n",
    "channels = 1\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "z_size = 40\n",
    "\n",
    "n = 28 * 28\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_layer_size, z_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(500, hidden_layer_size)\n",
    "        #self.fcextra = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, 500)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        #h1 = F.leaky_relu(self.fcextra(h0))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.relu(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_l1_diag(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size):\n",
    "        super(VAE_l1_diag, self).__init__()\n",
    "        \n",
    "        self.diag = nn.Parameter(torch.normal(torch.zeros(input_size), \n",
    "                                 torch.ones(input_size)).to(device).requires_grad_(True))\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fcextra = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.selection_layer = torch.diag(self.diag)\n",
    "        h0 = torch.mm(x, self.selection_layer)\n",
    "        h1 = F.leaky_relu(self.fc1(h0))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.leaky_relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_keys(w):\n",
    "    # sample some gumbels\n",
    "    uniform = (1.0 - EPSILON) * torch.rand_like(w) + EPSILON\n",
    "    z = torch.log(-torch.log(uniform))\n",
    "    w = w + z\n",
    "    return w\n",
    "\n",
    "\n",
    "#equations 3 and 4 and 5\n",
    "def continuous_topk(w, k, t, separate=False):\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    khot_list = []\n",
    "    onehot_approx = torch.zeros_like(w, dtype = torch.float32)\n",
    "    for i in range(k):\n",
    "        ### conver the following into pytorch\n",
    "        #khot_mask = tf.maximum(1.0 - onehot_approx, EPSILON)\n",
    "        max_mask = 1 - onehot_approx < EPSILON\n",
    "        khot_mask = 1 - onehot_approx\n",
    "        khot_mask[max_mask] = EPSILON\n",
    "        \n",
    "        w += torch.log(khot_mask)\n",
    "        #onehot_approx = tf.nn.softmax(w / t, axis=-1)\n",
    "        onehot_approx = softmax(w/t)\n",
    "        khot_list.append(onehot_approx)\n",
    "    if separate:\n",
    "        return torch.stack(khot_list)\n",
    "    else:\n",
    "        return torch.sum(torch.stack(khot_list), dim = 0) \n",
    "\n",
    "\n",
    "def sample_subset(w, k, t=0.1):\n",
    "    '''\n",
    "    Args:\n",
    "        w (Tensor): Float Tensor of weights for each element. In gumbel mode\n",
    "            these are interpreted as log probabilities\n",
    "        k (int): number of elements in the subset sample\n",
    "        t (float): temperature of the softmax\n",
    "    '''\n",
    "    w = gumbel_keys(w)\n",
    "    return continuous_topk(w, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_Gumbel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size, k, t = 0.1):\n",
    "        super(VAE_Gumbel, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self.t = t\n",
    "        \n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        #self.fcextra = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        w = self.weight_creator(x)\n",
    "        subset_indices = sample_subset(w, self.k, self.t)\n",
    "        x = x * subset_indices\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        #h1 = F.leaky_relu(self.fcextra(h0))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.relu(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KLD between the two variational blocks\n",
    "\n",
    "# KLD of D(P_1||P_2) where P_i are Gaussians, assuming diagonal\n",
    "def kld_joint_autoencoders(mu_1, mu_2, logvar_1, logvar_2):\n",
    "    # equation 6 of Tutorial on Variational Autoencoders by Carl Doersch\n",
    "    # https://arxiv.org/pdf/1606.05908.pdf\n",
    "    mu_12 = mu_1 - mu_2\n",
    "    kld = 0.5 * (-1 - (logvar_1 - logvar_2) + mu_12.pow(2) / logvar_2.exp() + torch.exp(logvar_1 - logvar_2))\n",
    "    #print(kld.shape)\n",
    "    kld = torch.sum(kld, dim = 1)\n",
    "    \n",
    "    return kld.sum()\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function_per_autoencoder(recon_x, x, mu, logvar):\n",
    "    #BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    base_loss = MSE\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return base_loss + KLD\n",
    "\n",
    "\n",
    "def loss_function(x, ae_1, ae_2):\n",
    "    # assuming that both autoencoders return recon_x, mu, and logvar\n",
    "    # try to make ae_1 the vanilla vae\n",
    "    # ae_2 should be the L1 penalty VAE\n",
    "    recon_x1, mu_1, logvar_1 = ae_1(x)\n",
    "    recon_x2, mu_2, logvar_2 = ae_2(x)\n",
    "    \n",
    "    loss_vae_1 = loss_function_per_autoencoder(recon_x1, x, mu_1, logvar_1)\n",
    "    loss_vae_2 = loss_function_per_autoencoder(recon_x2, x, mu_2, logvar_2)\n",
    "    joint_kld_loss = kld_joint_autoencoders(mu_1, mu_2, logvar_1, logvar_2)\n",
    "    #print(\"Losses\")\n",
    "    #print(loss_vae_1)\n",
    "    #print(loss_vae_2)\n",
    "    #print(joint_kld_loss)\n",
    "    return loss_vae_1, loss_vae_2, joint_kld_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_joint(df, model1, model2, optimizer, epoch):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_vae_1, loss_vae_2, joint_kld_loss = loss_function(batch_data, model1, model2)\n",
    "        \n",
    "        loss = loss_vae_1 + loss_vae_2 + joint_kld_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "            \n",
    "            print('Loss vae 1: {}\\tLoss vae 2: {}\\tJoint KLD Loss {}'.format(\n",
    "            loss_vae_1/len(batch_data),\n",
    "            loss_vae_2/len(batch_data),\n",
    "            joint_kld_loss/len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_joint(df, model1, model2, epoch):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    test_loss = 0\n",
    "    inds = np.arange(df.shape[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = df[batch_ind, :]\n",
    "            batch_data = batch_data.to(device)\n",
    "            loss_vae_1, loss_vae_2, joint_kld_loss = loss_function(batch_data, model1, model2)\n",
    "        \n",
    "            test_loss += loss_vae_1 + loss_vae_2 + joint_kld_loss\n",
    "\n",
    "\n",
    "    test_loss /= len(df)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sio.loadmat(\"../data/zeisel/CITEseq.mat\")\n",
    "data= a['G'].T\n",
    "N,d=data.shape\n",
    "#transformation from integer entries \n",
    "data=np.log(data+np.ones(data.shape))\n",
    "#for i in range(N):\n",
    "for i in range(d):\n",
    "    #data[i,:]=data[i,:]/np.linalg.norm(data[i,:])\n",
    "    data[:,i]= (data[:,i] - np.min(data[:,i])) /  (np.max(data[:,i]) - np.min(data[:, i]))\n",
    "\n",
    "#load labels from file\n",
    "a = sio.loadmat(\"../data/zeisel/CITEseq-labels.mat\")\n",
    "l_aux = a['labels']\n",
    "labels = np.array([i for [i] in l_aux])\n",
    "\n",
    "#load names from file\n",
    "a = sio.loadmat(\"../data/zeisel/CITEseq_names.mat\")\n",
    "names=[a['citeseq_names'][i][0][0] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(data.shape[0]))\n",
    "upto = int(.8 * len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[slices[:upto]]\n",
    "test_data = data[slices[upto:]]\n",
    "\n",
    "train_data = Tensor(train_data).to(device)\n",
    "test_data = Tensor(test_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (fc21): Linear(in_features=250, out_features=200, bias=True)\n",
       "  (fc22): Linear(in_features=250, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=250, bias=True)\n",
       "  (fc4): Linear(in_features=250, out_features=500, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae_zeisel = VAE(250, 200)\n",
    "vanilla_vae_zeisel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (fc21): Linear(in_features=250, out_features=200, bias=True)\n",
       "  (fc22): Linear(in_features=250, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=250, bias=True)\n",
       "  (fc4): Linear(in_features=250, out_features=500, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae_zeisel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE_Gumbel(\n",
       "  (weight_creator): Sequential(\n",
       "    (0): Linear(in_features=500, out_features=250, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=250, out_features=500, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (fc21): Linear(in_features=250, out_features=200, bias=True)\n",
       "  (fc22): Linear(in_features=250, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=250, bias=True)\n",
       "  (fc4): Linear(in_features=250, out_features=500, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel = VAE_Gumbel(500, 250, 200, k = 200)\n",
    "vae_gumbel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_optimizer = torch.optim.Adam(list(vanilla_vae_zeisel.parameters()) + list(vae_gumbel.parameters()), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 78.373070\n",
      "Loss vae 1: 38.75425338745117\tLoss vae 2: 38.25804901123047\tJoint KLD Loss 1.3607683181762695\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 52.825760\n",
      "Loss vae 1: 26.20536231994629\tLoss vae 2: 26.407848358154297\tJoint KLD Loss 0.2125486582517624\n",
      "====> Epoch: 1 Average loss: 59.7581\n",
      "====> Test set loss: 51.7693\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 50.720211\n",
      "Loss vae 1: 24.932126998901367\tLoss vae 2: 25.524137496948242\tJoint KLD Loss 0.26394617557525635\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 42.372643\n",
      "Loss vae 1: 21.23501205444336\tLoss vae 2: 20.887840270996094\tJoint KLD Loss 0.2497882843017578\n",
      "====> Epoch: 2 Average loss: 48.3270\n",
      "====> Test set loss: 45.1713\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 44.749535\n",
      "Loss vae 1: 22.338294982910156\tLoss vae 2: 22.102371215820312\tJoint KLD Loss 0.3088669776916504\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 40.252960\n",
      "Loss vae 1: 19.6541805267334\tLoss vae 2: 20.398244857788086\tJoint KLD Loss 0.2005358636379242\n",
      "====> Epoch: 3 Average loss: 42.8524\n",
      "====> Test set loss: 40.6811\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 42.606197\n",
      "Loss vae 1: 21.339506149291992\tLoss vae 2: 21.048254013061523\tJoint KLD Loss 0.21843723952770233\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 36.447239\n",
      "Loss vae 1: 18.05624771118164\tLoss vae 2: 18.214248657226562\tJoint KLD Loss 0.17674225568771362\n",
      "====> Epoch: 4 Average loss: 39.1408\n",
      "====> Test set loss: 37.6129\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 39.100048\n",
      "Loss vae 1: 19.333301544189453\tLoss vae 2: 19.553312301635742\tJoint KLD Loss 0.21343427896499634\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 37.142124\n",
      "Loss vae 1: 18.53815460205078\tLoss vae 2: 18.414400100708008\tJoint KLD Loss 0.18956983089447021\n",
      "====> Epoch: 5 Average loss: 36.4661\n",
      "====> Test set loss: 35.5890\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 35.979572\n",
      "Loss vae 1: 18.224048614501953\tLoss vae 2: 17.58806800842285\tJoint KLD Loss 0.16745442152023315\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 32.028446\n",
      "Loss vae 1: 16.27485466003418\tLoss vae 2: 15.624479293823242\tJoint KLD Loss 0.12911061942577362\n",
      "====> Epoch: 6 Average loss: 34.9231\n",
      "====> Test set loss: 34.3927\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 35.357307\n",
      "Loss vae 1: 17.779319763183594\tLoss vae 2: 17.44977378845215\tJoint KLD Loss 0.1282101422548294\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 35.293564\n",
      "Loss vae 1: 17.21432876586914\tLoss vae 2: 17.960329055786133\tJoint KLD Loss 0.1189032793045044\n",
      "====> Epoch: 7 Average loss: 34.0189\n",
      "====> Test set loss: 33.8169\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 35.527782\n",
      "Loss vae 1: 17.799528121948242\tLoss vae 2: 17.60832405090332\tJoint KLD Loss 0.1199294850230217\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 32.252224\n",
      "Loss vae 1: 16.047147750854492\tLoss vae 2: 16.10417938232422\tJoint KLD Loss 0.10089370608329773\n",
      "====> Epoch: 8 Average loss: 33.4972\n",
      "====> Test set loss: 33.2444\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 31.770781\n",
      "Loss vae 1: 15.625253677368164\tLoss vae 2: 16.047042846679688\tJoint KLD Loss 0.09848357737064362\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 31.882080\n",
      "Loss vae 1: 15.655961990356445\tLoss vae 2: 16.129718780517578\tJoint KLD Loss 0.09640003740787506\n",
      "====> Epoch: 9 Average loss: 33.0711\n",
      "====> Test set loss: 32.8994\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 32.362026\n",
      "Loss vae 1: 16.273691177368164\tLoss vae 2: 15.988391876220703\tJoint KLD Loss 0.09994176030158997\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 33.064423\n",
      "Loss vae 1: 16.55353355407715\tLoss vae 2: 16.40956687927246\tJoint KLD Loss 0.10132092237472534\n",
      "====> Epoch: 10 Average loss: 32.6432\n",
      "====> Test set loss: 32.5411\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 31.301477\n",
      "Loss vae 1: 15.812765121459961\tLoss vae 2: 15.390867233276367\tJoint KLD Loss 0.09784576296806335\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 31.922377\n",
      "Loss vae 1: 15.974754333496094\tLoss vae 2: 15.837833404541016\tJoint KLD Loss 0.10978883504867554\n",
      "====> Epoch: 11 Average loss: 32.4099\n",
      "====> Test set loss: 32.3826\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 30.431797\n",
      "Loss vae 1: 15.238571166992188\tLoss vae 2: 15.106483459472656\tJoint KLD Loss 0.08674147725105286\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 34.057617\n",
      "Loss vae 1: 16.88054084777832\tLoss vae 2: 17.088932037353516\tJoint KLD Loss 0.08814326673746109\n",
      "====> Epoch: 12 Average loss: 32.2905\n",
      "====> Test set loss: 32.1490\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 32.230560\n",
      "Loss vae 1: 15.654850006103516\tLoss vae 2: 16.48711585998535\tJoint KLD Loss 0.08859159797430038\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 32.245598\n",
      "Loss vae 1: 16.248085021972656\tLoss vae 2: 15.914493560791016\tJoint KLD Loss 0.0830191820859909\n",
      "====> Epoch: 13 Average loss: 32.1402\n",
      "====> Test set loss: 32.0117\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 31.332581\n",
      "Loss vae 1: 15.64008617401123\tLoss vae 2: 15.608131408691406\tJoint KLD Loss 0.08436192572116852\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 33.004204\n",
      "Loss vae 1: 16.41915512084961\tLoss vae 2: 16.496540069580078\tJoint KLD Loss 0.08850768208503723\n",
      "====> Epoch: 14 Average loss: 31.9776\n",
      "====> Test set loss: 31.9801\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 31.000168\n",
      "Loss vae 1: 15.543098449707031\tLoss vae 2: 15.344010353088379\tJoint KLD Loss 0.11305929720401764\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 31.008045\n",
      "Loss vae 1: 15.393783569335938\tLoss vae 2: 15.541610717773438\tJoint KLD Loss 0.07265081256628036\n",
      "====> Epoch: 15 Average loss: 31.8321\n",
      "====> Test set loss: 31.8020\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 30.397429\n",
      "Loss vae 1: 15.107150077819824\tLoss vae 2: 15.219382286071777\tJoint KLD Loss 0.07089617848396301\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 29.818293\n",
      "Loss vae 1: 14.956897735595703\tLoss vae 2: 14.799034118652344\tJoint KLD Loss 0.062361352145671844\n",
      "====> Epoch: 16 Average loss: 31.6646\n",
      "====> Test set loss: 31.5854\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 30.833401\n",
      "Loss vae 1: 14.958916664123535\tLoss vae 2: 15.797849655151367\tJoint KLD Loss 0.07663339376449585\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 31.102695\n",
      "Loss vae 1: 15.409332275390625\tLoss vae 2: 15.622247695922852\tJoint KLD Loss 0.07111599296331406\n",
      "====> Epoch: 17 Average loss: 31.5129\n",
      "====> Test set loss: 31.4939\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 31.019272\n",
      "Loss vae 1: 15.716703414916992\tLoss vae 2: 15.229571342468262\tJoint KLD Loss 0.07299801707267761\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 32.560753\n",
      "Loss vae 1: 16.081218719482422\tLoss vae 2: 16.40667724609375\tJoint KLD Loss 0.07285568118095398\n",
      "====> Epoch: 18 Average loss: 31.5062\n",
      "====> Test set loss: 31.4960\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 34.169388\n",
      "Loss vae 1: 16.818281173706055\tLoss vae 2: 17.25339698791504\tJoint KLD Loss 0.09770816564559937\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 31.611168\n",
      "Loss vae 1: 15.775529861450195\tLoss vae 2: 15.763566017150879\tJoint KLD Loss 0.0720706433057785\n",
      "====> Epoch: 19 Average loss: 31.3672\n",
      "====> Test set loss: 31.3455\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 31.628815\n",
      "Loss vae 1: 15.853134155273438\tLoss vae 2: 15.705885887145996\tJoint KLD Loss 0.06979386508464813\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 31.167940\n",
      "Loss vae 1: 15.465276718139648\tLoss vae 2: 15.632719039916992\tJoint KLD Loss 0.06994351744651794\n",
      "====> Epoch: 20 Average loss: 31.4009\n",
      "====> Test set loss: 31.3152\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 31.095165\n",
      "Loss vae 1: 15.393959045410156\tLoss vae 2: 15.620928764343262\tJoint KLD Loss 0.08027620613574982\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 33.047840\n",
      "Loss vae 1: 16.181501388549805\tLoss vae 2: 16.78594970703125\tJoint KLD Loss 0.08038711547851562\n",
      "====> Epoch: 21 Average loss: 31.2469\n",
      "====> Test set loss: 31.2158\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 34.181957\n",
      "Loss vae 1: 17.230377197265625\tLoss vae 2: 16.873193740844727\tJoint KLD Loss 0.07839004695415497\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 32.408825\n",
      "Loss vae 1: 16.213672637939453\tLoss vae 2: 16.123424530029297\tJoint KLD Loss 0.0717272162437439\n",
      "====> Epoch: 22 Average loss: 31.2088\n",
      "====> Test set loss: 31.1580\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 31.261744\n",
      "Loss vae 1: 15.989655494689941\tLoss vae 2: 15.205504417419434\tJoint KLD Loss 0.06658311188220978\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 31.460390\n",
      "Loss vae 1: 15.868657112121582\tLoss vae 2: 15.519463539123535\tJoint KLD Loss 0.0722687840461731\n",
      "====> Epoch: 23 Average loss: 31.1184\n",
      "====> Test set loss: 31.1469\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 32.384262\n",
      "Loss vae 1: 16.603731155395508\tLoss vae 2: 15.714277267456055\tJoint KLD Loss 0.0662543997168541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 29.790981\n",
      "Loss vae 1: 14.997343063354492\tLoss vae 2: 14.743370056152344\tJoint KLD Loss 0.050267454236745834\n",
      "====> Epoch: 24 Average loss: 31.0699\n",
      "====> Test set loss: 31.0906\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 30.544651\n",
      "Loss vae 1: 15.18156909942627\tLoss vae 2: 15.287999153137207\tJoint KLD Loss 0.07508301734924316\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 32.137623\n",
      "Loss vae 1: 16.07442283630371\tLoss vae 2: 15.987509727478027\tJoint KLD Loss 0.0756925567984581\n",
      "====> Epoch: 25 Average loss: 31.0014\n",
      "====> Test set loss: 30.9272\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 28.930866\n",
      "Loss vae 1: 14.69028377532959\tLoss vae 2: 14.180795669555664\tJoint KLD Loss 0.059787970036268234\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 32.278851\n",
      "Loss vae 1: 16.044801712036133\tLoss vae 2: 16.172056198120117\tJoint KLD Loss 0.06199256703257561\n",
      "====> Epoch: 26 Average loss: 31.0274\n",
      "====> Test set loss: 30.9478\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 32.631187\n",
      "Loss vae 1: 16.265708923339844\tLoss vae 2: 16.29326629638672\tJoint KLD Loss 0.07221371680498123\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 29.900616\n",
      "Loss vae 1: 14.635459899902344\tLoss vae 2: 15.20244312286377\tJoint KLD Loss 0.062713623046875\n",
      "====> Epoch: 27 Average loss: 30.9603\n",
      "====> Test set loss: 30.8924\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 29.059902\n",
      "Loss vae 1: 14.717414855957031\tLoss vae 2: 14.282855987548828\tJoint KLD Loss 0.05963195860385895\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 32.065578\n",
      "Loss vae 1: 15.97482967376709\tLoss vae 2: 16.024240493774414\tJoint KLD Loss 0.06651008129119873\n",
      "====> Epoch: 28 Average loss: 30.9603\n",
      "====> Test set loss: 30.8105\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 30.885504\n",
      "Loss vae 1: 15.22237491607666\tLoss vae 2: 15.609933853149414\tJoint KLD Loss 0.05319424718618393\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 30.663094\n",
      "Loss vae 1: 15.021982192993164\tLoss vae 2: 15.594488143920898\tJoint KLD Loss 0.04662259668111801\n",
      "====> Epoch: 29 Average loss: 30.8722\n",
      "====> Test set loss: 30.8443\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 28.703705\n",
      "Loss vae 1: 14.405813217163086\tLoss vae 2: 14.243828773498535\tJoint KLD Loss 0.054062116891145706\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 30.985128\n",
      "Loss vae 1: 15.41454029083252\tLoss vae 2: 15.512285232543945\tJoint KLD Loss 0.05830157920718193\n",
      "====> Epoch: 30 Average loss: 30.8247\n",
      "====> Test set loss: 30.8887\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 30.448694\n",
      "Loss vae 1: 15.133927345275879\tLoss vae 2: 15.252638816833496\tJoint KLD Loss 0.062127139419317245\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 31.641270\n",
      "Loss vae 1: 15.529265403747559\tLoss vae 2: 16.05489730834961\tJoint KLD Loss 0.0571058988571167\n",
      "====> Epoch: 31 Average loss: 30.8627\n",
      "====> Test set loss: 30.7413\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 33.204174\n",
      "Loss vae 1: 16.39837646484375\tLoss vae 2: 16.74159812927246\tJoint KLD Loss 0.0641975998878479\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 29.566561\n",
      "Loss vae 1: 14.423007011413574\tLoss vae 2: 15.070323944091797\tJoint KLD Loss 0.07322811335325241\n",
      "====> Epoch: 32 Average loss: 30.7392\n",
      "====> Test set loss: 30.7250\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 30.835440\n",
      "Loss vae 1: 15.077095031738281\tLoss vae 2: 15.704660415649414\tJoint KLD Loss 0.05368337407708168\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 29.571497\n",
      "Loss vae 1: 14.913538932800293\tLoss vae 2: 14.606940269470215\tJoint KLD Loss 0.05101814866065979\n",
      "====> Epoch: 33 Average loss: 30.7388\n",
      "====> Test set loss: 30.7229\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 28.889967\n",
      "Loss vae 1: 14.419245719909668\tLoss vae 2: 14.41402816772461\tJoint KLD Loss 0.05669289827346802\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 30.216911\n",
      "Loss vae 1: 15.081811904907227\tLoss vae 2: 15.079928398132324\tJoint KLD Loss 0.0551719106733799\n",
      "====> Epoch: 34 Average loss: 30.7431\n",
      "====> Test set loss: 30.6590\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 30.558754\n",
      "Loss vae 1: 15.4121675491333\tLoss vae 2: 15.09074592590332\tJoint KLD Loss 0.055840037763118744\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 31.104500\n",
      "Loss vae 1: 15.236506462097168\tLoss vae 2: 15.812471389770508\tJoint KLD Loss 0.05552211403846741\n",
      "====> Epoch: 35 Average loss: 30.7310\n",
      "====> Test set loss: 30.7577\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 30.229382\n",
      "Loss vae 1: 14.785713195800781\tLoss vae 2: 15.388172149658203\tJoint KLD Loss 0.05549655854701996\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 32.386936\n",
      "Loss vae 1: 15.494205474853516\tLoss vae 2: 16.84134864807129\tJoint KLD Loss 0.05138155817985535\n",
      "====> Epoch: 36 Average loss: 30.6794\n",
      "====> Test set loss: 30.6125\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 31.367434\n",
      "Loss vae 1: 15.766488075256348\tLoss vae 2: 15.546184539794922\tJoint KLD Loss 0.05476175993680954\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 32.355396\n",
      "Loss vae 1: 16.290281295776367\tLoss vae 2: 16.00775146484375\tJoint KLD Loss 0.05736237391829491\n",
      "====> Epoch: 37 Average loss: 30.6598\n",
      "====> Test set loss: 30.6577\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 29.310463\n",
      "Loss vae 1: 14.541056632995605\tLoss vae 2: 14.719324111938477\tJoint KLD Loss 0.0500824898481369\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 31.530088\n",
      "Loss vae 1: 15.704232215881348\tLoss vae 2: 15.767465591430664\tJoint KLD Loss 0.05838934704661369\n",
      "====> Epoch: 38 Average loss: 30.5926\n",
      "====> Test set loss: 30.6202\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 28.975538\n",
      "Loss vae 1: 14.375227928161621\tLoss vae 2: 14.547542572021484\tJoint KLD Loss 0.05276672542095184\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 31.886488\n",
      "Loss vae 1: 15.82938003540039\tLoss vae 2: 16.008258819580078\tJoint KLD Loss 0.04884960502386093\n",
      "====> Epoch: 39 Average loss: 30.6028\n",
      "====> Test set loss: 30.6783\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 29.298300\n",
      "Loss vae 1: 14.249711036682129\tLoss vae 2: 15.00784683227539\tJoint KLD Loss 0.0407429113984108\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 29.630970\n",
      "Loss vae 1: 14.678969383239746\tLoss vae 2: 14.902602195739746\tJoint KLD Loss 0.049397747963666916\n",
      "====> Epoch: 40 Average loss: 30.5529\n",
      "====> Test set loss: 30.5166\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 30.404751\n",
      "Loss vae 1: 14.876791954040527\tLoss vae 2: 15.47237491607666\tJoint KLD Loss 0.05558369308710098\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 30.824015\n",
      "Loss vae 1: 15.759645462036133\tLoss vae 2: 15.01297378540039\tJoint KLD Loss 0.05139518901705742\n",
      "====> Epoch: 41 Average loss: 30.5559\n",
      "====> Test set loss: 30.5708\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 29.642609\n",
      "Loss vae 1: 15.158114433288574\tLoss vae 2: 14.43354606628418\tJoint KLD Loss 0.05094829946756363\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 29.390648\n",
      "Loss vae 1: 14.556803703308105\tLoss vae 2: 14.782133102416992\tJoint KLD Loss 0.05171116441488266\n",
      "====> Epoch: 42 Average loss: 30.5044\n",
      "====> Test set loss: 30.5332\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 30.469904\n",
      "Loss vae 1: 14.75445556640625\tLoss vae 2: 15.666754722595215\tJoint KLD Loss 0.048693202435970306\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 30.205122\n",
      "Loss vae 1: 15.029191017150879\tLoss vae 2: 15.131257057189941\tJoint KLD Loss 0.04467391222715378\n",
      "====> Epoch: 43 Average loss: 30.4556\n",
      "====> Test set loss: 30.5318\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 29.988998\n",
      "Loss vae 1: 14.735514640808105\tLoss vae 2: 15.19688892364502\tJoint KLD Loss 0.05659548565745354\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 31.637114\n",
      "Loss vae 1: 15.95689868927002\tLoss vae 2: 15.624652862548828\tJoint KLD Loss 0.055563412606716156\n",
      "====> Epoch: 44 Average loss: 30.4677\n",
      "====> Test set loss: 30.4898\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 30.354809\n",
      "Loss vae 1: 15.500646591186523\tLoss vae 2: 14.808954238891602\tJoint KLD Loss 0.045208465307950974\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 31.700209\n",
      "Loss vae 1: 16.135217666625977\tLoss vae 2: 15.516496658325195\tJoint KLD Loss 0.04849468916654587\n",
      "====> Epoch: 45 Average loss: 30.4677\n",
      "====> Test set loss: 30.4173\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 29.498569\n",
      "Loss vae 1: 14.716299057006836\tLoss vae 2: 14.72588062286377\tJoint KLD Loss 0.05638839304447174\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 30.668686\n",
      "Loss vae 1: 15.167862892150879\tLoss vae 2: 15.457542419433594\tJoint KLD Loss 0.0432814359664917\n",
      "====> Epoch: 46 Average loss: 30.4888\n",
      "====> Test set loss: 30.4707\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 30.853561\n",
      "Loss vae 1: 15.689780235290527\tLoss vae 2: 15.113848686218262\tJoint KLD Loss 0.049932628870010376\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 30.577026\n",
      "Loss vae 1: 15.187448501586914\tLoss vae 2: 15.350130081176758\tJoint KLD Loss 0.03944841027259827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 47 Average loss: 30.4621\n",
      "====> Test set loss: 30.3816\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 30.608150\n",
      "Loss vae 1: 15.341231346130371\tLoss vae 2: 15.212363243103027\tJoint KLD Loss 0.05455673113465309\n",
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 31.112160\n",
      "Loss vae 1: 15.74246883392334\tLoss vae 2: 15.321748733520508\tJoint KLD Loss 0.04794357344508171\n",
      "====> Epoch: 48 Average loss: 30.3907\n",
      "====> Test set loss: 30.4078\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 31.594730\n",
      "Loss vae 1: 15.447562217712402\tLoss vae 2: 16.094022750854492\tJoint KLD Loss 0.053147003054618835\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 29.907650\n",
      "Loss vae 1: 14.497469902038574\tLoss vae 2: 15.361926078796387\tJoint KLD Loss 0.048253707587718964\n",
      "====> Epoch: 49 Average loss: 30.3841\n",
      "====> Test set loss: 30.3752\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 31.595829\n",
      "Loss vae 1: 15.511667251586914\tLoss vae 2: 16.03175926208496\tJoint KLD Loss 0.05240229517221451\n",
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 30.375883\n",
      "Loss vae 1: 15.115747451782227\tLoss vae 2: 15.207712173461914\tJoint KLD Loss 0.05242437869310379\n",
      "====> Epoch: 50 Average loss: 30.4151\n",
      "====> Test set loss: 30.3500\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50 + 1):\n",
    "    train_joint(train_data, vanilla_vae_zeisel, vae_gumbel, joint_optimizer, epoch)\n",
    "    test_joint(test_data, vanilla_vae_zeisel, vae_gumbel, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.6654, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((vanilla_vae_zeisel(test_data[0:64, :])[0] - test_data[0:64, :])**2) / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.6343, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((vae_gumbel(test_data[0:64, :])[0] - test_data[0:64, :])**2) / 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_losses = []\n",
    "for k in [10, 25, 50, 250]:\n",
    "    vanilla_vae_zeisel = VAE(250, 20)\n",
    "    vanilla_vae_zeisel.to(device)\n",
    "    vanilla_optimizer_zeisel = torch.optim.Adam(vanilla_vae_zeisel.parameters(), \n",
    "                                            lr=lr, \n",
    "                                            betas = (b1,b2))\n",
    "    \n",
    "    vae_gumbel = VAE_Gumbel(500, 250, 20, k = k)\n",
    "    vae_gumbel.to(device)\n",
    "    vae_gumbel_optimizer = torch.optim.Adam(vae_gumbel.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))\n",
    "    for epoch in range(1, 50 + 1):\n",
    "        train_joint(train_data, vanilla_vae_zeisel, vae_gumbel, vanilla_optimizer_zeisel, vae_gumbel_optimizer, epoch)\n",
    "    print(\"Gumbel Reconstruction Loss with Joint Training at k {}\".format(k))\n",
    "    with torch.no_grad():\n",
    "        final_losses.append(torch.sum((vae_gumbel(test_data[0:64, :])[0] - test_data[0:64, :])**2) / 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5349, 0.5274, 0.4967, 0.4788, 0.4552, 0.4752, 0.4918, 0.4644, 0.4842,\n",
       "        0.4379, 0.4471, 0.4018, 0.4543, 0.3787, 0.3776, 0.4840, 0.3472, 0.3995,\n",
       "        0.4212, 0.4618, 0.4138, 0.3804, 0.4153, 0.4940, 0.3286, 0.4405, 0.4333,\n",
       "        0.4224, 0.4106, 0.3891, 0.4117, 0.4167, 0.3601, 0.4228, 0.4414, 0.4511,\n",
       "        0.3945, 0.4099, 0.3780, 0.4205, 0.4538, 0.3754, 0.4761, 0.4042, 0.3593,\n",
       "        0.3754, 0.3514, 0.4047, 0.3747, 0.3668, 0.2910, 0.4076, 0.4145, 0.2974,\n",
       "        0.4268, 0.4075, 0.3685, 0.3524, 0.3864, 0.4225, 0.3351, 0.3585, 0.2713,\n",
       "        0.3217, 0.3263, 0.3203, 0.3258, 0.3880, 0.3618, 0.3389, 0.3422, 0.3264,\n",
       "        0.3431, 0.2994, 0.3474, 0.3597, 0.3084, 0.3165, 0.3374, 0.2574, 0.3257,\n",
       "        0.2464, 0.3373, 0.2775, 0.3463, 0.2811, 0.3196, 0.2957, 0.3210, 0.2780,\n",
       "        0.3342, 0.2280, 0.2287, 0.3220, 0.3781, 0.2444, 0.2456, 0.2455, 0.3262,\n",
       "        0.2350, 0.2349, 0.1963, 0.2622, 0.2510, 0.2336, 0.2822, 0.2149, 0.3032,\n",
       "        0.1881, 0.2813, 0.1917, 0.2142, 0.2372, 0.2873, 0.2665, 0.2520, 0.3042,\n",
       "        0.2000, 0.2318, 0.2368, 0.2594, 0.2352, 0.2810, 0.2233, 0.2576, 0.1122,\n",
       "        0.2510, 0.2504, 0.1717, 0.2079, 0.2175, 0.2300, 0.1865, 0.1829, 0.2433,\n",
       "        0.1916, 0.2253, 0.1770, 0.2385, 0.1799, 0.2096, 0.2478, 0.1971, 0.2192,\n",
       "        0.2239, 0.2220, 0.2143, 0.2318, 0.1853, 0.2331, 0.1566, 0.1951, 0.1993,\n",
       "        0.1634, 0.2234, 0.2000, 0.1813, 0.1715, 0.1922, 0.1853, 0.2080, 0.1982,\n",
       "        0.1852, 0.2262, 0.1476, 0.1523, 0.1560, 0.1989, 0.1725, 0.1640, 0.2118,\n",
       "        0.1736, 0.1478, 0.1686, 0.2068, 0.1742, 0.2200, 0.1339, 0.1861, 0.2127,\n",
       "        0.1780, 0.1837, 0.1673, 0.2498, 0.1791, 0.1797, 0.1777, 0.1836, 0.2132,\n",
       "        0.1686, 0.1978, 0.1878, 0.1623, 0.1683, 0.2150, 0.1484, 0.1292, 0.1359,\n",
       "        0.1800, 0.2434, 0.1765, 0.1600, 0.1146, 0.0893, 0.1983, 0.1972, 0.1997,\n",
       "        0.1349, 0.2011, 0.1658, 0.1222, 0.0583, 0.2265, 0.1568, 0.1422, 0.2342,\n",
       "        0.1590, 0.1550, 0.1815, 0.1718, 0.1814, 0.1451, 0.1216, 0.1500, 0.1986,\n",
       "        0.1944, 0.1008, 0.2006, 0.1222, 0.1850, 0.1582, 0.1534, 0.1878, 0.1309,\n",
       "        0.1185, 0.0948, 0.1433, 0.1306, 0.1656, 0.1455, 0.1252, 0.1818, 0.1405,\n",
       "        0.1445, 0.1359, 0.1657, 0.1735, 0.1386, 0.1153, 0.1141, 0.1591, 0.1688,\n",
       "        0.1372, 0.1212, 0.1478, 0.1274, 0.1334, 0.1635, 0.1174, 0.2110, 0.1680,\n",
       "        0.2045, 0.1027, 0.1714, 0.1778, 0.1404, 0.1361, 0.1414, 0.1600, 0.1370,\n",
       "        0.1648, 0.1263, 0.0968, 0.0915, 0.1420, 0.1041, 0.1092, 0.1329, 0.1568,\n",
       "        0.1677, 0.0986, 0.1713, 0.1639, 0.1562, 0.2029, 0.1316, 0.1367, 0.1397,\n",
       "        0.1104, 0.1694, 0.1351, 0.1560, 0.1743, 0.1199, 0.1407, 0.1132, 0.1101,\n",
       "        0.1267, 0.1047, 0.1205, 0.1565, 0.1133, 0.1469, 0.1234, 0.1555, 0.0627,\n",
       "        0.1031, 0.1159, 0.1656, 0.1204, 0.1302, 0.1551, 0.1402, 0.1633, 0.0988,\n",
       "        0.1042, 0.1015, 0.1092, 0.1622, 0.0916, 0.0761, 0.1145, 0.0935, 0.1188,\n",
       "        0.0789, 0.1207, 0.1377, 0.1311, 0.1820, 0.0913, 0.1256, 0.1732, 0.1020,\n",
       "        0.0989, 0.1187, 0.1627, 0.1819, 0.0753, 0.0812, 0.1249, 0.1256, 0.1411,\n",
       "        0.0874, 0.1004, 0.1504, 0.1490, 0.1195, 0.1554, 0.1096, 0.1116, 0.1188,\n",
       "        0.0970, 0.1196, 0.0977, 0.1172, 0.1041, 0.1172, 0.1058, 0.1064, 0.0651,\n",
       "        0.1140, 0.1574, 0.0933, 0.1045, 0.1081, 0.0788, 0.0825, 0.0698, 0.0935,\n",
       "        0.0749, 0.1095, 0.1054, 0.1367, 0.1204, 0.1330, 0.1070, 0.1467, 0.0938,\n",
       "        0.1236, 0.1357, 0.0967, 0.0404, 0.1246, 0.0870, 0.1597, 0.1077, 0.1341,\n",
       "        0.0976, 0.0993, 0.1008, 0.0931, 0.1483, 0.1285, 0.1214, 0.1118, 0.0870,\n",
       "        0.1495, 0.1226, 0.1099, 0.0938, 0.1202, 0.0634, 0.0907, 0.0954, 0.1003,\n",
       "        0.0946, 0.1154, 0.1515, 0.1178, 0.1203, 0.0753, 0.1058, 0.1244, 0.1045,\n",
       "        0.0960, 0.0813, 0.0881, 0.2061, 0.1044, 0.0968, 0.0993, 0.0926, 0.0654,\n",
       "        0.1104, 0.0954, 0.0927, 0.1138, 0.1218, 0.0000, 0.0925, 0.0687, 0.0993,\n",
       "        0.0996, 0.1391, 0.1301, 0.0705, 0.1424, 0.1391, 0.1357, 0.1153, 0.1393,\n",
       "        0.0848, 0.1144, 0.1532, 0.0599, 0.1414, 0.0768, 0.0600, 0.1240, 0.1304,\n",
       "        0.1058, 0.1434, 0.0730, 0.0870, 0.1117, 0.0823, 0.1273, 0.1186, 0.0981,\n",
       "        0.0883, 0.1005, 0.0719, 0.1358, 0.0864, 0.1048, 0.1210, 0.0712, 0.1031,\n",
       "        0.1193, 0.0702, 0.0817, 0.0973, 0.1185, 0.0661, 0.0979, 0.1039, 0.1083,\n",
       "        0.0376, 0.0675, 0.0836, 0.0913, 0.1230, 0.1006, 0.0692, 0.1192, 0.1034,\n",
       "        0.0842, 0.0785, 0.0831, 0.0646, 0.1155, 0.1066, 0.0099, 0.0960, 0.0843,\n",
       "        0.0937, 0.0585, 0.0762, 0.0866, 0.0803], device='cuda:0',\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_gumbel(test_data[0, :])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6068, 0.6551, 0.5780, 0.5294, 0.4702, 0.6572, 0.6578, 0.5857, 0.6613,\n",
       "        0.5666, 0.5739, 0.3545, 0.6372, 0.3434, 0.2615, 0.6320, 0.2276, 0.3662,\n",
       "        0.5136, 0.4590, 0.6113, 0.4520, 0.4931, 0.6468, 0.2969, 0.5324, 0.5899,\n",
       "        0.5625, 0.5376, 0.5772, 0.5386, 0.6193, 0.2588, 0.5929, 0.5571, 0.6239,\n",
       "        0.6625, 0.5682, 0.5634, 0.5998, 0.5809, 0.5121, 0.6033, 0.4546, 0.3926,\n",
       "        0.5438, 0.5582, 0.4037, 0.4857, 0.4359, 0.3070, 0.5646, 0.6615, 0.2881,\n",
       "        0.7467, 0.4565, 0.5441, 0.3444, 0.5953, 0.4732, 0.5364, 0.4981, 0.2493,\n",
       "        0.1810, 0.5172, 0.2361, 0.5444, 0.5303, 0.4621, 0.1596, 0.4457, 0.3931,\n",
       "        0.1700, 0.1679, 0.2717, 0.5608, 0.4428, 0.3694, 0.4735, 0.1499, 0.4308,\n",
       "        0.0000, 0.4117, 0.5303, 0.4604, 0.4424, 0.5058, 0.4513, 0.4016, 0.4232,\n",
       "        0.4628, 0.3828, 0.2537, 0.2999, 0.4363, 0.3020, 0.3709, 0.1700, 0.2000,\n",
       "        0.3182, 0.4113, 0.1407, 0.5723, 0.5377, 0.0000, 0.4232, 0.3623, 0.0000,\n",
       "        0.2759, 0.0000, 0.2275, 0.2652, 0.3372, 0.5568, 0.3372, 0.2127, 0.0000,\n",
       "        0.2939, 0.1950, 0.0000, 0.3801, 0.4553, 0.2560, 0.4421, 0.4307, 0.2259,\n",
       "        0.3333, 0.3413, 0.0000, 0.0000, 0.3504, 0.7340, 0.2103, 0.0000, 0.0000,\n",
       "        0.2038, 0.2447, 0.2058, 0.2181, 0.0000, 0.3170, 0.2560, 0.2153, 0.3333,\n",
       "        0.3621, 0.1982, 0.0000, 0.0000, 0.0000, 0.3731, 0.0000, 0.2211, 0.2242,\n",
       "        0.2853, 0.0000, 0.2702, 0.2080, 0.1237, 0.0000, 0.0000, 0.3878, 0.2211,\n",
       "        0.0000, 0.0000, 0.0000, 0.1950, 0.0000, 0.0000, 0.0000, 0.0000, 0.2447,\n",
       "        0.0000, 0.1843, 0.2354, 0.0000, 0.1966, 0.1215, 0.3263, 0.2181, 0.0000,\n",
       "        0.0000, 0.3878, 0.5943, 0.4492, 0.4157, 0.0000, 0.2626, 0.2447, 0.0000,\n",
       "        0.0000, 0.2702, 0.2560, 0.0000, 0.2789, 0.0000, 0.1906, 0.0000, 0.4000,\n",
       "        0.2626, 0.1707, 0.0000, 0.2447, 0.1982, 0.0000, 0.2626, 0.0000, 0.0000,\n",
       "        0.3297, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2500, 0.0000, 0.3554, 0.6477, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2058, 0.4421, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.1920, 0.0000, 0.0000, 0.0000, 0.2626, 0.5000, 0.0000, 0.2789, 0.0000,\n",
       "        0.2626, 0.0000, 0.2500, 0.2891, 0.2354, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2500, 0.2103, 0.3731, 0.2277, 0.2398, 0.0000, 0.0000, 0.5000, 0.0000,\n",
       "        0.6667, 0.0000, 0.0000, 0.5466, 0.0000, 0.2789, 0.0000, 0.5781, 0.2500,\n",
       "        0.0000, 0.0000, 0.3784, 0.0000, 0.0000, 0.1982, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.4771, 0.0000, 0.4582, 0.0000, 0.0000, 0.0000,\n",
       "        0.2153, 0.5000, 0.0000, 0.2789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2626, 0.0000, 0.0000, 0.3010, 0.2500, 0.0000, 0.0000, 0.4283, 0.0000,\n",
       "        0.2354, 0.0000, 0.0000, 0.0000, 0.3801, 0.0000, 0.0000, 0.3155, 0.1821,\n",
       "        0.0000, 0.2277, 0.0000, 0.3155, 0.0000, 0.0000, 0.0000, 0.2080, 0.0000,\n",
       "        0.0000, 0.2702, 0.0000, 0.0000, 0.5372, 0.2886, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2891, 0.1810, 0.0000, 0.0000, 0.4421, 0.0000,\n",
       "        0.1854, 0.2560, 0.0000, 0.2000, 0.0000, 0.3562, 0.0000, 0.0000, 0.2211,\n",
       "        0.1763, 0.0000, 0.0000, 0.0000, 0.2058, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.2560, 0.2626, 0.0000, 0.2789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.2702, 0.0000, 0.2500, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0860, 0.0000, 0.0000, 0.0000, 0.0000, 0.3155,\n",
       "        0.3010, 0.2702, 0.0000, 0.2789, 0.0000, 0.0000, 0.3333, 0.4421, 0.2211,\n",
       "        0.0000, 0.0000, 0.2702, 0.4628, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7602, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2891, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2891, 0.0000, 0.0969, 0.4421, 0.2211, 0.0000,\n",
       "        0.0000, 0.0000, 0.2789, 0.0000, 0.4163, 0.0000, 0.2560, 0.4163, 0.0000,\n",
       "        0.0000, 0.0000, 0.4582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
       "        0.4771, 0.0000, 0.2447, 0.0000, 0.0000, 0.2211, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1440, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.3333, 0.3263, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.2978, 0.0000, 0.0000, 0.0000, 0.0000, 0.2211,\n",
       "        0.0000, 0.2447, 0.0000, 0.0000, 0.0000, 0.4771, 0.0000, 0.0000, 0.2626,\n",
       "        0.0000, 0.2447, 0.0000, 0.0000, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5567, 0.5163, 0.4704, 0.4493, 0.4056, 0.4780, 0.5040, 0.4574, 0.4642,\n",
       "        0.4493, 0.3840, 0.3351, 0.4668, 0.3232, 0.3194, 0.4631, 0.2301, 0.3734,\n",
       "        0.4104, 0.4340, 0.4029, 0.3576, 0.3383, 0.4662, 0.2737, 0.4077, 0.3662,\n",
       "        0.4180, 0.4007, 0.3476, 0.3970, 0.4113, 0.2377, 0.3731, 0.4503, 0.4273,\n",
       "        0.3733, 0.3936, 0.3678, 0.4015, 0.4214, 0.3478, 0.4196, 0.3530, 0.3519,\n",
       "        0.3371, 0.3481, 0.3700, 0.2923, 0.4254, 0.2635, 0.3925, 0.3928, 0.2604,\n",
       "        0.4173, 0.3662, 0.3461, 0.3125, 0.3394, 0.4337, 0.3063, 0.3426, 0.2847,\n",
       "        0.2596, 0.2968, 0.2763, 0.3311, 0.3793, 0.3387, 0.3180, 0.3449, 0.2736,\n",
       "        0.2344, 0.2327, 0.3148, 0.3415, 0.2950, 0.2932, 0.2958, 0.2145, 0.3119,\n",
       "        0.1669, 0.3295, 0.2516, 0.3234, 0.2192, 0.3227, 0.3188, 0.2986, 0.2863,\n",
       "        0.3453, 0.1990, 0.2382, 0.2161, 0.3761, 0.2013, 0.2577, 0.1789, 0.3311,\n",
       "        0.2012, 0.2155, 0.1727, 0.2801, 0.2502, 0.1598, 0.3068, 0.2291, 0.2302,\n",
       "        0.1302, 0.2056, 0.0892, 0.1937, 0.2033, 0.2734, 0.2468, 0.1661, 0.1421,\n",
       "        0.1619, 0.1988, 0.1101, 0.2223, 0.2806, 0.2580, 0.1966, 0.2348, 0.2566,\n",
       "        0.2407, 0.2635, 0.1906, 0.1236, 0.1764, 0.2832, 0.1682, 0.1418, 0.2734,\n",
       "        0.1605, 0.2132, 0.1608, 0.2079, 0.1194, 0.1266, 0.2197, 0.1270, 0.1801,\n",
       "        0.1096, 0.2165, 0.1348, 0.1971, 0.1826, 0.3015, 0.1481, 0.1533, 0.1864,\n",
       "        0.1628, 0.1662, 0.2044, 0.1310, 0.0995, 0.0725, 0.2076, 0.1535, 0.2464,\n",
       "        0.1896, 0.2111, 0.1009, 0.1176, 0.1358, 0.0908, 0.1236, 0.0629, 0.1871,\n",
       "        0.1702, 0.0926, 0.2016, 0.1690, 0.0834, 0.0316, 0.1616, 0.1966, 0.1869,\n",
       "        0.0677, 0.1680, 0.1690, 0.3043, 0.2191, 0.1410, 0.1462, 0.1860, 0.1006,\n",
       "        0.0415, 0.1853, 0.1435, 0.0499, 0.1447, 0.1574, 0.1090, 0.1106, 0.1026,\n",
       "        0.1090, 0.0782, 0.1732, 0.1388, 0.1143, 0.0843, 0.1714, 0.2907, 0.1581,\n",
       "        0.1308, 0.0616, 0.1199, 0.0871, 0.2105, 0.0000, 0.1173, 0.0844, 0.0000,\n",
       "        0.1489, 0.0766, 0.1509, 0.1856, 0.1468, 0.1046, 0.1135, 0.1169, 0.2005,\n",
       "        0.0000, 0.0901, 0.2017, 0.1230, 0.1747, 0.0524, 0.1036, 0.2340, 0.0889,\n",
       "        0.1247, 0.0554, 0.1071, 0.0923, 0.0958, 0.1631, 0.1183, 0.1152, 0.0531,\n",
       "        0.1277, 0.1174, 0.1393, 0.1518, 0.0874, 0.0967, 0.0385, 0.1082, 0.1601,\n",
       "        0.1176, 0.0827, 0.1367, 0.1013, 0.0990, 0.2351, 0.0843, 0.2995, 0.1217,\n",
       "        0.2000, 0.0208, 0.0000, 0.2527, 0.0505, 0.1065, 0.0000, 0.1711, 0.1598,\n",
       "        0.0455, 0.0906, 0.0801, 0.0809, 0.0932, 0.0985, 0.0943, 0.1139, 0.1285,\n",
       "        0.0000, 0.0663, 0.1070, 0.1019, 0.0000, 0.2707, 0.1220, 0.1249, 0.1052,\n",
       "        0.0627, 0.1572, 0.1137, 0.2229, 0.0190, 0.0667, 0.1004, 0.1169, 0.0678,\n",
       "        0.1221, 0.0879, 0.0624, 0.1592, 0.1163, 0.0844, 0.0717, 0.1393, 0.1829,\n",
       "        0.1057, 0.1294, 0.1407, 0.0880, 0.0901, 0.0000, 0.1266, 0.1253, 0.0965,\n",
       "        0.0836, 0.0904, 0.1282, 0.1601, 0.1036, 0.0762, 0.1196, 0.0731, 0.0904,\n",
       "        0.0661, 0.1595, 0.1429, 0.0884, 0.2290, 0.0887, 0.0504, 0.0000, 0.0935,\n",
       "        0.0431, 0.1240, 0.1784, 0.1678, 0.0571, 0.0787, 0.0840, 0.0936, 0.1195,\n",
       "        0.0969, 0.1028, 0.1024, 0.1971, 0.1168, 0.1557, 0.0911, 0.1168, 0.1155,\n",
       "        0.0459, 0.0921, 0.1034, 0.0814, 0.0933, 0.1586, 0.0693, 0.1024, 0.0458,\n",
       "        0.0896, 0.1875, 0.0943, 0.1006, 0.0893, 0.1011, 0.0700, 0.1839, 0.0869,\n",
       "        0.0439, 0.0887, 0.0863, 0.1308, 0.1116, 0.1318, 0.0689, 0.0000, 0.0643,\n",
       "        0.1032, 0.1366, 0.0975, 0.0912, 0.1358, 0.0507, 0.1964, 0.0830, 0.1801,\n",
       "        0.0894, 0.1041, 0.0992, 0.0322, 0.0000, 0.1344, 0.1247, 0.1036, 0.0955,\n",
       "        0.1351, 0.1122, 0.0848, 0.1014, 0.0699, 0.1785, 0.0907, 0.0468, 0.0309,\n",
       "        0.0911, 0.0521, 0.0000, 0.1239, 0.0371, 0.0466, 0.1097, 0.1267, 0.0481,\n",
       "        0.0746, 0.0570, 0.0720, 0.1812, 0.0920, 0.0810, 0.0864, 0.0871, 0.0580,\n",
       "        0.0860, 0.0732, 0.0915, 0.0886, 0.0520, 0.1529, 0.0550, 0.0742, 0.1501,\n",
       "        0.0952, 0.1464, 0.0752, 0.0546, 0.1880, 0.1560, 0.1664, 0.1152, 0.1520,\n",
       "        0.0564, 0.0993, 0.1881, 0.1007, 0.0000, 0.0578, 0.0185, 0.1262, 0.1547,\n",
       "        0.0944, 0.2043, 0.0852, 0.0552, 0.0000, 0.0776, 0.1147, 0.1154, 0.0794,\n",
       "        0.0902, 0.0865, 0.0705, 0.1447, 0.0713, 0.0912, 0.0442, 0.0409, 0.0746,\n",
       "        0.1106, 0.0572, 0.0905, 0.0734, 0.1437, 0.0762, 0.0920, 0.1041, 0.0021,\n",
       "        0.1501, 0.0827, 0.0726, 0.0761, 0.0936, 0.0513, 0.0770, 0.1354, 0.0939,\n",
       "        0.0744, 0.0667, 0.0000, 0.0986, 0.1482, 0.0997, 0.0807, 0.0804, 0.0646,\n",
       "        0.0520, 0.0686, 0.0428, 0.0528, 0.0648], device='cuda:0',\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_vae_zeisel(test_data[0,:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176, 199, 212, 215, 225, 262, 263, 265, 267, 279, 283, 292, 311,\n",
       "       331, 376, 391, 407, 409, 445, 447, 454, 476, 488])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vanilla_vae_zeisel(test_data[0,:])[0].clone().detach().cpu().numpy() == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(499, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(vae_gumbel(test_data[0, :])[0] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(263, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(test_data[0, :] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(488, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(vanilla_vae_zeisel(test_data[0, :])[0] != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = vae_gumbel.weight_creator(test_data[0:64, :])\n",
    "subset_indices = sample_subset(w, k = 200, t = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000,\n",
       "        200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000,\n",
       "        200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000,\n",
       "        200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000,\n",
       "        200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000,\n",
       "        200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000,\n",
       "        200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000,\n",
       "        200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000,\n",
       "        200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000, 200.0000,\n",
       "        200.0000], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_indices.sum(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_unique(arr):\n",
    "    return len(np.unique(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([182, 252, 149, ..., 166, 220, 193])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.apply_along_axis(arr=test_data.clone().detach().cpu().numpy(), axis = 1, func1d=len_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3677494199535963"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_data[:, 499].clone().detach().cpu().numpy() > 0) /len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408352668213457"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_data[:, 60].clone().detach().cpu().numpy() > 0) /len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
