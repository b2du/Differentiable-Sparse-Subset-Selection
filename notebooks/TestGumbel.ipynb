{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the Xie Ermon paper of Continuous relaxation for subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "img_size = 28\n",
    "channels = 1\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "z_size = 20\n",
    "\n",
    "n = 28 * 28\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sio.loadmat(\"../data/zeisel/CITEseq.mat\")\n",
    "data= a['G'].T\n",
    "N,d=data.shape\n",
    "#transformation from integer entries \n",
    "data=np.log(data+np.ones(data.shape))\n",
    "for i in range(N):\n",
    "    data[i,:]=data[i,:]/np.linalg.norm(data[i,:])\n",
    "\n",
    "#load labels from file\n",
    "a = sio.loadmat(\"../data/zeisel/CITEseq-labels.mat\")\n",
    "l_aux = a['labels']\n",
    "labels = np.array([i for [i] in l_aux])\n",
    "\n",
    "#load names from file\n",
    "a = sio.loadmat(\"../data/zeisel/CITEseq_names.mat\")\n",
    "names=[a['citeseq_names'][i][0][0] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(data.shape[0]))\n",
    "upto = int(.8 * len(data))\n",
    "\n",
    "train_data = data[slices[:upto]]\n",
    "test_data = data[slices[upto:]]\n",
    "\n",
    "train_data = Tensor(train_data).to(device)\n",
    "test_data = Tensor(test_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_keys(w):\n",
    "    # sample some gumbels\n",
    "    uniform = (1.0 - EPSILON) * torch.rand_like(w) + EPSILON\n",
    "    z = torch.log(-torch.log(uniform))\n",
    "    w = w + z\n",
    "    return w\n",
    "\n",
    "\n",
    "def continuous_topk(w, k, t, separate=False):\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    khot_list = []\n",
    "    onehot_approx = torch.zeros_like(w, dtype = torch.float32)\n",
    "    for i in range(k):\n",
    "        ### conver the following into pytorch\n",
    "        #khot_mask = tf.maximum(1.0 - onehot_approx, EPSILON)\n",
    "        max_mask = 1 - onehot_approx < EPSILON\n",
    "        khot_mask = 1 - onehot_approx\n",
    "        khot_mask[max_mask] = EPSILON\n",
    "        \n",
    "        w += torch.log(khot_mask)\n",
    "        #onehot_approx = tf.nn.softmax(w / t, axis=-1)\n",
    "        onehot_approx = softmax(w/t)\n",
    "        khot_list.append(onehot_approx)\n",
    "    if separate:\n",
    "        return torch.stack(khot_list)\n",
    "    else:\n",
    "        return torch.sum(torch.stack(khot_list), dim = 0) \n",
    "\n",
    "\n",
    "def sample_subset(w, k, t=0.1):\n",
    "    '''\n",
    "    Args:\n",
    "        w (Tensor): Float Tensor of weights for each element. In gumbel mode\n",
    "            these are interpreted as log probabilities\n",
    "        k (int): number of elements in the subset sample\n",
    "        t (float): temperature of the softmax\n",
    "    '''\n",
    "    w = gumbel_keys(w)\n",
    "    return continuous_topk(w, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_Gumbel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size, k, t = 0.1):\n",
    "        super(VAE_Gumbel, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self.t = t\n",
    "        \n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        w = self.weight_creator(x)\n",
    "        subset_indices = sample_subset(w, self.k, self.t)\n",
    "        x = x * subset_indices\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(batch_data)\n",
    "        loss = loss_function(recon_batch, batch_data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df, model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    inds = np.arange(df.shape[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = df[batch_ind, :]\n",
    "            batch_data = batch_data.to(device)\n",
    "            recon_batch, mu, logvar = model(batch_data)\n",
    "            test_loss += loss_function(recon_batch, batch_data, mu, logvar).item()\n",
    "\n",
    "\n",
    "    test_loss /= len(df)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel = VAE_Gumbel(500, 250, 20, k = 50)\n",
    "vae_gumbel.to(device)\n",
    "vae_gumbel_optimizer = torch.optim.Adam(vae_gumbel.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 351.419769\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 93.473923\n",
      "====> Epoch: 1 Average loss: 201.9731\n",
      "====> Test set loss: 88.6152\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 87.780014\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 77.534172\n",
      "====> Epoch: 2 Average loss: 80.9494\n",
      "====> Test set loss: 76.5628\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 76.366646\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 72.925552\n",
      "====> Epoch: 3 Average loss: 74.7947\n",
      "====> Test set loss: 73.7309\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 72.869598\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 72.798859\n",
      "====> Epoch: 4 Average loss: 73.2649\n",
      "====> Test set loss: 72.8139\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 73.335297\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 72.797577\n",
      "====> Epoch: 5 Average loss: 72.4091\n",
      "====> Test set loss: 72.0782\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 71.793350\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 71.692802\n",
      "====> Epoch: 6 Average loss: 71.8642\n",
      "====> Test set loss: 71.6768\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 69.526794\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 70.283836\n",
      "====> Epoch: 7 Average loss: 71.2473\n",
      "====> Test set loss: 70.8740\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 71.214333\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 69.621964\n",
      "====> Epoch: 8 Average loss: 70.5291\n",
      "====> Test set loss: 70.0870\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 69.428154\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 67.996437\n",
      "====> Epoch: 9 Average loss: 69.7739\n",
      "====> Test set loss: 69.4165\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 70.768951\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 67.092079\n",
      "====> Epoch: 10 Average loss: 68.9996\n",
      "====> Test set loss: 68.5028\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 67.422577\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 67.499451\n",
      "====> Epoch: 11 Average loss: 68.0274\n",
      "====> Test set loss: 67.6850\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 66.941307\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 64.871284\n",
      "====> Epoch: 12 Average loss: 67.1969\n",
      "====> Test set loss: 66.9268\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 67.863762\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 66.141418\n",
      "====> Epoch: 13 Average loss: 66.5495\n",
      "====> Test set loss: 66.5136\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 66.323639\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 65.228409\n",
      "====> Epoch: 14 Average loss: 66.2013\n",
      "====> Test set loss: 66.0763\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 66.594604\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 65.829933\n",
      "====> Epoch: 15 Average loss: 65.8976\n",
      "====> Test set loss: 65.9249\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 64.990822\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 66.764160\n",
      "====> Epoch: 16 Average loss: 65.7509\n",
      "====> Test set loss: 65.8316\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 66.017097\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 66.178947\n",
      "====> Epoch: 17 Average loss: 65.6770\n",
      "====> Test set loss: 65.7053\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 65.678879\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 62.305286\n",
      "====> Epoch: 18 Average loss: 65.5823\n",
      "====> Test set loss: 65.5952\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 64.882080\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 65.122520\n",
      "====> Epoch: 19 Average loss: 65.5565\n",
      "====> Test set loss: 65.5438\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 66.951759\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 65.262802\n",
      "====> Epoch: 20 Average loss: 65.4307\n",
      "====> Test set loss: 65.4557\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 64.674072\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 66.144867\n",
      "====> Epoch: 21 Average loss: 65.4062\n",
      "====> Test set loss: 65.3387\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 64.881828\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 64.373116\n",
      "====> Epoch: 22 Average loss: 65.3091\n",
      "====> Test set loss: 65.3011\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 67.120201\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 66.179321\n",
      "====> Epoch: 23 Average loss: 65.2049\n",
      "====> Test set loss: 65.2088\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 65.203735\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 64.063484\n",
      "====> Epoch: 24 Average loss: 65.0923\n",
      "====> Test set loss: 64.9573\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 63.785255\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 63.641029\n",
      "====> Epoch: 25 Average loss: 64.9324\n",
      "====> Test set loss: 64.8706\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 64.264122\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 65.739174\n",
      "====> Epoch: 26 Average loss: 64.7193\n",
      "====> Test set loss: 64.6714\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 63.052460\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 63.336506\n",
      "====> Epoch: 27 Average loss: 64.5013\n",
      "====> Test set loss: 64.3858\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 65.341782\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 64.644806\n",
      "====> Epoch: 28 Average loss: 64.2603\n",
      "====> Test set loss: 64.1996\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 63.983040\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 63.218864\n",
      "====> Epoch: 29 Average loss: 64.1471\n",
      "====> Test set loss: 64.1481\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 63.814381\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 63.567307\n",
      "====> Epoch: 30 Average loss: 64.0865\n",
      "====> Test set loss: 64.1019\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 62.974308\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 65.022552\n",
      "====> Epoch: 31 Average loss: 64.0212\n",
      "====> Test set loss: 64.0584\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 64.030060\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 63.070045\n",
      "====> Epoch: 32 Average loss: 63.9871\n",
      "====> Test set loss: 64.0440\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 64.068153\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 64.434776\n",
      "====> Epoch: 33 Average loss: 63.9506\n",
      "====> Test set loss: 63.9934\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 63.264442\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 64.969139\n",
      "====> Epoch: 34 Average loss: 63.9062\n",
      "====> Test set loss: 63.9344\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 62.998215\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 64.025887\n",
      "====> Epoch: 35 Average loss: 63.8877\n",
      "====> Test set loss: 63.9010\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 65.159668\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 64.148918\n",
      "====> Epoch: 36 Average loss: 63.8389\n",
      "====> Test set loss: 63.8827\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 63.869789\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 64.139038\n",
      "====> Epoch: 37 Average loss: 63.8067\n",
      "====> Test set loss: 63.8277\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 63.615879\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 64.068573\n",
      "====> Epoch: 38 Average loss: 63.7968\n",
      "====> Test set loss: 63.7995\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 63.313324\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 63.180126\n",
      "====> Epoch: 39 Average loss: 63.7591\n",
      "====> Test set loss: 63.7917\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 61.719963\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 63.871281\n",
      "====> Epoch: 40 Average loss: 63.7232\n",
      "====> Test set loss: 63.7310\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 63.542355\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 64.239059\n",
      "====> Epoch: 41 Average loss: 63.6904\n",
      "====> Test set loss: 63.6996\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 63.990662\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 62.727749\n",
      "====> Epoch: 42 Average loss: 63.6705\n",
      "====> Test set loss: 63.7097\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 63.306362\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 63.940918\n",
      "====> Epoch: 43 Average loss: 63.6472\n",
      "====> Test set loss: 63.6408\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 63.796673\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 62.895687\n",
      "====> Epoch: 44 Average loss: 63.6024\n",
      "====> Test set loss: 63.6119\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 64.266350\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 64.409424\n",
      "====> Epoch: 45 Average loss: 63.5834\n",
      "====> Test set loss: 63.6240\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 63.235523\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 64.143806\n",
      "====> Epoch: 46 Average loss: 63.5753\n",
      "====> Test set loss: 63.5926\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 62.288345\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 63.912827\n",
      "====> Epoch: 47 Average loss: 63.5384\n",
      "====> Test set loss: 63.5612\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 65.029495\n",
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 63.936909\n",
      "====> Epoch: 48 Average loss: 63.5170\n",
      "====> Test set loss: 63.5359\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 62.582272\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 62.333458\n",
      "====> Epoch: 49 Average loss: 63.5112\n",
      "====> Test set loss: 63.5309\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 64.161392\n",
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 65.339439\n",
      "====> Epoch: 50 Average loss: 63.4835\n",
      "====> Test set loss: 63.5456\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50 + 1):\n",
    "        train(train_data, vae_gumbel, vae_gumbel_optimizer, epoch)\n",
    "        #with torch.no_grad():\n",
    "        #    model.diag.data[torch.abs(model.diag) < 0.05] = 0\n",
    "        test(test_data, vae_gumbel, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2363)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((vae_gumbel(train_data)[0][1,:] - train_data[1,:])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2005)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((vae_gumbel(test_data)[0][1,:] - test_data[1,:])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae_gumbel.state_dict(), BASE_PATH_DATA + \"../data/models/zeisel/gumbel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2706, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((vae_gumbel(train_data)[0] - train_data)**2) / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2654, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((vae_gumbel(test_data)[0] - test_data)**2) / len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try modifying our previous methods to have the weights be the output of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_L1_Hypernetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size):\n",
    "        super(VAE_L1_Hypernetwork, self).__init__()\n",
    "        \n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.l1_weights = self.weight_creator(x)\n",
    "        x = x * self.l1_weights\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_l1_norm(weights):\n",
    "    return torch.sum(torch.stack([torch.norm(weights[i, :], 1) for i in range(weights.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hypernetwork(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(batch_data)\n",
    "        loss = loss_function(recon_batch, batch_data, mu, logvar)\n",
    "        \n",
    "        loss += mean_l1_norm(model.l1_weights)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_l1_hypernetwork = VAE_L1_Hypernetwork(500, 250, 20)\n",
    "vae_l1_hypernetwork.to(device)\n",
    "vae_l1_hypernetwork_optimizer = torch.optim.Adam(vae_l1_hypernetwork.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 366.609772\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 132.566559\n",
      "====> Epoch: 1 Average loss: 231.0197\n",
      "====> Test set loss: 124.8660\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 126.035240\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 77.111519\n",
      "====> Epoch: 2 Average loss: 90.6851\n",
      "====> Test set loss: 75.4202\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 77.230080\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 74.003052\n",
      "====> Epoch: 3 Average loss: 75.1994\n",
      "====> Test set loss: 72.8025\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 74.955193\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 73.445549\n",
      "====> Epoch: 4 Average loss: 73.2216\n",
      "====> Test set loss: 71.4933\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 73.350037\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 70.613625\n",
      "====> Epoch: 5 Average loss: 71.8990\n",
      "====> Test set loss: 70.6264\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 72.736588\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 71.697807\n",
      "====> Epoch: 6 Average loss: 70.9756\n",
      "====> Test set loss: 69.3226\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 72.072975\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 70.132988\n",
      "====> Epoch: 7 Average loss: 69.9467\n",
      "====> Test set loss: 68.7080\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 70.452408\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 67.668373\n",
      "====> Epoch: 8 Average loss: 69.1143\n",
      "====> Test set loss: 68.1165\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 69.257477\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 66.602715\n",
      "====> Epoch: 9 Average loss: 68.3865\n",
      "====> Test set loss: 67.1594\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 67.949844\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 69.040131\n",
      "====> Epoch: 10 Average loss: 67.6970\n",
      "====> Test set loss: 66.6947\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 67.212921\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 67.682167\n",
      "====> Epoch: 11 Average loss: 67.1095\n",
      "====> Test set loss: 66.0758\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 66.080948\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 67.395462\n",
      "====> Epoch: 12 Average loss: 66.5872\n",
      "====> Test set loss: 65.7160\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 66.054482\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 65.521843\n",
      "====> Epoch: 13 Average loss: 66.1843\n",
      "====> Test set loss: 65.5178\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 65.654320\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 65.651016\n",
      "====> Epoch: 14 Average loss: 65.9462\n",
      "====> Test set loss: 65.2116\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 66.715446\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 64.997231\n",
      "====> Epoch: 15 Average loss: 65.6303\n",
      "====> Test set loss: 65.0563\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 65.296997\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 65.295753\n",
      "====> Epoch: 16 Average loss: 65.5154\n",
      "====> Test set loss: 64.7893\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 64.888298\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 65.446701\n",
      "====> Epoch: 17 Average loss: 65.4087\n",
      "====> Test set loss: 64.6974\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 65.297028\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 65.673798\n",
      "====> Epoch: 18 Average loss: 65.2618\n",
      "====> Test set loss: 64.6779\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 65.216026\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 64.201584\n",
      "====> Epoch: 19 Average loss: 65.1915\n",
      "====> Test set loss: 64.6994\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 64.922371\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 64.565643\n",
      "====> Epoch: 20 Average loss: 65.1223\n",
      "====> Test set loss: 64.6198\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 65.293663\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 65.497963\n",
      "====> Epoch: 21 Average loss: 65.0710\n",
      "====> Test set loss: 64.6144\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 64.630386\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 64.556648\n",
      "====> Epoch: 22 Average loss: 64.9507\n",
      "====> Test set loss: 64.5568\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 63.751778\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 63.708256\n",
      "====> Epoch: 23 Average loss: 64.9653\n",
      "====> Test set loss: 64.4278\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 64.256645\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 66.028641\n",
      "====> Epoch: 24 Average loss: 64.9127\n",
      "====> Test set loss: 64.3947\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 65.735558\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 65.257477\n",
      "====> Epoch: 25 Average loss: 64.8616\n",
      "====> Test set loss: 64.4247\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 63.707432\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 63.901791\n",
      "====> Epoch: 26 Average loss: 64.7883\n",
      "====> Test set loss: 64.3843\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 65.069595\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 63.737030\n",
      "====> Epoch: 27 Average loss: 64.7554\n",
      "====> Test set loss: 64.3410\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 65.794296\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 64.162880\n",
      "====> Epoch: 28 Average loss: 64.7151\n",
      "====> Test set loss: 64.3227\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 65.381363\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 64.875397\n",
      "====> Epoch: 29 Average loss: 64.7070\n",
      "====> Test set loss: 64.3327\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 64.709679\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 64.528481\n",
      "====> Epoch: 30 Average loss: 64.6374\n",
      "====> Test set loss: 64.2253\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 63.588306\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 65.213287\n",
      "====> Epoch: 31 Average loss: 64.6075\n",
      "====> Test set loss: 64.1543\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 63.643555\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 64.500717\n",
      "====> Epoch: 32 Average loss: 64.5574\n",
      "====> Test set loss: 64.1033\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 63.893314\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 63.891296\n",
      "====> Epoch: 33 Average loss: 64.5243\n",
      "====> Test set loss: 64.0901\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 63.753250\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 63.651237\n",
      "====> Epoch: 34 Average loss: 64.4661\n",
      "====> Test set loss: 64.1033\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 64.122185\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 63.048962\n",
      "====> Epoch: 35 Average loss: 64.4270\n",
      "====> Test set loss: 64.0257\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 64.338043\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 64.015793\n",
      "====> Epoch: 36 Average loss: 64.3771\n",
      "====> Test set loss: 63.9785\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 65.277496\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 63.313030\n",
      "====> Epoch: 37 Average loss: 64.3307\n",
      "====> Test set loss: 63.9757\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 65.292847\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 63.632633\n",
      "====> Epoch: 38 Average loss: 64.2925\n",
      "====> Test set loss: 63.9421\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 64.296425\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 63.845039\n",
      "====> Epoch: 39 Average loss: 64.2522\n",
      "====> Test set loss: 63.9212\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 64.222321\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 63.441998\n",
      "====> Epoch: 40 Average loss: 64.2231\n",
      "====> Test set loss: 63.8146\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 64.411209\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 63.955048\n",
      "====> Epoch: 41 Average loss: 64.1959\n",
      "====> Test set loss: 63.8278\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 62.209866\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 64.021248\n",
      "====> Epoch: 42 Average loss: 64.1511\n",
      "====> Test set loss: 63.8189\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 64.207077\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 64.750816\n",
      "====> Epoch: 43 Average loss: 64.1101\n",
      "====> Test set loss: 63.7142\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 64.523308\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 63.360180\n",
      "====> Epoch: 44 Average loss: 64.0630\n",
      "====> Test set loss: 63.6990\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 64.305923\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 63.447227\n",
      "====> Epoch: 45 Average loss: 64.0301\n",
      "====> Test set loss: 63.6835\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 63.575027\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 63.019363\n",
      "====> Epoch: 46 Average loss: 64.0031\n",
      "====> Test set loss: 63.6478\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 65.126785\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 65.908058\n",
      "====> Epoch: 47 Average loss: 63.9700\n",
      "====> Test set loss: 63.6470\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 62.929546\n",
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 64.946281\n",
      "====> Epoch: 48 Average loss: 63.9247\n",
      "====> Test set loss: 63.6027\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 65.781647\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 64.729683\n",
      "====> Epoch: 49 Average loss: 63.9086\n",
      "====> Test set loss: 63.5775\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 63.484772\n",
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 65.315834\n",
      "====> Epoch: 50 Average loss: 63.8645\n",
      "====> Test set loss: 63.5374\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50 + 1):\n",
    "        train_hypernetwork(train_data, vae_l1_hypernetwork, vae_l1_hypernetwork_optimizer, epoch)\n",
    "        #with torch.no_grad():\n",
    "        #    model.diag.data[torch.abs(model.diag) < 0.05] = 0\n",
    "        test(test_data, vae_l1_hypernetwork, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2847, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((vae_l1_hypernetwork(train_data)[0] - train_data)**2) / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2894, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((vae_l1_hypernetwork(test_data)[0] - test_data)**2) / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vae_l1_hypernetwork(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([154, 122, 143,  ..., 109, 140, 184])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(vae_l1_hypernetwork.l1_weights) < 1e-4, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([138, 121,  90,  62,  38,  24,  12,   5,   7,   3]),\n",
       " array([7.6973811e-07, 8.8643887e-05, 1.7651804e-04, 2.6439218e-04,\n",
       "        3.5226633e-04, 4.4014049e-04, 5.2801461e-04, 6.1588880e-04,\n",
       "        7.0376293e-04, 7.9163711e-04, 8.7951124e-04], dtype=float32))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(torch.abs(vae_l1_hypernetwork.l1_weights)[0, :].clone().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
