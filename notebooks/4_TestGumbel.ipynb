{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the Xie Ermon paper of Continuous relaxation for subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "img_size = 28\n",
    "channels = 1\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "z_size = 20\n",
    "\n",
    "n = 28 * 28\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sio.loadmat(\"../data/zeisel/CITEseq.mat\")\n",
    "data= a['G'].T\n",
    "N,d=data.shape\n",
    "#transformation from integer entries \n",
    "data=np.log(data+np.ones(data.shape))\n",
    "for i in range(N):\n",
    "    data[i,:]=data[i,:]/np.linalg.norm(data[i,:])\n",
    "\n",
    "#load labels from file\n",
    "a = sio.loadmat(\"../data/zeisel/CITEseq-labels.mat\")\n",
    "l_aux = a['labels']\n",
    "labels = np.array([i for [i] in l_aux])\n",
    "\n",
    "#load names from file\n",
    "a = sio.loadmat(\"../data/zeisel/CITEseq_names.mat\")\n",
    "names=[a['citeseq_names'][i][0][0] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.random.permutation(np.arange(data.shape[0]))\n",
    "upto = int(.8 * len(data))\n",
    "\n",
    "train_data = data[slices[:upto]]\n",
    "test_data = data[slices[upto:]]\n",
    "\n",
    "train_data = Tensor(train_data).to(device)\n",
    "test_data = Tensor(test_data).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_keys(w):\n",
    "    # sample some gumbels\n",
    "    uniform = (1.0 - EPSILON) * torch.rand_like(w) + EPSILON\n",
    "    z = torch.log(-torch.log(uniform))\n",
    "    w = w + z\n",
    "    return w\n",
    "\n",
    "\n",
    "def continuous_topk(w, k, t, separate=False):\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    khot_list = []\n",
    "    onehot_approx = torch.zeros_like(w, dtype = torch.float32)\n",
    "    for i in range(k):\n",
    "        ### conver the following into pytorch\n",
    "        #khot_mask = tf.maximum(1.0 - onehot_approx, EPSILON)\n",
    "        max_mask = 1 - onehot_approx < EPSILON\n",
    "        khot_mask = 1 - onehot_approx\n",
    "        khot_mask[max_mask] = EPSILON\n",
    "        \n",
    "        w += torch.log(khot_mask)\n",
    "        #onehot_approx = tf.nn.softmax(w / t, axis=-1)\n",
    "        onehot_approx = softmax(w/t)\n",
    "        khot_list.append(onehot_approx)\n",
    "    if separate:\n",
    "        return torch.stack(khot_list)\n",
    "    else:\n",
    "        return torch.sum(torch.stack(khot_list), dim = 0) \n",
    "\n",
    "\n",
    "def sample_subset(w, k, t=0.1):\n",
    "    '''\n",
    "    Args:\n",
    "        w (Tensor): Float Tensor of weights for each element. In gumbel mode\n",
    "            these are interpreted as log probabilities\n",
    "        k (int): number of elements in the subset sample\n",
    "        t (float): temperature of the softmax\n",
    "    '''\n",
    "    w = gumbel_keys(w)\n",
    "    return continuous_topk(w, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_Gumbel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size, k, t = 0.1):\n",
    "        super(VAE_Gumbel, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self.t = t\n",
    "        \n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        w = self.weight_creator(x)\n",
    "        subset_indices = sample_subset(w, self.k, self.t)\n",
    "        x = x * subset_indices\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(batch_data)\n",
    "        loss = loss_function(recon_batch, batch_data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df, model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    inds = np.arange(df.shape[0])\n",
    "    with torch.no_grad():\n",
    "        for i in range(math.ceil(len(df)/batch_size)):\n",
    "            batch_ind = inds[i * batch_size : (i+1) * batch_size]\n",
    "            batch_data = df[batch_ind, :]\n",
    "            batch_data = batch_data.to(device)\n",
    "            recon_batch, mu, logvar = model(batch_data)\n",
    "            test_loss += loss_function(recon_batch, batch_data, mu, logvar).item()\n",
    "\n",
    "\n",
    "    test_loss /= len(df)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_gumbel = VAE_Gumbel(500, 250, 20, k = 50)\n",
    "vae_gumbel.to(device)\n",
    "vae_gumbel_optimizer = torch.optim.Adam(vae_gumbel.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 350.120178\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 89.994255\n",
      "====> Epoch: 1 Average loss: 201.8854\n",
      "====> Test set loss: 89.2234\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 90.761772\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 76.080963\n",
      "====> Epoch: 2 Average loss: 81.1297\n",
      "====> Test set loss: 76.9310\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 75.364143\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 74.325439\n",
      "====> Epoch: 3 Average loss: 74.8473\n",
      "====> Test set loss: 73.6720\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 75.031143\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 72.577911\n",
      "====> Epoch: 4 Average loss: 72.9427\n",
      "====> Test set loss: 73.0174\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 71.724403\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 72.360687\n",
      "====> Epoch: 5 Average loss: 72.2479\n",
      "====> Test set loss: 72.1608\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 72.218407\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 73.073257\n",
      "====> Epoch: 6 Average loss: 71.6219\n",
      "====> Test set loss: 71.9428\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 70.683487\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 71.212967\n",
      "====> Epoch: 7 Average loss: 70.9108\n",
      "====> Test set loss: 71.2396\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 71.584801\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 71.967918\n",
      "====> Epoch: 8 Average loss: 70.3231\n",
      "====> Test set loss: 70.0839\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 70.794670\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 70.114944\n",
      "====> Epoch: 9 Average loss: 69.5585\n",
      "====> Test set loss: 69.6861\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 70.158989\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 69.194756\n",
      "====> Epoch: 10 Average loss: 68.8508\n",
      "====> Test set loss: 68.7922\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 69.732208\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 68.009750\n",
      "====> Epoch: 11 Average loss: 68.0780\n",
      "====> Test set loss: 68.0090\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 67.650398\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 65.304230\n",
      "====> Epoch: 12 Average loss: 67.4405\n",
      "====> Test set loss: 67.5323\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 68.016594\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 67.290375\n",
      "====> Epoch: 13 Average loss: 67.0006\n",
      "====> Test set loss: 67.1028\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 67.165939\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 67.912186\n",
      "====> Epoch: 14 Average loss: 66.6365\n",
      "====> Test set loss: 66.7566\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 66.027191\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 67.716682\n",
      "====> Epoch: 15 Average loss: 66.3018\n",
      "====> Test set loss: 66.4537\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 64.656265\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 68.467606\n",
      "====> Epoch: 16 Average loss: 65.9184\n",
      "====> Test set loss: 66.2067\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 64.215302\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 65.356194\n",
      "====> Epoch: 17 Average loss: 65.6645\n",
      "====> Test set loss: 65.9149\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 65.158829\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 65.143883\n",
      "====> Epoch: 18 Average loss: 65.4659\n",
      "====> Test set loss: 65.8358\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 65.677116\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 64.231422\n",
      "====> Epoch: 19 Average loss: 65.3149\n",
      "====> Test set loss: 65.5698\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 64.489822\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 63.406521\n",
      "====> Epoch: 20 Average loss: 65.1411\n",
      "====> Test set loss: 65.4536\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 64.426170\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 63.831711\n",
      "====> Epoch: 21 Average loss: 65.0050\n",
      "====> Test set loss: 65.2404\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 64.173355\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 66.686462\n",
      "====> Epoch: 22 Average loss: 64.8136\n",
      "====> Test set loss: 65.0656\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 65.597412\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 63.590134\n",
      "====> Epoch: 23 Average loss: 64.5912\n",
      "====> Test set loss: 64.8905\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 66.051437\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 64.340523\n",
      "====> Epoch: 24 Average loss: 64.4023\n",
      "====> Test set loss: 64.6706\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 64.123604\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 64.534492\n",
      "====> Epoch: 25 Average loss: 64.2242\n",
      "====> Test set loss: 64.6024\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 63.760708\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 64.712982\n",
      "====> Epoch: 26 Average loss: 64.1496\n",
      "====> Test set loss: 64.5020\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 63.177113\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 63.450619\n",
      "====> Epoch: 27 Average loss: 64.0925\n",
      "====> Test set loss: 64.4515\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 64.494507\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 63.342640\n",
      "====> Epoch: 28 Average loss: 64.0500\n",
      "====> Test set loss: 64.4077\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 64.092232\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 64.711411\n",
      "====> Epoch: 29 Average loss: 64.0289\n",
      "====> Test set loss: 64.4084\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 65.039162\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 62.142948\n",
      "====> Epoch: 30 Average loss: 63.9907\n",
      "====> Test set loss: 64.3278\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 63.470654\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 64.732178\n",
      "====> Epoch: 31 Average loss: 63.9259\n",
      "====> Test set loss: 64.2920\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 61.961987\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 64.380112\n",
      "====> Epoch: 32 Average loss: 63.9075\n",
      "====> Test set loss: 64.2864\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 64.488228\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 64.071953\n",
      "====> Epoch: 33 Average loss: 63.8803\n",
      "====> Test set loss: 64.1770\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 64.122429\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 63.517365\n",
      "====> Epoch: 34 Average loss: 63.8477\n",
      "====> Test set loss: 64.1674\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 61.870762\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 64.712540\n",
      "====> Epoch: 35 Average loss: 63.7842\n",
      "====> Test set loss: 64.1518\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 62.973812\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 63.314014\n",
      "====> Epoch: 36 Average loss: 63.7547\n",
      "====> Test set loss: 64.1771\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 65.158470\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 64.514870\n",
      "====> Epoch: 37 Average loss: 63.7453\n",
      "====> Test set loss: 64.1293\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 63.657120\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 63.166931\n",
      "====> Epoch: 38 Average loss: 63.6836\n",
      "====> Test set loss: 64.0757\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 64.739670\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 63.284912\n",
      "====> Epoch: 39 Average loss: 63.6795\n",
      "====> Test set loss: 64.0470\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 63.340424\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 63.518154\n",
      "====> Epoch: 40 Average loss: 63.6427\n",
      "====> Test set loss: 64.0055\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 63.927357\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 65.009949\n",
      "====> Epoch: 41 Average loss: 63.6079\n",
      "====> Test set loss: 63.9548\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 64.221390\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 64.442528\n",
      "====> Epoch: 42 Average loss: 63.5969\n",
      "====> Test set loss: 63.9745\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 62.801971\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 63.058670\n",
      "====> Epoch: 43 Average loss: 63.5568\n",
      "====> Test set loss: 63.9170\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 62.828453\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 64.254150\n",
      "====> Epoch: 44 Average loss: 63.5216\n",
      "====> Test set loss: 63.9281\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 64.020866\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 63.389374\n",
      "====> Epoch: 45 Average loss: 63.5048\n",
      "====> Test set loss: 63.8892\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 63.893986\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 63.788219\n",
      "====> Epoch: 46 Average loss: 63.4994\n",
      "====> Test set loss: 63.8561\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 63.200829\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 64.250816\n",
      "====> Epoch: 47 Average loss: 63.4641\n",
      "====> Test set loss: 63.8541\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 65.539169\n",
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 63.877586\n",
      "====> Epoch: 48 Average loss: 63.4389\n",
      "====> Test set loss: 63.7921\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 65.239471\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 63.591839\n",
      "====> Epoch: 49 Average loss: 63.4303\n",
      "====> Test set loss: 63.8053\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 63.359474\n",
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 63.434422\n",
      "====> Epoch: 50 Average loss: 63.4101\n",
      "====> Test set loss: 63.7776\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50 + 1):\n",
    "        train(train_data, vae_gumbel, vae_gumbel_optimizer, epoch)\n",
    "        #with torch.no_grad():\n",
    "        #    model.diag.data[torch.abs(model.diag) < 0.05] = 0\n",
    "        test(test_data, vae_gumbel, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae_gumbel.state_dict(), BASE_PATH_DATA + \"../data/models/zeisel/gumbel.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a subset for calculation error because of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2657, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((vae_gumbel(train_data[0:64, :])[0] - train_data[0:64,:])**2) / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2445, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((vae_gumbel(test_data[0:64, :])[0] - test_data[0:64,:])**2) / 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sometimes does better on test!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the weights. Are the sparse ones consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    weights_train = vae_gumbel.weight_creator(train_data[0:64,:])\n",
    "# same k and t as above\n",
    "subset_indices = sample_subset(weights_train, k=50, t=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices = subset_indices.clone().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   8,  11,  23,  31,  34,  36,  42,  44,  48,  50,  52,\n",
       "        53,  55,  56,  57,  61,  63,  64,  66,  67,  68,  69,  71,  72,\n",
       "        73,  74,  75,  77,  78,  79,  80,  82,  83,  84,  85,  86,  88,\n",
       "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "       103, 104, 105, 106, 107, 109, 110, 111, 112, 114, 115, 117, 118,\n",
       "       119, 121, 122, 123, 124, 126, 127, 129, 130, 131, 132, 133, 134,\n",
       "       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n",
       "       148, 149, 150, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162,\n",
       "       163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176,\n",
       "       177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 190,\n",
       "       191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203,\n",
       "       204, 205, 206, 207, 208, 209, 210, 213, 214, 215, 216, 217, 218,\n",
       "       219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231,\n",
       "       232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
       "       245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257,\n",
       "       258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270,\n",
       "       271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
       "       284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296,\n",
       "       297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 308, 309, 310,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364,\n",
       "       365, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379,\n",
       "       380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
       "       393, 394, 395, 396, 397, 398, 399, 400, 402, 403, 404, 405, 406,\n",
       "       407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "       420, 421, 422, 423, 424, 425, 426, 427, 429, 430, 431, 432, 433,\n",
       "       434, 435, 436, 437, 438, 439, 441, 442, 443, 444, 445, 446, 447,\n",
       "       448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
       "       461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
       "       474, 475, 476, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487,\n",
       "       488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isclose(subset_indices[0,:], 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6,   7,   9,  11,  15,  17,  23,  30,  31,  34,  36,  39,  40,\n",
       "        42,  43,  44,  45,  46,  48,  50,  52,  53,  54,  55,  56,  57,\n",
       "        60,  61,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
       "        74,  76,  77,  78,  79,  80,  82,  83,  84,  85,  86,  88,  89,\n",
       "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "       103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 177, 178, 179, 180, 181, 182, 183,\n",
       "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "       197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "       210, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
       "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
       "       251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263,\n",
       "       264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276,\n",
       "       277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289,\n",
       "       290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302,\n",
       "       303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
       "       317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
       "       330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342,\n",
       "       343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
       "       356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369,\n",
       "       370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382,\n",
       "       383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
       "       396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409,\n",
       "       410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422,\n",
       "       423, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436,\n",
       "       437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449,\n",
       "       450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
       "       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
       "       476, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "       490, 491, 492, 493, 494, 495, 496, 497, 498, 499])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isclose(subset_indices[1,:], 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11,  23,  31,  34,  36,  42,  44,  48,  50,  52,  53,  55,  56,\n",
       "        57,  61,  63,  64,  66,  67,  68,  69,  71,  72,  73,  74,  77,\n",
       "        78,  79,  80,  82,  83,  84,  85,  86,  88,  90,  91,  92,  93,\n",
       "        94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
       "       107, 109, 110, 111, 112, 114, 115, 117, 118, 119, 121, 122, 123,\n",
       "       124, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "       139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152,\n",
       "       153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166,\n",
       "       167, 168, 169, 170, 172, 173, 174, 175, 177, 178, 179, 180, 181,\n",
       "       182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "       209, 210, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
       "       224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236,\n",
       "       237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249,\n",
       "       250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262,\n",
       "       263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275,\n",
       "       276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288,\n",
       "       289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301,\n",
       "       302, 303, 304, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316,\n",
       "       317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
       "       330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342,\n",
       "       343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
       "       356, 357, 358, 359, 360, 361, 362, 364, 365, 368, 369, 370, 371,\n",
       "       372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384,\n",
       "       385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
       "       398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411,\n",
       "       412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424,\n",
       "       425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438,\n",
       "       439, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
       "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 478, 479,\n",
       "       480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492,\n",
       "       493, 494, 495, 496, 497, 498, 499])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d( np.where(np.isclose(subset_indices[1,:], 0))[0], np.where(np.isclose(subset_indices[0,:], 0))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11,  17,  23,  31,  34,  36,  42,  44,  45,  48,  50,  52,  53,\n",
       "        54,  55,  56,  57,  58,  60,  61,  63,  66,  67,  68,  69,  71,\n",
       "        73,  74,  76,  78,  79,  80,  82,  83,  84,  85,  86,  88,  89,\n",
       "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "       103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 177, 178, 179, 180, 181, 182, 183,\n",
       "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
       "       197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "       210, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
       "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
       "       251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263,\n",
       "       264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276,\n",
       "       277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289,\n",
       "       290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302,\n",
       "       303, 304, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
       "       317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
       "       330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342,\n",
       "       343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
       "       356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 369,\n",
       "       370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382,\n",
       "       383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
       "       396, 397, 398, 399, 400, 402, 403, 404, 405, 406, 407, 408, 409,\n",
       "       410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422,\n",
       "       423, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436,\n",
       "       437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449,\n",
       "       450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,\n",
       "       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
       "       476, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
       "       490, 491, 492, 493, 494, 495, 496, 497, 498, 499])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.intersect1d( np.where(np.isclose(subset_indices[12,:], 0))[0], np.where(np.isclose(subset_indices[15,:], 0))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tend to select the same sparse features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try modifying our previous methods to have the weights be the output of a neural network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 VAE model we are loading\n",
    "class VAE_L1_Hypernetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size):\n",
    "        super(VAE_L1_Hypernetwork, self).__init__()\n",
    "        \n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.l1_weights = self.weight_creator(x)\n",
    "        x = x * self.l1_weights\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm_50_2(weights):\n",
    "    return torch.sum(torch.stack([(torch.norm(weights[i, :], 1)**2 - 50)**2 for i in range(weights.shape[0])]))\n",
    "\n",
    "def l1_norm_50(weights):\n",
    "    return torch.sum(torch.stack([(torch.norm(weights[i, :], 1) - 50)**2 for i in range(weights.shape[0])]))\n",
    "\n",
    "def mean_l1_norm(weights):\n",
    "    return torch.sum(torch.stack([torch.norm(weights[i, :], 1) for i in range(weights.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hypernetwork(df, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    permutations = torch.randperm(df.shape[0])\n",
    "    for i in range(math.ceil(len(df)/batch_size)):\n",
    "        batch_ind = permutations[i * batch_size : (i+1) * batch_size]\n",
    "        batch_data = df[batch_ind, :]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(batch_data)\n",
    "        loss = loss_function(recon_batch, batch_data, mu, logvar)\n",
    "        \n",
    "        loss += 100 * l1_norm_50_2(model.l1_weights)\n",
    "        loss += 100 * l1_norm_50(model.l1_weights)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(batch_data), len(df),\n",
    "                100. * i / len(df),\n",
    "                loss.item() / len(batch_data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_l1_hypernetwork = VAE_L1_Hypernetwork(500, 250, 20)\n",
    "vae_l1_hypernetwork.to(device)\n",
    "vae_l1_hypernetwork_optimizer = torch.optim.Adam(vae_l1_hypernetwork.parameters(), \n",
    "                                                lr=lr, \n",
    "                                                betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6893 (0%)]\tLoss: 4642754.500000\n",
      "Train Epoch: 1 [6400/6893 (1%)]\tLoss: 183888.312500\n",
      "====> Epoch: 1 Average loss: 394124.1187\n",
      "====> Test set loss: 116.8953\n",
      "Train Epoch: 2 [0/6893 (0%)]\tLoss: 186667.421875\n",
      "Train Epoch: 2 [6400/6893 (1%)]\tLoss: 183702.812500\n",
      "====> Epoch: 2 Average loss: 183976.3396\n",
      "====> Test set loss: 75.7574\n",
      "Train Epoch: 3 [0/6893 (0%)]\tLoss: 183771.750000\n",
      "Train Epoch: 3 [6400/6893 (1%)]\tLoss: 183669.125000\n",
      "====> Epoch: 3 Average loss: 183682.8196\n",
      "====> Test set loss: 73.1656\n",
      "Train Epoch: 4 [0/6893 (0%)]\tLoss: 183651.843750\n",
      "Train Epoch: 4 [6400/6893 (1%)]\tLoss: 183594.906250\n",
      "====> Epoch: 4 Average loss: 183623.6807\n",
      "====> Test set loss: 72.2464\n",
      "Train Epoch: 5 [0/6893 (0%)]\tLoss: 183592.421875\n",
      "Train Epoch: 5 [6400/6893 (1%)]\tLoss: 183583.171875\n",
      "====> Epoch: 5 Average loss: 183590.4244\n",
      "====> Test set loss: 71.2622\n",
      "Train Epoch: 6 [0/6893 (0%)]\tLoss: 183585.765625\n",
      "Train Epoch: 6 [6400/6893 (1%)]\tLoss: 183556.718750\n",
      "====> Epoch: 6 Average loss: 183563.2187\n",
      "====> Test set loss: 70.1436\n",
      "Train Epoch: 7 [0/6893 (0%)]\tLoss: 183569.062500\n",
      "Train Epoch: 7 [6400/6893 (1%)]\tLoss: 183553.937500\n",
      "====> Epoch: 7 Average loss: 183547.1288\n",
      "====> Test set loss: 69.2699\n",
      "Train Epoch: 8 [0/6893 (0%)]\tLoss: 183547.578125\n",
      "Train Epoch: 8 [6400/6893 (1%)]\tLoss: 183688.640625\n",
      "====> Epoch: 8 Average loss: 183535.3562\n",
      "====> Test set loss: 68.8273\n",
      "Train Epoch: 9 [0/6893 (0%)]\tLoss: 183547.531250\n",
      "Train Epoch: 9 [6400/6893 (1%)]\tLoss: 183513.843750\n",
      "====> Epoch: 9 Average loss: 183527.5683\n",
      "====> Test set loss: 67.9229\n",
      "Train Epoch: 10 [0/6893 (0%)]\tLoss: 183507.062500\n",
      "Train Epoch: 10 [6400/6893 (1%)]\tLoss: 183502.062500\n",
      "====> Epoch: 10 Average loss: 183518.9251\n",
      "====> Test set loss: 67.3084\n",
      "Train Epoch: 11 [0/6893 (0%)]\tLoss: 183523.734375\n",
      "Train Epoch: 11 [6400/6893 (1%)]\tLoss: 183514.031250\n",
      "====> Epoch: 11 Average loss: 183514.3807\n",
      "====> Test set loss: 66.5727\n",
      "Train Epoch: 12 [0/6893 (0%)]\tLoss: 183498.125000\n",
      "Train Epoch: 12 [6400/6893 (1%)]\tLoss: 183495.140625\n",
      "====> Epoch: 12 Average loss: 183508.2074\n",
      "====> Test set loss: 66.1613\n",
      "Train Epoch: 13 [0/6893 (0%)]\tLoss: 183501.453125\n",
      "Train Epoch: 13 [6400/6893 (1%)]\tLoss: 183512.156250\n",
      "====> Epoch: 13 Average loss: 183505.4259\n",
      "====> Test set loss: 65.7909\n",
      "Train Epoch: 14 [0/6893 (0%)]\tLoss: 183495.828125\n",
      "Train Epoch: 14 [6400/6893 (1%)]\tLoss: 183499.046875\n",
      "====> Epoch: 14 Average loss: 183506.2770\n",
      "====> Test set loss: 65.5926\n",
      "Train Epoch: 15 [0/6893 (0%)]\tLoss: 183492.625000\n",
      "Train Epoch: 15 [6400/6893 (1%)]\tLoss: 183487.000000\n",
      "====> Epoch: 15 Average loss: 183505.3254\n",
      "====> Test set loss: 65.4565\n",
      "Train Epoch: 16 [0/6893 (0%)]\tLoss: 183497.359375\n",
      "Train Epoch: 16 [6400/6893 (1%)]\tLoss: 183492.984375\n",
      "====> Epoch: 16 Average loss: 183504.9947\n",
      "====> Test set loss: 65.3463\n",
      "Train Epoch: 17 [0/6893 (0%)]\tLoss: 183485.312500\n",
      "Train Epoch: 17 [6400/6893 (1%)]\tLoss: 183480.359375\n",
      "====> Epoch: 17 Average loss: 183503.2530\n",
      "====> Test set loss: 65.0589\n",
      "Train Epoch: 18 [0/6893 (0%)]\tLoss: 183494.484375\n",
      "Train Epoch: 18 [6400/6893 (1%)]\tLoss: 183503.468750\n",
      "====> Epoch: 18 Average loss: 183501.0228\n",
      "====> Test set loss: 65.1027\n",
      "Train Epoch: 19 [0/6893 (0%)]\tLoss: 183484.671875\n",
      "Train Epoch: 19 [6400/6893 (1%)]\tLoss: 183476.390625\n",
      "====> Epoch: 19 Average loss: 183515.3284\n",
      "====> Test set loss: 64.9351\n",
      "Train Epoch: 20 [0/6893 (0%)]\tLoss: 183480.359375\n",
      "Train Epoch: 20 [6400/6893 (1%)]\tLoss: 183494.328125\n",
      "====> Epoch: 20 Average loss: 183502.8888\n",
      "====> Test set loss: 65.0222\n",
      "Train Epoch: 21 [0/6893 (0%)]\tLoss: 183492.984375\n",
      "Train Epoch: 21 [6400/6893 (1%)]\tLoss: 183515.781250\n",
      "====> Epoch: 21 Average loss: 183513.4518\n",
      "====> Test set loss: 64.9392\n",
      "Train Epoch: 22 [0/6893 (0%)]\tLoss: 183476.578125\n",
      "Train Epoch: 22 [6400/6893 (1%)]\tLoss: 183539.718750\n",
      "====> Epoch: 22 Average loss: 183512.4927\n",
      "====> Test set loss: 64.8296\n",
      "Train Epoch: 23 [0/6893 (0%)]\tLoss: 183479.000000\n",
      "Train Epoch: 23 [6400/6893 (1%)]\tLoss: 183548.296875\n",
      "====> Epoch: 23 Average loss: 183510.6657\n",
      "====> Test set loss: 64.8204\n",
      "Train Epoch: 24 [0/6893 (0%)]\tLoss: 183488.218750\n",
      "Train Epoch: 24 [6400/6893 (1%)]\tLoss: 183497.515625\n",
      "====> Epoch: 24 Average loss: 183508.9056\n",
      "====> Test set loss: 64.7313\n",
      "Train Epoch: 25 [0/6893 (0%)]\tLoss: 183484.890625\n",
      "Train Epoch: 25 [6400/6893 (1%)]\tLoss: 183529.671875\n",
      "====> Epoch: 25 Average loss: 183518.7376\n",
      "====> Test set loss: 64.7108\n",
      "Train Epoch: 26 [0/6893 (0%)]\tLoss: 183477.171875\n",
      "Train Epoch: 26 [6400/6893 (1%)]\tLoss: 183482.250000\n",
      "====> Epoch: 26 Average loss: 183521.3173\n",
      "====> Test set loss: 64.6727\n",
      "Train Epoch: 27 [0/6893 (0%)]\tLoss: 183474.484375\n",
      "Train Epoch: 27 [6400/6893 (1%)]\tLoss: 183474.890625\n",
      "====> Epoch: 27 Average loss: 183512.9912\n",
      "====> Test set loss: 64.6536\n",
      "Train Epoch: 28 [0/6893 (0%)]\tLoss: 183477.421875\n",
      "Train Epoch: 28 [6400/6893 (1%)]\tLoss: 183519.687500\n",
      "====> Epoch: 28 Average loss: 183526.1445\n",
      "====> Test set loss: 64.5495\n",
      "Train Epoch: 29 [0/6893 (0%)]\tLoss: 183480.031250\n",
      "Train Epoch: 29 [6400/6893 (1%)]\tLoss: 183694.531250\n",
      "====> Epoch: 29 Average loss: 183514.6569\n",
      "====> Test set loss: 64.5933\n",
      "Train Epoch: 30 [0/6893 (0%)]\tLoss: 183481.921875\n",
      "Train Epoch: 30 [6400/6893 (1%)]\tLoss: 183599.468750\n",
      "====> Epoch: 30 Average loss: 183548.0092\n",
      "====> Test set loss: 64.5972\n",
      "Train Epoch: 31 [0/6893 (0%)]\tLoss: 183576.718750\n",
      "Train Epoch: 31 [6400/6893 (1%)]\tLoss: 183487.203125\n",
      "====> Epoch: 31 Average loss: 183494.1727\n",
      "====> Test set loss: 64.4758\n",
      "Train Epoch: 32 [0/6893 (0%)]\tLoss: 183474.203125\n",
      "Train Epoch: 32 [6400/6893 (1%)]\tLoss: 183579.500000\n",
      "====> Epoch: 32 Average loss: 183522.7938\n",
      "====> Test set loss: 64.5049\n",
      "Train Epoch: 33 [0/6893 (0%)]\tLoss: 183478.937500\n",
      "Train Epoch: 33 [6400/6893 (1%)]\tLoss: 183517.734375\n",
      "====> Epoch: 33 Average loss: 183512.0339\n",
      "====> Test set loss: 64.4003\n",
      "Train Epoch: 34 [0/6893 (0%)]\tLoss: 183480.828125\n",
      "Train Epoch: 34 [6400/6893 (1%)]\tLoss: 183475.750000\n",
      "====> Epoch: 34 Average loss: 183535.2357\n",
      "====> Test set loss: 64.3765\n",
      "Train Epoch: 35 [0/6893 (0%)]\tLoss: 183475.953125\n",
      "Train Epoch: 35 [6400/6893 (1%)]\tLoss: 183477.375000\n",
      "====> Epoch: 35 Average loss: 183522.3733\n",
      "====> Test set loss: 64.3043\n",
      "Train Epoch: 36 [0/6893 (0%)]\tLoss: 183479.421875\n",
      "Train Epoch: 36 [6400/6893 (1%)]\tLoss: 183520.421875\n",
      "====> Epoch: 36 Average loss: 183525.8327\n",
      "====> Test set loss: 64.3021\n",
      "Train Epoch: 37 [0/6893 (0%)]\tLoss: 183472.515625\n",
      "Train Epoch: 37 [6400/6893 (1%)]\tLoss: 183489.812500\n",
      "====> Epoch: 37 Average loss: 183521.2970\n",
      "====> Test set loss: 64.2597\n",
      "Train Epoch: 38 [0/6893 (0%)]\tLoss: 183474.953125\n",
      "Train Epoch: 38 [6400/6893 (1%)]\tLoss: 183503.890625\n",
      "====> Epoch: 38 Average loss: 183533.8652\n",
      "====> Test set loss: 64.2452\n",
      "Train Epoch: 39 [0/6893 (0%)]\tLoss: 183477.015625\n",
      "Train Epoch: 39 [6400/6893 (1%)]\tLoss: 183488.296875\n",
      "====> Epoch: 39 Average loss: 183519.3081\n",
      "====> Test set loss: 64.2356\n",
      "Train Epoch: 40 [0/6893 (0%)]\tLoss: 183481.687500\n",
      "Train Epoch: 40 [6400/6893 (1%)]\tLoss: 183506.312500\n",
      "====> Epoch: 40 Average loss: 183525.7864\n",
      "====> Test set loss: 64.1707\n",
      "Train Epoch: 41 [0/6893 (0%)]\tLoss: 183475.593750\n",
      "Train Epoch: 41 [6400/6893 (1%)]\tLoss: 183540.515625\n",
      "====> Epoch: 41 Average loss: 183532.0050\n",
      "====> Test set loss: 64.1380\n",
      "Train Epoch: 42 [0/6893 (0%)]\tLoss: 183487.171875\n",
      "Train Epoch: 42 [6400/6893 (1%)]\tLoss: 183510.500000\n",
      "====> Epoch: 42 Average loss: 183517.0603\n",
      "====> Test set loss: 64.1391\n",
      "Train Epoch: 43 [0/6893 (0%)]\tLoss: 183478.203125\n",
      "Train Epoch: 43 [6400/6893 (1%)]\tLoss: 183564.046875\n",
      "====> Epoch: 43 Average loss: 183517.7056\n",
      "====> Test set loss: 64.0776\n",
      "Train Epoch: 44 [0/6893 (0%)]\tLoss: 183474.578125\n",
      "Train Epoch: 44 [6400/6893 (1%)]\tLoss: 183770.218750\n",
      "====> Epoch: 44 Average loss: 183534.8924\n",
      "====> Test set loss: 64.0583\n",
      "Train Epoch: 45 [0/6893 (0%)]\tLoss: 183478.609375\n",
      "Train Epoch: 45 [6400/6893 (1%)]\tLoss: 183474.609375\n",
      "====> Epoch: 45 Average loss: 183514.2899\n",
      "====> Test set loss: 64.0106\n",
      "Train Epoch: 46 [0/6893 (0%)]\tLoss: 183471.953125\n",
      "Train Epoch: 46 [6400/6893 (1%)]\tLoss: 183550.781250\n",
      "====> Epoch: 46 Average loss: 183534.2129\n",
      "====> Test set loss: 64.0025\n",
      "Train Epoch: 47 [0/6893 (0%)]\tLoss: 183474.437500\n",
      "Train Epoch: 47 [6400/6893 (1%)]\tLoss: 183475.781250\n",
      "====> Epoch: 47 Average loss: 183524.1527\n",
      "====> Test set loss: 63.9323\n",
      "Train Epoch: 48 [0/6893 (0%)]\tLoss: 183472.078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 48 [6400/6893 (1%)]\tLoss: 183483.203125\n",
      "====> Epoch: 48 Average loss: 183524.9320\n",
      "====> Test set loss: 63.9373\n",
      "Train Epoch: 49 [0/6893 (0%)]\tLoss: 183469.375000\n",
      "Train Epoch: 49 [6400/6893 (1%)]\tLoss: 183613.328125\n",
      "====> Epoch: 49 Average loss: 183524.2789\n",
      "====> Test set loss: 63.9189\n",
      "Train Epoch: 50 [0/6893 (0%)]\tLoss: 183474.109375\n",
      "Train Epoch: 50 [6400/6893 (1%)]\tLoss: 183520.625000\n",
      "====> Epoch: 50 Average loss: 183529.8860\n",
      "====> Test set loss: 63.8926\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 50 + 1):\n",
    "        train_hypernetwork(train_data, vae_l1_hypernetwork, vae_l1_hypernetwork_optimizer, epoch)\n",
    "        #with torch.no_grad():\n",
    "        #    model.diag.data[torch.abs(model.diag) < 0.05] = 0\n",
    "        test(test_data, vae_l1_hypernetwork, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2707, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((vae_l1_hypernetwork(train_data[0:64, :])[0] - train_data[0:64, :])**2) / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2552, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum((vae_l1_hypernetwork(test_data[0:64, :])[0] - test_data[0:64, :])**2) / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vae_l1_hypernetwork(test_data[0:64, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([103, 130, 126,  72,  29,  18,  10,   6,   5,   1]),\n",
       " array([6.9841743e-05, 6.0694804e-03, 1.2069119e-02, 1.8068759e-02,\n",
       "        2.4068397e-02, 3.0068036e-02, 3.6067676e-02, 4.2067315e-02,\n",
       "        4.8066951e-02, 5.4066591e-02, 6.0066231e-02], dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(torch.abs(vae_l1_hypernetwork.l1_weights)[1, :].clone().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.255764"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(torch.abs(vae_l1_hypernetwork.l1_weights)[1, :].clone().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(torch.abs(vae_l1_hypernetwork.l1_weights)[1, :].clone().detach().cpu().numpy() < 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.69319303e-02, 3.10958177e-03, 2.43317857e-02, 1.94285996e-02,\n",
       "       1.20058507e-02, 4.48931754e-03, 2.50979159e-02, 1.51752867e-02,\n",
       "       5.92885353e-03, 7.98936561e-03, 1.91015601e-02, 3.21855247e-02,\n",
       "       2.04350948e-02, 9.63868387e-03, 3.49931046e-02, 1.67993177e-02,\n",
       "       3.26597458e-03, 7.01383129e-03, 1.01694260e-02, 1.76095217e-02,\n",
       "       9.89056379e-03, 1.47589184e-02, 2.23821178e-02, 9.32968594e-03,\n",
       "       1.00713596e-02, 1.85512789e-02, 7.53364898e-03, 3.59241106e-03,\n",
       "       3.11556123e-02, 1.80314649e-02, 1.20125711e-04, 5.79204410e-03,\n",
       "       3.33424881e-02, 6.16349839e-03, 4.64464352e-03, 1.42324269e-02,\n",
       "       4.22804570e-03, 5.95882162e-03, 3.46041471e-03, 3.89055870e-02,\n",
       "       1.00746844e-02, 2.13784948e-02, 5.36743551e-03, 2.38343831e-02,\n",
       "       1.26601188e-02, 9.98461246e-03, 1.27780885e-02, 1.92644857e-02,\n",
       "       5.58926724e-03, 1.03354119e-02, 2.02848762e-03, 6.94607943e-03,\n",
       "       2.93867569e-02, 1.17492406e-02, 1.06210560e-02, 5.27538732e-03,\n",
       "       1.48054166e-02, 1.93771534e-02, 5.45595540e-03, 9.56112612e-03,\n",
       "       1.77266560e-02, 1.20707229e-02, 7.14296475e-04, 1.89404842e-03,\n",
       "       1.07112005e-02, 1.71176419e-02, 1.98904313e-02, 3.09408512e-02,\n",
       "       7.13274209e-03, 6.00750791e-04, 2.05511246e-02, 1.41724609e-02,\n",
       "       1.51210185e-02, 1.79001205e-02, 1.25600919e-02, 5.00186905e-03,\n",
       "       2.25322470e-02, 3.28414217e-02, 9.11435112e-03, 1.41388737e-02,\n",
       "       7.76052848e-03, 1.38608925e-03, 1.98111888e-02, 7.09449453e-03,\n",
       "       2.63804048e-02, 1.18215829e-02, 1.28445439e-02, 7.67267868e-03,\n",
       "       8.33297893e-03, 8.22275504e-03, 2.12106109e-03, 1.70768127e-02,\n",
       "       1.59748532e-02, 1.26877800e-04, 1.58926360e-02, 1.29009821e-02,\n",
       "       2.97174007e-02, 1.89979468e-02, 8.84686410e-03, 1.75913647e-02,\n",
       "       1.62240993e-02, 5.04303165e-03, 5.70533797e-03, 1.83004271e-02,\n",
       "       1.16707943e-02, 4.13436294e-02, 1.24442317e-02, 6.71863556e-03,\n",
       "       3.55851278e-03, 8.99026729e-03, 8.73147324e-03, 2.40318961e-02,\n",
       "       1.42263873e-02, 2.31792033e-02, 1.04869725e-02, 8.08872655e-03,\n",
       "       7.49691762e-03, 4.38080952e-02, 1.87298954e-02, 2.07473058e-02,\n",
       "       3.82032990e-03, 5.82814869e-03, 3.77961285e-02, 1.11294650e-02,\n",
       "       2.86967829e-02, 2.30101198e-02, 1.13045694e-02, 5.85611723e-03,\n",
       "       1.11304224e-02, 2.36505568e-02, 2.07526609e-02, 1.14120748e-02,\n",
       "       8.97486322e-03, 2.82959696e-02, 9.62313451e-03, 2.43380889e-02,\n",
       "       1.04343360e-02, 1.60857439e-02, 1.40448436e-02, 9.15975310e-03,\n",
       "       6.57086074e-03, 1.13575514e-02, 2.77045090e-03, 8.45709629e-03,\n",
       "       1.99079700e-02, 3.32148112e-02, 1.43779926e-02, 9.37143620e-03,\n",
       "       1.83291435e-02, 7.89769180e-03, 1.19477399e-02, 1.38951652e-02,\n",
       "       1.53128970e-02, 1.60239041e-02, 1.31101590e-02, 8.71929899e-03,\n",
       "       5.87709248e-04, 1.64829865e-02, 9.79030877e-03, 2.00022459e-02,\n",
       "       1.10852458e-02, 2.61760615e-02, 2.49904618e-02, 1.36692971e-02,\n",
       "       1.70964319e-02, 1.51515491e-02, 3.54967192e-02, 2.61176620e-02,\n",
       "       1.49562582e-03, 2.02374160e-02, 1.40847452e-03, 5.81945293e-03,\n",
       "       2.84414738e-04, 5.41357510e-03, 1.89439803e-02, 3.64493690e-02,\n",
       "       2.99243163e-02, 1.12012215e-02, 6.02100883e-03, 8.31611548e-03,\n",
       "       9.05093923e-03, 5.36308996e-03, 1.41978823e-03, 4.89566359e-04,\n",
       "       1.84460040e-02, 1.92195661e-02, 1.41294710e-02, 1.12908483e-02,\n",
       "       1.56359449e-02, 1.95671208e-02, 9.48569970e-04, 9.96494107e-03,\n",
       "       1.74416900e-02, 1.58045217e-02, 2.58859210e-02, 4.61464236e-03,\n",
       "       1.93321686e-02, 2.62194313e-03, 1.57046448e-02, 2.78551318e-03,\n",
       "       2.31729709e-02, 1.43357012e-02, 4.84249555e-03, 1.09150968e-02,\n",
       "       5.10855857e-03, 7.61371106e-04, 3.24152075e-02, 6.07831776e-03,\n",
       "       1.07938796e-02, 1.90672837e-02, 1.69049446e-02, 1.65569838e-02,\n",
       "       9.35507752e-03, 5.35942428e-02, 3.25503051e-02, 2.53395028e-02,\n",
       "       4.88861203e-02, 9.69917886e-03, 1.86472386e-02, 1.14591941e-02,\n",
       "       1.79396439e-02, 1.10272299e-02, 1.87791139e-02, 2.05753744e-03,\n",
       "       1.93504654e-02, 2.73153000e-03, 1.25323571e-02, 9.04094800e-03,\n",
       "       1.32008642e-02, 1.40454415e-02, 1.70215480e-02, 8.34988430e-03,\n",
       "       9.71016288e-03, 1.22282971e-02, 1.63351446e-02, 1.80747360e-03,\n",
       "       2.91195977e-03, 1.00558046e-02, 8.62860307e-03, 1.38073824e-02,\n",
       "       1.77767929e-02, 2.33730450e-02, 2.01721378e-02, 5.29166497e-03,\n",
       "       1.85192246e-02, 1.08288564e-02, 1.34574091e-02, 5.31554967e-02,\n",
       "       1.41466074e-02, 4.17218357e-02, 7.43618701e-04, 3.79253551e-03,\n",
       "       1.72122754e-02, 1.49903633e-02, 1.07751507e-02, 1.13314912e-02,\n",
       "       1.79472305e-02, 1.75077654e-02, 1.73025392e-02, 1.06317736e-03,\n",
       "       3.51469964e-04, 2.83568557e-02, 6.97171316e-03, 4.47440520e-03,\n",
       "       9.08399560e-03, 7.12182745e-03, 1.49115250e-02, 1.89230889e-02,\n",
       "       2.06304025e-02, 2.68643573e-02, 1.80590190e-02, 1.02981664e-02,\n",
       "       2.94832140e-02, 1.47724394e-02, 2.66568009e-02, 5.06411679e-03,\n",
       "       1.41041102e-02, 1.01777595e-02, 5.32926433e-03, 3.30119431e-02,\n",
       "       8.69465899e-03, 1.35327335e-02, 6.36970252e-03, 1.23584084e-02,\n",
       "       5.08342311e-03, 3.22930366e-02, 6.04721531e-03, 1.51521973e-02,\n",
       "       1.29813068e-02, 1.97044555e-02, 1.32706873e-02, 3.70275937e-02,\n",
       "       1.43772233e-02, 1.65553782e-02, 1.29835214e-02, 1.71398260e-02,\n",
       "       1.22116338e-02, 2.80055106e-02, 1.15867890e-02, 1.03486162e-02,\n",
       "       1.32733327e-03, 6.42072409e-04, 3.24230716e-02, 8.51237588e-03,\n",
       "       5.00936899e-03, 1.13399047e-02, 2.06608325e-04, 1.19006801e-02,\n",
       "       1.87111814e-02, 1.59312561e-02, 1.41419638e-02, 1.43148545e-02,\n",
       "       1.63544677e-02, 9.49954148e-03, 2.36979816e-02, 1.51679339e-02,\n",
       "       9.26106423e-03, 1.08633041e-02, 2.21806113e-02, 6.76052086e-03,\n",
       "       1.60604920e-02, 4.26067598e-02, 8.70267302e-03, 1.37536041e-03,\n",
       "       2.96701416e-02, 1.62702389e-02, 4.92649525e-03, 1.04622673e-02,\n",
       "       7.65426969e-03, 1.56087447e-02, 7.97609985e-03, 4.55608219e-02,\n",
       "       4.17117029e-04, 1.75221823e-02, 5.31716943e-02, 1.83096807e-02,\n",
       "       8.38115439e-03, 1.22294351e-02, 7.76871014e-03, 3.43847238e-02,\n",
       "       7.19451532e-03, 9.88878962e-03, 2.91036814e-03, 4.70410846e-03,\n",
       "       3.59710827e-02, 9.77192633e-03, 1.31997578e-02, 1.02266707e-02,\n",
       "       1.27430633e-02, 1.57255419e-02, 6.60746079e-03, 1.71222351e-03,\n",
       "       2.03600898e-03, 7.87585974e-03, 3.89935970e-02, 6.98417425e-05,\n",
       "       1.72156654e-03, 6.91390410e-03, 7.80823175e-04, 2.34105252e-03,\n",
       "       1.92571618e-03, 2.12629661e-02, 5.65259997e-03, 2.99342815e-02,\n",
       "       1.27061876e-03, 1.53699573e-02, 9.19519737e-03, 8.36391281e-03,\n",
       "       3.46772186e-03, 1.47365332e-02, 1.34840226e-02, 1.51140392e-02,\n",
       "       1.47472937e-02, 5.07798791e-03, 2.66250968e-03, 2.63708876e-04,\n",
       "       2.38329265e-02, 2.60576047e-02, 2.52891555e-02, 1.87490471e-02,\n",
       "       7.88017549e-03, 1.65106468e-02, 1.83438975e-02, 1.43560059e-02,\n",
       "       1.08072758e-02, 9.58831236e-03, 1.91249158e-02, 1.76748242e-02,\n",
       "       1.29771028e-02, 2.24144366e-02, 1.72843635e-02, 1.61904655e-02,\n",
       "       2.00548097e-02, 1.46958865e-02, 5.49088418e-03, 1.29640307e-02,\n",
       "       1.97666138e-02, 1.89206582e-02, 9.91157815e-03, 2.25709751e-03,\n",
       "       3.93760949e-03, 1.82890147e-03, 5.28230891e-03, 1.91298779e-03,\n",
       "       3.52680720e-02, 1.94841903e-02, 2.72291303e-02, 4.94262204e-02,\n",
       "       1.92257389e-02, 8.56583472e-03, 2.34729834e-02, 1.92776732e-02,\n",
       "       2.08374672e-02, 9.26585309e-03, 1.60071701e-02, 1.92460455e-02,\n",
       "       1.74353719e-02, 5.77176129e-03, 7.69010931e-03, 1.15992567e-02,\n",
       "       1.54666193e-02, 2.22713500e-02, 1.75539423e-02, 2.59209424e-04,\n",
       "       1.04033193e-02, 1.26236118e-03, 1.03881964e-02, 1.20361149e-03,\n",
       "       2.52357163e-02, 1.23362541e-02, 6.14193082e-03, 9.48160142e-03,\n",
       "       4.29060236e-02, 1.90050770e-02, 8.99224356e-03, 2.12375410e-02,\n",
       "       3.73406522e-02, 7.33442605e-03, 1.49525329e-02, 1.47364549e-02,\n",
       "       1.88074894e-02, 1.25761107e-02, 1.33684203e-02, 3.36229801e-03,\n",
       "       3.75725701e-03, 2.25166269e-02, 1.77790336e-02, 1.45695414e-02,\n",
       "       1.36298127e-02, 4.72492725e-03, 9.43165645e-03, 1.81053635e-02,\n",
       "       1.16645396e-02, 1.47512667e-02, 9.84438136e-03, 4.12954353e-02,\n",
       "       4.94557805e-03, 2.97135860e-03, 3.13789509e-02, 1.21128168e-02,\n",
       "       9.12387669e-03, 1.14371143e-02, 8.39623064e-03, 5.41286916e-03,\n",
       "       1.12335728e-02, 9.54907201e-03, 3.98400277e-02, 1.39026362e-02,\n",
       "       1.28587997e-02, 1.41456556e-02, 1.99302956e-02, 1.45689221e-02,\n",
       "       9.58453957e-03, 1.42141134e-02, 1.40603967e-02, 2.32974123e-02,\n",
       "       3.19501851e-03, 4.92250826e-03, 1.41235553e-02, 2.19350457e-02,\n",
       "       2.75414400e-02, 6.45656837e-03, 1.50192715e-02, 2.03586202e-02,\n",
       "       5.42023405e-03, 1.00858510e-02, 9.26283002e-03, 2.34462116e-02,\n",
       "       4.41011675e-02, 7.50123523e-03, 3.32293771e-02, 4.44721170e-02,\n",
       "       1.31570566e-02, 2.91151926e-03, 2.63574347e-02, 1.25744846e-02,\n",
       "       1.14988955e-02, 6.00662306e-02, 1.40093947e-02, 2.81669572e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(vae_l1_hypernetwork.l1_weights)[1, :].clone().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance wise does not work very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
