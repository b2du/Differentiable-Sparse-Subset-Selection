{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "img_size = 28\n",
    "channels = 1\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "z_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        BASE_PATH_DATA + '/mnist/train',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor()]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        BASE_PATH_DATA + '/mnist/test', \n",
    "        train=False, \n",
    "        download = True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor()]\n",
    "        )\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n"
     ]
    }
   ],
   "source": [
    "for i, (imgs, _) in enumerate(train_loader):\n",
    "    print(imgs.view(64, -1).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create VAE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_layer_size, z_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         '../data/first_test_results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(400, 20).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 549.899109\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 232.273758\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 216.937256\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 185.842010\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 173.044464\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 162.104416\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 157.920715\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 154.271851\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 159.920441\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 151.986801\n",
      "====> Epoch: 1 Average loss: 189.1177\n",
      "====> Test set loss: 146.2478\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 148.430496\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 142.440918\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 147.284546\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 135.962860\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 137.306427\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 133.011887\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 137.823700\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 129.961212\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 131.790955\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 132.804489\n",
      "====> Epoch: 2 Average loss: 137.1189\n",
      "====> Test set loss: 128.6407\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 131.235458\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 129.351456\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 130.306122\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 118.274696\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 122.968391\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 119.852745\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 127.901749\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 122.177628\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 122.130432\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 122.209862\n",
      "====> Epoch: 3 Average loss: 125.7413\n",
      "====> Test set loss: 121.1904\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 121.499603\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 125.272644\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 117.595932\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 124.923645\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 118.309883\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 116.289185\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 119.994530\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 117.793335\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 123.237694\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 116.449265\n",
      "====> Epoch: 4 Average loss: 120.0691\n",
      "====> Test set loss: 117.0501\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 113.382187\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 122.845703\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 116.752106\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 113.702324\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 124.506088\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 124.652924\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 119.082695\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 112.397552\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 122.065666\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 121.975769\n",
      "====> Epoch: 5 Average loss: 116.7338\n",
      "====> Test set loss: 114.4730\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 116.591156\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 110.954460\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 116.051132\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 119.606865\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 112.309013\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 117.339134\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 109.806175\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 115.035835\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 115.304588\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 113.886093\n",
      "====> Epoch: 6 Average loss: 114.5801\n",
      "====> Test set loss: 112.9060\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 120.317581\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 107.264397\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 110.356949\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 107.424362\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 116.071884\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 116.101814\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 119.845085\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 109.532494\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 110.397049\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 112.653748\n",
      "====> Epoch: 7 Average loss: 112.9663\n",
      "====> Test set loss: 111.4084\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 114.089516\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 114.734833\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 117.026321\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 110.927933\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 114.687180\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 109.669540\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 106.698723\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 114.886436\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 118.988937\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 113.606972\n",
      "====> Epoch: 8 Average loss: 111.7743\n",
      "====> Test set loss: 110.3703\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 109.332069\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 111.263138\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 115.801331\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 107.834396\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 111.654808\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 108.971008\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 117.833344\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 106.189339\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 107.622864\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 112.388664\n",
      "====> Epoch: 9 Average loss: 110.7858\n",
      "====> Test set loss: 109.6843\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 110.522118\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 108.387054\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 114.222061\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 109.481483\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 105.383453\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 106.387611\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 106.923569\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 107.582436\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 114.494247\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 110.407257\n",
      "====> Epoch: 10 Average loss: 110.0380\n",
      "====> Test set loss: 108.9337\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 107.220360\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 107.504616\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 108.582100\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 102.166656\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 110.953377\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 110.854652\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 107.288223\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 106.312485\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 110.785294\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 107.616577\n",
      "====> Epoch: 11 Average loss: 109.3506\n",
      "====> Test set loss: 108.4855\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 108.823181\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 108.797379\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 109.182976\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 105.554779\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 104.146996\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 108.011101\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 109.917564\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 111.868286\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 110.616852\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 102.180679\n",
      "====> Epoch: 12 Average loss: 108.8034\n",
      "====> Test set loss: 107.9893\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 111.218559\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 108.091599\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 103.728043\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 107.317940\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 111.794769\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 111.007614\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 111.695107\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 108.192314\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 116.749153\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 110.896698\n",
      "====> Epoch: 13 Average loss: 108.3068\n",
      "====> Test set loss: 107.6417\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 107.849304\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 112.194687\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 109.761642\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 106.772057\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 116.734634\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 107.001274\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 105.349480\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 104.888611\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 104.291092\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 113.461380\n",
      "====> Epoch: 14 Average loss: 107.9157\n",
      "====> Test set loss: 107.2868\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 106.069023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 110.580002\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 104.176094\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 109.173622\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 113.009750\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 103.355576\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 103.198006\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 106.284203\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 104.065674\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 107.116760\n",
      "====> Epoch: 15 Average loss: 107.5404\n",
      "====> Test set loss: 106.8031\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 106.903709\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 109.010292\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 109.700287\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 106.086975\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 106.148499\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 106.293884\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 104.195129\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 108.470291\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 105.119354\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 105.332108\n",
      "====> Epoch: 16 Average loss: 107.2138\n",
      "====> Test set loss: 106.7164\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 107.360428\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 104.624908\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 106.957176\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 106.560555\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 107.807137\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 108.100471\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 105.978493\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 107.049522\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 100.410187\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 104.640884\n",
      "====> Epoch: 17 Average loss: 106.9548\n",
      "====> Test set loss: 106.1914\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 108.529915\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 104.888176\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 105.413666\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 102.348228\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 109.433128\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 107.440117\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 108.569534\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 108.125839\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 108.076096\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 106.743263\n",
      "====> Epoch: 18 Average loss: 106.6605\n",
      "====> Test set loss: 105.9646\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 105.156738\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 116.233429\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 107.041298\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 100.978386\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 107.223572\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 102.018082\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 106.668884\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 108.906380\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 101.665451\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 105.406158\n",
      "====> Epoch: 19 Average loss: 106.4091\n",
      "====> Test set loss: 106.0776\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 102.419037\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 110.566986\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 105.806717\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 108.571152\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 110.030594\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 112.380531\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 104.292892\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 98.229057\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 105.436752\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 105.920975\n",
      "====> Epoch: 20 Average loss: 106.2158\n",
      "====> Test set loss: 105.6817\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train(model, epoch)\n",
    "        test(model, epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       '../data/first_test_results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), BASE_PATH_DATA + \"../data/models/first_try/no_norm.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_layer = model.fc1.weight.detach().clone().numpy()\n",
    "abs_selection_layer = np.abs(selection_layer)\n",
    "l1_norm_each_column = np.apply_along_axis(sum, 0, abs_selection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.16928223,  6.79531663,  7.04835737,  7.52466378,  7.03603591,\n",
       "        7.23565822,  7.33732005,  7.10005973,  7.25097034,  7.06094152,\n",
       "        7.078739  ,  7.22324464,  8.57032475,  8.53857566,  8.41128401,\n",
       "        8.27567321,  7.4529977 ,  7.32203708,  6.9548209 ,  7.14029472,\n",
       "        6.96818907,  7.47996694,  6.93000132,  7.06859413,  7.12124071,\n",
       "        7.40462647,  7.30687544,  7.13479727,  7.47113119,  7.01731506,\n",
       "        7.03029225,  6.9724107 ,  8.62737831,  9.56051702,  9.13327907,\n",
       "       10.82679726,  9.75390516, 10.20154518, 11.11397037, 10.99861551,\n",
       "       11.58842038, 12.51114525, 11.63198051, 11.70973964, 12.17519625,\n",
       "       11.04525674, 11.83737912, 10.94167665, 10.06003396,  9.99716537,\n",
       "        9.90048197,  9.96999168,  6.70643645,  7.43420202,  7.16658263,\n",
       "        7.32094636,  7.28066298,  7.16297664,  8.92313802,  9.70706975,\n",
       "       11.38708763, 12.89792362, 11.43264899, 14.12573355, 11.78248592,\n",
       "       12.17685827, 12.3706413 , 12.27785661, 12.61130308, 11.96072107,\n",
       "       12.6417858 , 12.82823046, 12.00461435, 12.46036296, 12.69088271,\n",
       "       11.86844945, 11.96091943, 11.09094638, 11.09165672, 11.4322377 ,\n",
       "        9.77550383,  9.25748547,  6.84665839,  6.94791203,  6.82103372,\n",
       "        6.860894  , 10.531161  , 10.82070973, 12.66621936, 10.54872851,\n",
       "       10.89455091, 11.59114886, 11.12340132, 12.28932465, 12.06759823,\n",
       "       11.65398456, 12.05767909, 11.96299862, 12.06141728, 12.30668857,\n",
       "       11.94939056, 12.73127137, 12.34094444, 13.03873371, 13.23294918,\n",
       "       13.45998356, 12.84680222, 11.41149524, 11.50966179, 11.12221787,\n",
       "        8.88661152,  7.38967881,  6.98470021, 10.31146015, 10.92147936,\n",
       "       12.14298454, 11.3832194 , 12.71546524, 12.93346753, 12.53740471,\n",
       "       11.54428712, 12.18688593, 11.96243985, 12.22138044, 11.67751047,\n",
       "       11.68185805, 12.18086046, 11.20199946, 12.01028779, 11.11778892,\n",
       "       11.81638822, 12.90299432, 12.83404995, 12.52926714, 13.22278074,\n",
       "       12.65985151, 12.60863206, 12.77914744, 10.13035335, 10.87250272,\n",
       "        7.07977165,  7.29997206, 12.24469076, 10.95932076, 12.34473517,\n",
       "       12.65878905, 12.43058039, 12.01094722, 12.3040237 , 11.54966143,\n",
       "       12.13621071, 11.36376354, 11.06034283, 10.78984944, 11.0721694 ,\n",
       "       10.97454738, 10.79438939, 10.95246432, 11.53303012, 11.65007212,\n",
       "       11.57534727, 13.17017922, 13.24329313, 13.08274   , 13.74382588,\n",
       "       12.29734582, 10.99732102, 10.17763618,  7.31543478,  9.54103545,\n",
       "       12.04038044, 12.52350536, 12.31879266, 11.84046822, 11.57555119,\n",
       "       11.62478861, 11.90424632, 11.58697201, 10.58424393, 11.45914694,\n",
       "       10.681102  , 10.27660155, 10.94669768, 10.58508623, 10.1073699 ,\n",
       "       10.13718523, 10.43591826, 11.57841134, 11.12792395, 12.34817366,\n",
       "       12.64484868, 12.98345291, 13.79734274, 13.49870163, 11.50757711,\n",
       "        9.84719613,  9.20840634, 10.82236357, 11.79111896, 11.3614982 ,\n",
       "       11.63439614, 11.97945591, 12.26836799, 11.69028589, 12.29105521,\n",
       "       10.91008729, 10.66399568, 11.25737445, 10.35970068, 10.58631497,\n",
       "       10.44889115,  9.92071068, 10.54060186, 10.42969923, 10.79140943,\n",
       "       10.84231721, 11.63300777, 12.14729927, 12.59530148, 13.31062525,\n",
       "       13.59007501, 13.07377625, 12.3206116 , 11.16732214,  9.88495327,\n",
       "       11.24421316, 11.07608935, 12.67120583, 12.37363843, 13.03371802,\n",
       "       12.83681206, 11.55593292, 11.90832783,  9.96594222, 11.03167723,\n",
       "       10.44690351, 10.12288336, 10.90695676,  9.98618241, 10.23636239,\n",
       "       10.23475063, 10.90751493, 11.41970771, 11.51538815, 11.70138521,\n",
       "       11.94488642, 12.84155517, 13.14756738, 13.11228602, 12.92201064,\n",
       "       11.43123151,  9.7802522 ,  9.70976795, 11.45779851, 11.09718159,\n",
       "       11.00590826, 11.72970998, 13.06278918, 12.58949901, 11.70532239,\n",
       "       11.22374166, 10.87437893, 10.93520355, 10.91703951, 10.75982209,\n",
       "       10.79228908, 10.48325191, 10.19806355, 10.36300828,  9.88648008,\n",
       "       10.56759741, 11.34240079, 10.91839415, 11.82813659, 13.06709262,\n",
       "       13.15728828, 13.21089578, 13.42952656, 12.24183575, 10.05085051,\n",
       "        9.49092232, 10.93249184, 11.846426  , 12.30736338, 12.72873577,\n",
       "       12.41911278, 11.75104772, 11.95301098, 11.14671228, 11.49582783,\n",
       "       11.0755849 , 10.87697118, 10.99552365, 10.5436793 , 11.02463777,\n",
       "       12.02405131, 10.99075129, 10.11589604, 10.98019071, 11.42204666,\n",
       "       11.58025038, 12.149198  , 13.07681486, 13.50280003, 14.34319944,\n",
       "       15.22369119, 12.34234705, 11.58378552,  9.71598964, 10.54033148,\n",
       "       11.54881457, 12.6742042 , 13.30931385, 11.70711823, 12.22037005,\n",
       "       12.09854735, 11.30598678, 12.02250033, 11.21190962, 11.72246065,\n",
       "       11.3875178 , 10.90514667, 11.43660432, 11.34752987, 10.85182627,\n",
       "       10.43662907, 10.73170812, 11.27822673, 10.99504636, 12.13601446,\n",
       "       12.13627199, 13.33964516, 13.93142756, 14.5032806 , 14.37081211,\n",
       "       11.81995035,  8.86404644,  9.85495861, 12.02173301, 11.86149335,\n",
       "       13.45063465, 11.92462419, 12.52105065, 11.48628022, 11.01490081,\n",
       "       11.56345399, 11.52016424, 11.32732963, 11.58444076, 11.53566871,\n",
       "       11.30143845, 11.70500622, 10.87685027, 10.63946077, 10.72061444,\n",
       "       10.84245671, 11.87653772, 12.36785715, 12.09559514, 13.3813499 ,\n",
       "       12.9862866 , 13.88579267, 12.83228035, 10.70795826,  8.85977086,\n",
       "       10.0399282 , 10.69763111, 12.59478838, 12.8031656 , 12.05546169,\n",
       "       11.93099158, 11.50931891, 11.33418112, 11.66097363, 11.24197546,\n",
       "       10.59044347, 12.18892439, 10.82733704, 10.96183287, 10.54498257,\n",
       "       10.63824931, 11.13581154, 10.46479094, 11.70808737, 11.04828333,\n",
       "       12.42313735, 12.96033782, 13.34481049, 13.17352935, 13.95000701,\n",
       "       12.22899517, 10.14626629,  9.02640251,  9.46686255, 11.52261921,\n",
       "       12.44911889, 11.94041703, 12.26310103, 11.47738164, 11.17078335,\n",
       "       11.96372332, 11.28530619, 11.1309498 , 11.0715898 , 11.73128736,\n",
       "       10.25381029, 10.63732511, 10.05969593, 10.56657776, 10.79030171,\n",
       "       11.14359755, 11.28857944, 12.36246876, 12.71618611, 12.36533101,\n",
       "       12.07802737, 12.66402333, 12.35808725, 10.62305883, 11.01549966,\n",
       "        9.88602009,  8.16347759, 12.55833845, 13.27812143, 12.46947858,\n",
       "       11.65907784, 12.41744919, 11.97605698, 12.15356613, 11.22396701,\n",
       "       11.60209477, 11.70084783, 11.98949498, 10.64954249, 10.72916244,\n",
       "       10.6974433 , 11.06979723, 10.50879752, 10.82792994, 11.20665266,\n",
       "       12.05423005, 12.54286184, 13.39064633, 12.43734752, 12.86746011,\n",
       "       13.54037414, 11.00964244,  9.74334069, 10.13086281,  9.28873908,\n",
       "       11.75296692, 13.23109812, 13.23029965, 12.8462274 , 12.48854014,\n",
       "       12.18020888, 11.67067988, 12.3756737 , 11.81668004, 11.34068711,\n",
       "       11.69043073, 11.11442121, 10.6558818 , 10.36751896, 11.1271803 ,\n",
       "       10.31782838, 11.04259797, 11.94291457, 11.94809206, 12.63280448,\n",
       "       12.44233159, 13.32783733, 12.81009411, 12.67502426, 12.29193568,\n",
       "        9.53694819,  6.98783609, 10.39691866, 11.03032556, 11.78236246,\n",
       "       12.53084463, 13.2521518 , 12.13663223, 12.05806256, 11.94295736,\n",
       "       12.40200772, 11.78407416, 11.90595181, 11.77258121, 11.3484616 ,\n",
       "       10.70899849, 10.63320159, 10.94713233, 11.00841813, 10.59025256,\n",
       "       11.82217283, 11.13009764, 12.1798145 , 11.93763551, 12.79345151,\n",
       "       13.45629312, 12.95309709, 11.58027711, 15.13827352,  9.96420494,\n",
       "        9.80771893, 10.88297197, 13.20469538, 13.10817648, 12.09530253,\n",
       "       11.97383607, 12.27298794, 11.78582547, 12.71869892, 12.50562373,\n",
       "       11.49651241, 12.18569285, 11.90126161, 10.58577616, 11.08278883,\n",
       "       11.15162396, 10.28695494, 11.60166736, 11.81679432, 12.14336274,\n",
       "       12.61005121, 12.23353357, 12.3425867 , 12.80348231, 12.07893732,\n",
       "       11.93217284, 11.53732314,  9.14960043, 10.9893268 , 11.37258296,\n",
       "       12.64353754, 13.28908238, 12.25623487, 12.37252174, 12.5563907 ,\n",
       "       12.04106405, 11.9242292 , 12.04062722, 11.65510422, 10.82130029,\n",
       "       11.17851476, 10.65070086, 10.58104313, 10.57417969, 10.63842634,\n",
       "       10.98629928, 11.82306611, 12.01373919, 12.25139484, 12.94686278,\n",
       "       12.79669894, 12.72329785, 12.96396079, 11.81902475, 10.84180752,\n",
       "        6.87778644, 11.26568447, 11.4953064 , 11.95323177, 12.49784264,\n",
       "       12.29685863, 11.3648895 , 12.10536062, 12.17961164, 11.51974807,\n",
       "       11.19744487, 11.2960673 , 10.67983286, 10.51953443, 10.21910649,\n",
       "       10.37422472, 10.81949297, 11.04702311, 11.14811723, 11.01617515,\n",
       "       11.98291277, 12.78133708, 12.98082718, 12.87341612, 14.68780162,\n",
       "       13.50513876, 11.69954353,  9.76194834, 10.99710292, 11.02433597,\n",
       "       11.95616453, 11.69942212, 13.04804578, 12.77888419, 11.69809069,\n",
       "       11.05532229, 11.06080728, 11.06845193, 11.25554723, 10.54385325,\n",
       "       10.28822623, 10.07586011,  9.75778659, 10.34371083, 11.13625297,\n",
       "       11.16420341, 11.24188701, 11.97980643, 12.65369718, 13.02403944,\n",
       "       13.99532785, 13.96120037, 13.56499772, 12.86003858, 11.9116566 ,\n",
       "        8.41778436, 10.75979574, 10.07989927, 11.75344921, 12.47102914,\n",
       "       12.91765522, 12.34178313, 11.70926435, 12.09555206, 11.14226598,\n",
       "       10.76878821, 10.17244961, 10.80957526, 10.12262715,  9.73679647,\n",
       "       10.24165666, 10.54111771, 10.81877519, 11.48850418, 12.12217795,\n",
       "       12.29572002, 13.14809607, 13.45664337, 14.49556316, 13.87423857,\n",
       "       12.10522957, 12.71321914, 12.88939607,  8.72713196,  7.25720779,\n",
       "        7.17317433, 11.65946744, 12.74897044, 12.74198917, 12.59822552,\n",
       "       12.50459267, 12.85253304, 11.13959843, 11.23947724, 11.2967502 ,\n",
       "       10.71819291, 10.55743916, 10.48071308, 11.04976245, 11.00953272,\n",
       "       11.49005757, 11.71391897, 12.12391205, 13.6426864 , 14.55243858,\n",
       "       13.91527636, 14.18323295, 13.71090487, 13.98765669, 13.34085885,\n",
       "       12.97527098,  7.13244854,  6.77069136,  6.85516146, 10.59220294,\n",
       "       11.79607226, 13.44934211, 13.15819762, 13.28106877, 12.67455094,\n",
       "       11.98654545, 12.06619988, 11.95708151, 11.71545964, 11.73210293,\n",
       "       12.25873215, 12.60574742, 12.09274856, 11.85471509, 11.36810577,\n",
       "       12.45909807, 13.18618984, 13.41618134, 14.03626957, 13.77720591,\n",
       "       13.49523172, 12.1579072 , 13.97141131, 11.81630442,  7.10721328,\n",
       "        7.06441357,  7.21202003, 11.53469722, 11.12100463, 11.03990925,\n",
       "       13.12990155, 13.0939703 , 12.88536897, 12.53553653, 12.55729018,\n",
       "       12.4562491 , 12.88552115, 12.85284376, 13.66700116, 13.16229882,\n",
       "       12.40772261, 13.09376765, 12.39882828, 11.87460894, 13.41153517,\n",
       "       13.41022366, 13.27509528, 12.63685833, 12.99650866, 11.27209257,\n",
       "       10.88288163,  9.81740054,  7.15367811,  6.89025469,  7.2821905 ,\n",
       "        6.95647752,  8.48529115, 10.27436273, 11.67557266, 12.48861314,\n",
       "       12.41333974, 12.03212139, 12.17687513, 12.89595251, 12.5043231 ,\n",
       "       12.89235791, 13.29862257, 13.14296591, 12.31115705, 12.9014392 ,\n",
       "       12.28670711, 12.68246385, 11.71471093, 10.5209434 , 12.56551776,\n",
       "       12.12351907, 11.61372182, 10.59229557,  8.69559581,  7.34128439,\n",
       "        7.17622766,  7.12978821,  6.97988743,  7.18495129,  7.21886427,\n",
       "        8.48644823,  8.87102038, 10.47204004, 10.67416583, 10.58482801,\n",
       "       10.34865657, 12.63740812, 12.33326224, 11.76587196, 12.55877691,\n",
       "       11.33119907, 11.15643407, 11.013537  , 11.8708082 , 10.73380646,\n",
       "       10.41817985, 10.58660576, 10.07886785, 10.30945169,  8.87987433,\n",
       "        7.37069232,  7.00931472,  6.99295807,  7.22313717])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm_each_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Norm with an extra layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_l1(nn.Module):\n",
    "    def __init__(self, hidden_layer_size, z_size):\n",
    "        super(VAE_l1, self).__init__()\n",
    "        \n",
    "        self.l1_clip_layer = nn.Linear(784, 784, bias = False)\n",
    "        self.fc1 = nn.Linear(784, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h0 = self.l1_clip_layer(x)\n",
    "        h1 = F.relu(self.fc1(h0))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1_clip = VAE_l1(400, 20).to(device)\n",
    "\n",
    "l1_norm = np.linalg.norm(model_l1_clip.l1_clip_layer.weight.detach().clone().numpy(), ord=1)\n",
    "model_l1_clip.l1_clip_layer.weight = torch.nn.Parameter(model_l1_clip.l1_clip_layer.weight / l1_norm)\n",
    "    \n",
    "optimizer = torch.optim.Adam(model_l1_clip.parameters(), lr=lr, betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clip_weight(model, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        l1_norm = np.linalg.norm(model.l1_clip_layer.weight.detach().clone().numpy(), ord=1)\n",
    "        model.l1_clip_layer.weight = torch.nn.Parameter(model.l1_clip_layer.weight / l1_norm)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "            \n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         '../data/first_test_results_l1_norm_extra_layer/reconstruction_' + \n",
    "                           str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.024780\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 232.441010\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 218.676849\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 223.276184\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 217.332779\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 207.934830\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 207.903534\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 208.782837\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 191.576874\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 200.293961\n",
      "====> Epoch: 1 Average loss: 221.3336\n",
      "====> Test set loss: 194.4671\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 191.051834\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 195.473236\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 197.087112\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 192.325607\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 194.291702\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 192.360519\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 180.752930\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 185.606796\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 180.501663\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 173.758759\n",
      "====> Epoch: 2 Average loss: 184.7982\n",
      "====> Test set loss: 168.8999\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 178.687164\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 161.453522\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 151.880203\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 160.063416\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 156.004440\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 164.352585\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 155.878052\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 156.611420\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 155.675690\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 144.002167\n",
      "====> Epoch: 3 Average loss: 160.0771\n",
      "====> Test set loss: 151.2395\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 162.113342\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 152.343246\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 146.772827\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 141.317596\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 148.044846\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 145.524811\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 144.912857\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 145.705231\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 141.001892\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 141.726547\n",
      "====> Epoch: 4 Average loss: 148.4783\n",
      "====> Test set loss: 143.9596\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 139.191849\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 149.882553\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 140.118607\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 138.556824\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 154.083878\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 146.216278\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 137.339981\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 144.108063\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 139.482361\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 143.931900\n",
      "====> Epoch: 5 Average loss: 142.1322\n",
      "====> Test set loss: 138.8216\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 151.642838\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 131.734528\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 133.904327\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 140.747986\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 128.474487\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 139.707153\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 141.854370\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 137.756470\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 140.083893\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 137.129715\n",
      "====> Epoch: 6 Average loss: 138.1818\n",
      "====> Test set loss: 135.8855\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 130.550369\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 138.312393\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 138.987396\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 141.047760\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 135.881714\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 134.615967\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 134.749573\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 136.886627\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 137.498795\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 134.682663\n",
      "====> Epoch: 7 Average loss: 135.2588\n",
      "====> Test set loss: 132.6710\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 135.201477\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 133.561172\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 135.837341\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 127.443993\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 149.436890\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 127.276909\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 132.494019\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 124.355194\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 132.380905\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 129.051849\n",
      "====> Epoch: 8 Average loss: 132.4307\n",
      "====> Test set loss: 130.4301\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 135.449783\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 132.475952\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 124.840340\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 128.745987\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 128.822067\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 129.820175\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 127.164856\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 128.497772\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 138.443161\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 131.471375\n",
      "====> Epoch: 9 Average loss: 130.4799\n",
      "====> Test set loss: 128.6188\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 132.534622\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 131.933365\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 127.856873\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 120.618500\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 124.745110\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 129.273407\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 133.005569\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 137.236542\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 128.145538\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 122.852188\n",
      "====> Epoch: 10 Average loss: 128.7326\n",
      "====> Test set loss: 126.8615\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 116.701942\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 128.999435\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 119.739532\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 122.947220\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 133.960541\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 126.115204\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 128.682922\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 135.170425\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 122.942856\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 134.674683\n",
      "====> Epoch: 11 Average loss: 126.9505\n",
      "====> Test set loss: 125.0430\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 131.863327\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 121.076378\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 120.800667\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 124.694550\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 122.957253\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 127.487610\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 126.272522\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 124.189552\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 124.408249\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 130.646927\n",
      "====> Epoch: 12 Average loss: 124.7795\n",
      "====> Test set loss: 122.4927\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 127.590637\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 117.187584\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 119.878021\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 127.744469\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 111.181717\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 120.403687\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 120.529480\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 123.551620\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 123.896400\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 118.136185\n",
      "====> Epoch: 13 Average loss: 122.4313\n",
      "====> Test set loss: 120.4769\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 128.922272\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 120.125900\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 121.294693\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 120.895287\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 105.744728\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 119.570885\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 123.379318\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 122.005646\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 115.273338\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 116.146538\n",
      "====> Epoch: 14 Average loss: 120.7439\n",
      "====> Test set loss: 119.0221\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 117.319824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 120.001701\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 119.543190\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 114.506714\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 124.427155\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 122.071037\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 115.796143\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 121.298340\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 128.231949\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 120.705811\n",
      "====> Epoch: 15 Average loss: 119.3840\n",
      "====> Test set loss: 117.9141\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 115.340256\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 119.855743\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 121.681366\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 113.947479\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 118.920601\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 131.010513\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 118.651962\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 118.206787\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 119.673462\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 117.483879\n",
      "====> Epoch: 16 Average loss: 118.2819\n",
      "====> Test set loss: 116.9377\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 128.156479\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 115.837463\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 115.944321\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 122.185318\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 119.831635\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 118.663315\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 121.332977\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 118.988724\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 121.204277\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 113.554825\n",
      "====> Epoch: 17 Average loss: 117.2931\n",
      "====> Test set loss: 115.9811\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 114.096313\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 113.213737\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 116.511551\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 106.790428\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 115.222641\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 116.676697\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 120.904495\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 119.428848\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 114.835480\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 126.153816\n",
      "====> Epoch: 18 Average loss: 116.5124\n",
      "====> Test set loss: 115.2760\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 117.134308\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 113.413094\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 115.070145\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 110.728569\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 109.675705\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 113.087624\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 105.557137\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 112.868820\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 115.066940\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 118.338768\n",
      "====> Epoch: 19 Average loss: 115.7937\n",
      "====> Test set loss: 114.5800\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 114.543533\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 109.615524\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 112.599915\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 115.115356\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 115.503662\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 118.712936\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 111.627563\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 107.335579\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 110.460495\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 112.163818\n",
      "====> Epoch: 20 Average loss: 115.0716\n",
      "====> Test set loss: 113.8397\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_clip_weight(model_l1_clip, epoch)\n",
    "        test(model_l1_clip, epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       '../data/first_test_results_l1_norm_extra_layer/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_l1_clip.state_dict(), BASE_PATH_DATA + \"../data/models/first_try/with_norm_extra_layer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_norm = np.linalg.norm(model_l1_clip.l1_clip_layer.weight.detach().clone().numpy(), ord=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999976"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_layer = model_l1_clip.l1_clip_layer.weight.detach().clone().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_selection_layer = np.abs(selection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_norm_each_column = np.apply_along_axis(sum, 0, abs_selection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99958752, 0.99114071, 0.99352123, 0.99343291, 0.99315413,\n",
       "       0.99029483, 0.99244394, 0.99312003, 0.99902319, 0.99087082,\n",
       "       0.995436  , 0.99610161, 0.99089983, 0.99401757, 0.99071468,\n",
       "       0.99165669, 0.99006348, 0.9928895 , 1.00000042])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm_each_column[l1_norm_each_column > 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96363839, 0.94277012, 0.92898385, 0.94791537, 0.97497246,\n",
       "       0.94601693, 0.9271083 , 0.98082783, 0.9483509 , 0.95613437,\n",
       "       0.99958752, 0.9690532 , 0.91838443, 0.99114071, 0.93684556,\n",
       "       0.9661863 , 0.92152835, 0.93072117, 0.96490775, 0.91512867,\n",
       "       0.93719633, 0.94837722, 0.92129787, 0.93845987, 0.91059505,\n",
       "       0.94099315, 0.94274611, 0.91532886, 0.95414467, 0.96288287,\n",
       "       0.9400504 , 0.9478958 , 0.92025725, 0.9422504 , 0.95795396,\n",
       "       0.94663274, 0.93096003, 0.98378093, 0.92258021, 0.96402687,\n",
       "       0.94444344, 0.93993438, 0.95652801, 0.89961878, 0.95106552,\n",
       "       0.9334911 , 0.94041348, 0.97387813, 0.94015661, 0.97620439,\n",
       "       0.93559095, 0.92863166, 0.91972933, 0.94992082, 0.96025307,\n",
       "       0.9391986 , 0.94952587, 0.95019449, 0.96257972, 0.95784391,\n",
       "       0.93014431, 0.94558953, 0.94124681, 0.91651581, 0.95430474,\n",
       "       0.92792337, 0.94036961, 0.97166686, 0.90676528, 0.9564508 ,\n",
       "       0.95715016, 0.92576259, 0.95855679, 0.94216327, 0.94009878,\n",
       "       0.94637631, 0.9469678 , 0.95673599, 0.93641037, 0.93492224,\n",
       "       0.93403628, 0.92665672, 0.93888618, 0.96319684, 0.93136302,\n",
       "       0.93652514, 0.9509997 , 0.94182467, 0.97797296, 0.92796795,\n",
       "       0.94203041, 0.93780908, 0.98089143, 0.94610914, 0.99352123,\n",
       "       0.9457036 , 0.94386748, 0.95110713, 0.97557175, 0.9442172 ,\n",
       "       0.94503561, 0.93519003, 0.93440694, 0.94401213, 0.95910261,\n",
       "       0.96568074, 0.93897875, 0.96971077, 0.94875712, 0.95781234,\n",
       "       0.93613077, 0.94601643, 0.93480106, 0.96191075, 0.97005594,\n",
       "       0.96450747, 0.92769136, 0.9664039 , 0.94405165, 0.9722831 ,\n",
       "       0.96149869, 0.9313844 , 0.96315258, 0.9587985 , 0.92465485,\n",
       "       0.93695701, 0.97058938, 0.97596853, 0.97225808, 0.98528645,\n",
       "       0.9675537 , 0.9598856 , 0.94424072, 0.98152438, 0.96732316,\n",
       "       0.94623566, 0.95497011, 0.94699215, 0.96205393, 0.94437812,\n",
       "       0.95591055, 0.94214603, 0.93281829, 0.93413567, 0.9612538 ,\n",
       "       0.95118415, 0.94561355, 0.95329456, 0.95453768, 0.9675627 ,\n",
       "       0.94546176, 0.98972558, 0.97481832, 0.95240217, 0.951793  ,\n",
       "       0.960673  , 0.96253871, 0.94455469, 0.96049779, 0.96347594,\n",
       "       0.95026432, 0.93683571, 0.98554884, 0.9325197 , 0.93402659,\n",
       "       0.92983173, 0.93326799, 0.95178235, 0.99343291, 0.93702487,\n",
       "       0.93733006, 0.93972498, 0.95541652, 0.98381825, 0.92897637,\n",
       "       0.97559884, 0.92466111, 0.92694055, 0.93801708, 0.95717769,\n",
       "       0.9246783 , 0.95378369, 0.9651317 , 0.96902367, 0.93864315,\n",
       "       0.93778107, 0.96041676, 0.92846263, 0.92579524, 0.97517837,\n",
       "       0.96242475, 0.95009241, 0.94356598, 0.94914361, 0.96145548,\n",
       "       0.92882255, 0.9419433 , 0.98452387, 0.94998935, 0.9653184 ,\n",
       "       0.94366213, 0.96703381, 0.97838017, 0.93441552, 0.95271443,\n",
       "       0.9463391 , 0.95436305, 0.94412857, 0.92349578, 0.95644153,\n",
       "       0.96350169, 0.99315413, 0.96451864, 0.98115576, 0.94364549,\n",
       "       0.95209647, 0.97979281, 0.97031919, 0.95728762, 0.99029483,\n",
       "       0.96601832, 0.91858067, 0.91079919, 0.98424056, 0.94768075,\n",
       "       0.9529459 , 0.9357721 , 0.93507626, 0.95265933, 0.94185838,\n",
       "       0.99244394, 0.94839859, 0.94639455, 0.93911471, 0.96935804,\n",
       "       0.94365851, 0.96962461, 0.94115563, 0.94184867, 0.92718385,\n",
       "       0.94300776, 0.92673171, 0.99312003, 0.98557768, 0.96074845,\n",
       "       0.97286354, 0.96431802, 0.96987798, 0.95609955, 0.93424123,\n",
       "       0.91359025, 0.92506398, 0.94662023, 0.93526705, 0.96274788,\n",
       "       0.91630422, 0.93430856, 0.95808435, 0.93562751, 0.98001367,\n",
       "       0.94099258, 0.94547475, 0.97946283, 0.93579975, 0.91240462,\n",
       "       0.97166151, 0.97195936, 0.94279745, 0.95282548, 0.99902319,\n",
       "       0.95595498, 0.93262388, 0.94397292, 0.92062197, 0.9108129 ,\n",
       "       0.93187539, 0.97328233, 0.93362774, 0.97624707, 0.95843767,\n",
       "       0.93067146, 0.90534492, 0.95280408, 0.97079037, 0.95114685,\n",
       "       0.95678609, 0.95809906, 0.94014269, 0.93678997, 0.94029486,\n",
       "       0.90271688, 0.95956572, 0.93622139, 0.95926071, 0.95972301,\n",
       "       0.9514304 , 0.98642911, 0.95840445, 0.98826282, 0.98533854,\n",
       "       0.92654611, 0.93661856, 0.96756508, 0.9481295 , 0.96016213,\n",
       "       0.97723334, 0.94150029, 0.95801235, 0.93342111, 0.94399397,\n",
       "       0.93551977, 0.95166344, 0.94788785, 0.97904132, 0.94583673,\n",
       "       0.95526431, 0.97182784, 0.94849161, 0.97597596, 0.93626387,\n",
       "       0.91443719, 0.96124432, 0.94067343, 0.95635398, 0.98577391,\n",
       "       0.94817817, 0.9469696 , 0.93872671, 0.97658297, 0.96586392,\n",
       "       0.96597653, 0.95440515, 0.93448377, 0.92573226, 0.94926995,\n",
       "       0.95044236, 0.93755019, 0.97161399, 0.94434233, 0.95857301,\n",
       "       0.9630196 , 0.97331724, 0.97491192, 0.99087082, 0.95570853,\n",
       "       0.95730476, 0.94398003, 0.94465544, 0.94762577, 0.9331479 ,\n",
       "       0.93893642, 0.95595818, 0.93072327, 0.995436  , 0.94549405,\n",
       "       0.92687391, 0.99610161, 0.94299706, 0.95978729, 0.95988498,\n",
       "       0.94200903, 0.96422721, 0.93469739, 0.93957548, 0.95928486,\n",
       "       0.9632479 , 0.9538976 , 0.91869267, 0.93581454, 0.93417171,\n",
       "       0.97822498, 0.97661313, 0.92849507, 0.95956528, 0.96712677,\n",
       "       0.95127199, 0.93708826, 0.95235576, 0.95730282, 0.9326872 ,\n",
       "       0.99089983, 0.93863147, 0.968029  , 0.96857994, 0.96705993,\n",
       "       0.95389385, 0.9450133 , 0.95447339, 0.95438126, 0.91940386,\n",
       "       0.90966162, 0.94348466, 0.92275223, 0.90171946, 0.90647481,\n",
       "       0.94184621, 0.98477175, 0.9267394 , 0.96734286, 0.95246749,\n",
       "       0.95867131, 0.94638889, 0.94204626, 0.96430417, 0.93842023,\n",
       "       0.96693045, 0.95714751, 0.9586182 , 0.95901045, 0.94588453,\n",
       "       0.96209591, 0.9240604 , 0.9845667 , 0.94307736, 0.94982789,\n",
       "       0.96934323, 0.9501507 , 0.9349611 , 0.93074679, 0.94230492,\n",
       "       0.96960566, 0.94593502, 0.954377  , 0.92745891, 0.94206392,\n",
       "       0.94927193, 0.95104144, 0.93431087, 0.94791159, 0.97904052,\n",
       "       0.98738997, 0.93962809, 0.95296303, 0.97745088, 0.93635775,\n",
       "       0.95413472, 0.96508071, 0.92101738, 0.96951927, 0.94592667,\n",
       "       0.9408304 , 0.91309112, 0.97860515, 0.93762493, 0.97790812,\n",
       "       0.94667213, 0.9754113 , 0.97589184, 0.97840404, 0.95669103,\n",
       "       0.91112004, 0.93489815, 0.93828331, 0.93347035, 0.94475043,\n",
       "       0.93239355, 0.95962729, 0.93192006, 0.93537652, 0.92583644,\n",
       "       0.92427007, 0.96705636, 0.94723232, 0.94357918, 0.95989018,\n",
       "       0.90312865, 0.94161401, 0.94485757, 0.93033874, 0.93711965,\n",
       "       0.96936449, 0.96181838, 0.97716073, 0.95106625, 0.97113466,\n",
       "       0.95978946, 0.9611339 , 0.96628173, 0.95721675, 0.95922726,\n",
       "       0.9169095 , 0.92102225, 0.98203759, 0.97479056, 0.93540042,\n",
       "       0.9612215 , 0.97329788, 0.9548623 , 0.95290258, 0.95596968,\n",
       "       0.96498639, 0.96791699, 0.95198962, 0.95588709, 0.97753662,\n",
       "       0.97609276, 0.93225307, 0.94001549, 0.93068605, 0.91285656,\n",
       "       0.97416894, 0.94338593, 0.93439692, 0.93481789, 0.9300595 ,\n",
       "       0.96982807, 0.93989922, 0.97229149, 0.95740849, 0.9413761 ,\n",
       "       0.90984807, 0.97518762, 0.95196575, 0.94237995, 0.94748418,\n",
       "       0.93289331, 0.95009402, 0.99401757, 0.97926135, 0.95245075,\n",
       "       0.91441228, 0.94123914, 0.9693985 , 0.94327262, 0.97382437,\n",
       "       0.97834481, 0.94670388, 0.9244527 , 0.91955403, 0.94912298,\n",
       "       0.94744572, 0.91747297, 0.93276319, 0.95410653, 0.94128891,\n",
       "       0.93815126, 0.9561858 , 0.94238535, 0.96728329, 0.93072368,\n",
       "       0.93910314, 0.92548704, 0.92546092, 0.97828766, 0.93692918,\n",
       "       0.93129553, 0.94497219, 0.94679357, 0.96977351, 0.97370199,\n",
       "       0.94488621, 0.93721304, 0.9464508 , 0.91997195, 0.9231786 ,\n",
       "       0.9733667 , 0.97411078, 0.94138614, 0.92075151, 0.96473493,\n",
       "       0.94218227, 0.95077863, 0.96731409, 0.95326121, 0.92444416,\n",
       "       0.94944717, 0.97327005, 0.93655185, 0.95528077, 0.95149335,\n",
       "       0.92529116, 0.93629014, 0.92199314, 0.92735852, 0.95217624,\n",
       "       0.93713839, 0.95280578, 0.94150913, 0.93177016, 0.95502108,\n",
       "       0.9851078 , 0.93946523, 0.97696857, 0.92085366, 0.95623597,\n",
       "       0.95270493, 0.92557103, 0.96640208, 0.96580864, 0.95010436,\n",
       "       0.92214257, 0.94272771, 0.96342354, 0.91125573, 0.93959644,\n",
       "       0.95465309, 0.93104508, 0.93699035, 0.94538779, 0.95981102,\n",
       "       0.91025107, 0.90757016, 0.94705931, 0.93253329, 0.95162461,\n",
       "       0.95092203, 0.99071468, 0.94341465, 0.95581646, 0.97385439,\n",
       "       0.9618432 , 0.9567265 , 0.94753001, 0.95229085, 0.97579103,\n",
       "       0.94525581, 0.94441148, 0.99165669, 0.95041628, 0.93145299,\n",
       "       0.9496449 , 0.93571715, 0.96618953, 0.96813523, 0.94242286,\n",
       "       0.95726515, 0.96424022, 0.96071285, 0.91979416, 0.94705315,\n",
       "       0.96283549, 0.94945074, 0.96838151, 0.95461411, 0.97392909,\n",
       "       0.96639152, 0.98469498, 0.99006348, 0.96869085, 0.95698024,\n",
       "       0.93459386, 0.92609518, 0.95707201, 0.95935763, 0.96079491,\n",
       "       0.93746687, 0.9928895 , 0.97603049, 0.96391989, 0.94125109,\n",
       "       0.95101526, 0.91765924, 0.95969649, 0.96625521, 0.95487381,\n",
       "       0.91238167, 0.94866781, 0.94361695, 0.95800404, 0.97540706,\n",
       "       0.98429661, 0.94291337, 0.96879371, 0.91136595, 0.9441756 ,\n",
       "       0.95862711, 0.96236286, 0.93560694, 0.95172702, 0.92127781,\n",
       "       0.92851357, 0.95095268, 1.00000042, 0.95324978, 0.94715518,\n",
       "       0.94115381, 0.89641115, 0.91725954, 0.92341999, 0.94291606,\n",
       "       0.96302631, 0.9487075 , 0.92615041, 0.9493326 , 0.93818257,\n",
       "       0.94492962, 0.95796882, 0.96554429, 0.96163717, 0.92510336,\n",
       "       0.94371367, 0.97772358, 0.96058987, 0.92156298, 0.96555591,\n",
       "       0.93799836, 0.90816537, 0.97017086, 0.92457828, 0.91088158,\n",
       "       0.95535539, 0.94296808, 0.93434045, 0.90595064, 0.94066645,\n",
       "       0.95835913, 0.90944818, 0.94688158, 0.94095408, 0.95461687,\n",
       "       0.94056851, 0.96560672, 0.93599414, 0.92521768, 0.97514729,\n",
       "       0.94734676, 0.94764486, 0.94098668, 0.92464147, 0.94325746,\n",
       "       0.9584113 , 0.90278453, 0.97231617, 0.91656117, 0.9131924 ,\n",
       "       0.93919564, 0.9277149 , 0.94302937, 0.95808241, 0.95225997,\n",
       "       0.89852844, 0.94589144, 0.95402511, 0.94874955, 0.9454563 ,\n",
       "       0.92746706, 0.94004618, 0.94098528, 0.93637872, 0.95364942,\n",
       "       0.96959227, 0.96950473, 0.93655117, 0.91214774, 0.94178309,\n",
       "       0.96055575, 0.93504103, 0.92028667, 0.93491889, 0.91500268,\n",
       "       0.96556875, 0.9398441 , 0.93976029, 0.92870294, 0.90133033,\n",
       "       0.9401805 , 0.95240355, 0.91870029, 0.90976951, 0.94651673,\n",
       "       0.94403936, 0.97468957, 0.92229501, 0.921176  , 0.97768803,\n",
       "       0.94425774, 0.92168246, 0.92670635, 0.92026707, 0.9270396 ,\n",
       "       0.94491433, 0.92807209, 0.95047229, 0.93387237, 0.98828693,\n",
       "       0.93130865, 0.933731  , 0.96487182, 0.94402177, 0.95302655,\n",
       "       0.91494988, 0.92991103, 0.93324758, 0.91530883])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm_each_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Norm with no extra layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clip_weight(model, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        l1_norm = np.linalg.norm(model.fc1.weight.detach().clone().numpy(), ord=1)\n",
    "        model.fc1.weight = torch.nn.Parameter(model.fc1.weight / l1_norm)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "            \n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1_clip = VAE(400, 20).to(device)\n",
    "\n",
    "l1_norm = np.linalg.norm(model_l1_clip.fc1.weight.detach().clone().numpy(), ord=1)\n",
    "model_l1_clip.fc1.weight = torch.nn.Parameter(model_l1_clip.fc1.weight / l1_norm)\n",
    "    \n",
    "optimizer = torch.optim.Adam(model_l1_clip.parameters(), lr=lr, betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         '../data/first_test_results_l1_norm_no_extra_layer/reconstruction_' + \n",
    "                           str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.048279\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 253.677917\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 225.875015\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 215.630554\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 210.943649\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 206.674820\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 210.875839\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 219.844879\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 204.992157\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 201.706650\n",
      "====> Epoch: 1 Average loss: 230.5469\n",
      "====> Test set loss: 207.5712\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 206.770554\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 211.870331\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 208.397110\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 211.528412\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 198.429214\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 193.665405\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 211.422195\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 194.758896\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 188.214493\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 204.014786\n",
      "====> Epoch: 2 Average loss: 201.4883\n",
      "====> Test set loss: 195.4437\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 195.809555\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 192.054260\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 185.290848\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 181.793289\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 190.333359\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 197.330505\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 188.468582\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 191.924820\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 183.435593\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 184.664215\n",
      "====> Epoch: 3 Average loss: 188.8083\n",
      "====> Test set loss: 179.8177\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 180.119934\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 181.460922\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 179.766449\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 172.677231\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 179.511520\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 177.593140\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 168.372177\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 175.947906\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 164.659256\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 165.136017\n",
      "====> Epoch: 4 Average loss: 173.7922\n",
      "====> Test set loss: 165.8813\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 167.826035\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 154.197266\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 173.763306\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 162.490524\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 164.692795\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 154.758728\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 158.422714\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 159.406311\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 164.576065\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 153.830994\n",
      "====> Epoch: 5 Average loss: 161.9046\n",
      "====> Test set loss: 157.1648\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 157.960876\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 161.522247\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 162.586899\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 164.390503\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 150.790726\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 153.725113\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 155.807190\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 158.416550\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 154.343567\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 151.263428\n",
      "====> Epoch: 6 Average loss: 155.5379\n",
      "====> Test set loss: 152.2724\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 151.376541\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 147.070007\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 156.405807\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 160.756912\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 148.758728\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 155.308670\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 144.816605\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 151.308960\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 147.035126\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 142.894852\n",
      "====> Epoch: 7 Average loss: 151.2295\n",
      "====> Test set loss: 148.2155\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 152.433167\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 146.618622\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 147.470978\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 141.536865\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 152.524124\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 146.860336\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 146.269043\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 147.458755\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 146.433548\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 144.402908\n",
      "====> Epoch: 8 Average loss: 147.6519\n",
      "====> Test set loss: 144.9476\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 150.134460\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 149.254166\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 150.192596\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 144.920807\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 147.901443\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 135.447815\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 147.079666\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 136.441055\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 144.542206\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 146.361481\n",
      "====> Epoch: 9 Average loss: 144.5562\n",
      "====> Test set loss: 142.0700\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 144.674026\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 139.672150\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 146.381439\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 139.192932\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 137.597046\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 142.386810\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 148.780167\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 134.153900\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 142.891891\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 149.206467\n",
      "====> Epoch: 10 Average loss: 141.8306\n",
      "====> Test set loss: 139.5829\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 140.155350\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 139.947311\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 145.014511\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 140.341034\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 130.401031\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 136.444641\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 131.501266\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 136.448532\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 137.259125\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 146.790131\n",
      "====> Epoch: 11 Average loss: 139.5201\n",
      "====> Test set loss: 137.3008\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 146.096146\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 140.974716\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 141.252716\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 141.472305\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 142.348679\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 135.495148\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 146.242935\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 141.048965\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 133.164429\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 127.083755\n",
      "====> Epoch: 12 Average loss: 137.5511\n",
      "====> Test set loss: 135.4000\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 139.667130\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 132.563751\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 138.501663\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 137.456985\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 127.689072\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 138.311432\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 143.625229\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 135.871399\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 137.912552\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 134.619049\n",
      "====> Epoch: 13 Average loss: 135.8242\n",
      "====> Test set loss: 133.7509\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 132.980057\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 137.363220\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 133.592178\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 135.989487\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 129.080246\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 123.969849\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 132.552734\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 134.187668\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 136.262283\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 139.713882\n",
      "====> Epoch: 14 Average loss: 134.2678\n",
      "====> Test set loss: 132.2419\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 139.354279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 126.751114\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 137.441971\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 136.163879\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 131.694992\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 133.560043\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 127.935623\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 132.007294\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 128.458023\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 133.797791\n",
      "====> Epoch: 15 Average loss: 132.8495\n",
      "====> Test set loss: 130.9870\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 127.130035\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 128.866852\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 134.257416\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 127.036148\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 139.577225\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 143.420105\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 131.121948\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 139.882004\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 132.061798\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 133.032471\n",
      "====> Epoch: 16 Average loss: 131.6381\n",
      "====> Test set loss: 129.6847\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 125.020531\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 131.059387\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 132.753281\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 131.319366\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 140.917816\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 125.726494\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 125.764847\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 127.012657\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 134.654007\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 131.929916\n",
      "====> Epoch: 17 Average loss: 130.5219\n",
      "====> Test set loss: 128.8277\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 134.281586\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 122.623016\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 122.981270\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 127.150040\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 145.763336\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 126.969696\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 133.934540\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 131.585342\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 128.248352\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 129.202652\n",
      "====> Epoch: 18 Average loss: 129.5514\n",
      "====> Test set loss: 127.8032\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 129.827301\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 125.349388\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 124.474915\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 124.478584\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 126.054779\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 117.970894\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 135.617783\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 135.475159\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 126.778900\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 128.214523\n",
      "====> Epoch: 19 Average loss: 128.6319\n",
      "====> Test set loss: 127.0045\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 134.595734\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 131.370834\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 129.815842\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 125.006958\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 127.959251\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 121.145523\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 134.724152\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 129.142365\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 126.047958\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 120.753082\n",
      "====> Epoch: 20 Average loss: 127.8516\n",
      "====> Test set loss: 126.1386\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_clip_weight(model_l1_clip, epoch)\n",
    "        test(model_l1_clip, epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       '../data/first_test_results_l1_norm_no_extra_layer/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_l1_clip.state_dict(), BASE_PATH_DATA + \"../data/models/first_try/with_norm_no_extra_layer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_layer = model_l1_clip.fc1.weight.detach().clone().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_selection_layer = np.abs(selection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_norm_each_column = np.apply_along_axis(sum, 0, abs_selection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88263911, 0.9069716 , 0.94035317, 0.95554182, 0.92194361,\n",
       "       0.92753977, 0.91475894, 0.93628588, 0.93750909, 0.90349619,\n",
       "       0.90528965, 0.98850023, 0.89126152, 0.88007821, 0.85963397,\n",
       "       0.89675208, 0.86876468, 0.90588691, 0.91891894, 0.9707311 ,\n",
       "       0.92667354, 0.90813464, 0.93753976, 0.91275767, 0.90759018,\n",
       "       0.94160638, 0.88998179, 0.92786227, 0.93633731, 0.89500375,\n",
       "       0.91942608, 0.84941216, 0.93089064, 0.91893064, 0.93966949,\n",
       "       0.85505713, 0.90391677, 0.90449732, 0.92599324, 0.89539913,\n",
       "       0.93892315, 0.89727228, 0.93386219, 0.9375714 , 0.92146765,\n",
       "       0.93503228, 0.88995671, 0.89624714, 0.93806094, 0.93276455,\n",
       "       0.89602345, 0.91047644, 0.90502183, 0.89641111, 0.9100012 ,\n",
       "       0.91426379, 0.87865201, 0.88050088, 0.8888116 , 0.91856039,\n",
       "       0.91922711, 0.92111592, 0.88764808, 0.88302815, 0.92497973,\n",
       "       0.89357756, 0.8756083 , 0.90828206, 0.90160484, 0.90677957,\n",
       "       0.89944031, 0.87905926, 0.88349854, 0.90633766, 0.87332242,\n",
       "       0.90656184, 0.90904809, 0.91964298, 0.90107433, 0.92143824,\n",
       "       0.90851088, 0.94652752, 0.86484103, 0.89512424, 0.92901317,\n",
       "       0.91001644, 0.89759746, 0.93029911, 0.88589289, 0.8858436 ,\n",
       "       0.92037994, 0.88139116, 0.93830163, 0.91272749, 0.95995131,\n",
       "       0.89379897, 0.87038343, 0.91944701, 0.91200964, 0.88924627,\n",
       "       0.93282521, 0.94198958, 0.88594906, 0.90226944, 0.89154477,\n",
       "       0.91982432, 0.90129052, 0.91054661, 0.86071749, 0.91800069,\n",
       "       0.90641397, 0.85971067, 0.9478723 , 0.90743148, 0.89189252,\n",
       "       0.92220756, 0.89293701, 0.89936817, 0.92884824, 0.94928054,\n",
       "       0.93033962, 0.87929217, 0.9559727 , 0.86412526, 0.92357862,\n",
       "       0.88243238, 0.95968303, 0.90816779, 0.88709338, 0.93465149,\n",
       "       0.92429402, 0.894452  , 0.89652946, 0.91216003, 1.00000003,\n",
       "       0.92321084, 0.91476734, 0.89296621, 0.87404678, 0.90287634,\n",
       "       0.93925868, 0.91720997, 0.9119799 , 0.89497519, 0.89453334,\n",
       "       0.94420881, 0.86718815, 0.90491405, 0.9149913 , 0.93548819,\n",
       "       0.88596786, 0.87576613, 0.9030507 , 0.92258674, 0.90904416,\n",
       "       0.91731864, 0.90158434, 0.88891646, 0.9130738 , 0.88477947,\n",
       "       0.93392691, 0.87293548, 0.91706205, 0.90192115, 0.92604113,\n",
       "       0.90902297, 0.91799302, 0.93799205, 0.88902214, 0.87756977,\n",
       "       0.90827899, 0.86882236, 0.94578243, 0.95709942, 0.9513672 ,\n",
       "       0.91463921, 0.91694346, 0.91068159, 0.88954703, 0.89242287,\n",
       "       0.94217213, 0.88204856, 0.90876571, 0.9413195 , 0.91129794,\n",
       "       0.91260031, 0.90705422, 0.93842196, 0.91418817, 0.89399992,\n",
       "       0.87507062, 0.84881995, 0.94704658, 0.95703113, 0.92185561,\n",
       "       0.88646944, 0.92993204, 0.91609404, 0.89596781, 0.86498528,\n",
       "       0.84601139, 0.88302975, 0.85349833, 0.8880597 , 0.90534335,\n",
       "       0.92205821, 0.93805317, 0.91399803, 0.91987698, 0.92224428,\n",
       "       0.86102322, 0.86511755, 0.9132727 , 0.90037646, 0.89225539,\n",
       "       0.88229382, 0.91912449, 0.88938602, 0.96058527, 0.88484107,\n",
       "       0.96256926, 0.8752015 , 0.86984578, 0.89589137, 0.87989473,\n",
       "       0.89924697, 0.91740915, 0.8851614 , 0.8859259 , 0.8744054 ,\n",
       "       0.94793033, 0.9243924 , 0.86664244, 0.93286066, 0.83560964,\n",
       "       0.93062036, 0.89174467, 0.87085599, 0.87629171, 0.8639109 ,\n",
       "       0.87718573, 0.88924281, 0.90778126, 0.84836172, 0.90159323,\n",
       "       0.89158198, 0.87946102, 0.92666433, 0.92939044, 0.90515717,\n",
       "       0.91458298, 0.86909465, 0.8878354 , 0.89665257, 0.86421536,\n",
       "       0.92595944, 0.94049868, 0.88618123, 0.90854361, 0.91128127,\n",
       "       0.92599185, 0.94506825, 0.90968328, 0.87113773, 0.92632827,\n",
       "       0.87975907, 0.92924582, 0.89291322, 0.91795255, 0.95816812,\n",
       "       0.90253911, 0.92873951, 0.89794539, 0.87770426, 0.90852599,\n",
       "       0.90220817, 0.94129905, 0.9144016 , 0.89342632, 0.90305811,\n",
       "       0.92350664, 0.90095411, 0.89685378, 0.93731075, 0.86516256,\n",
       "       0.86511609, 0.89975428, 0.94637659, 0.93148053, 0.89023151,\n",
       "       0.92562822, 0.92429136, 0.91417542, 0.8672798 , 0.92813726,\n",
       "       0.93940615, 0.94631463, 0.93502421, 0.83754201, 0.87013272,\n",
       "       0.89888137, 0.91664803, 0.88460656, 0.91501742, 0.91395848,\n",
       "       0.8824591 , 0.90949153, 0.92489086, 0.91470157, 0.90069357,\n",
       "       0.93884084, 0.87529564, 0.91814065, 0.92620516, 0.91065508,\n",
       "       0.86882587, 0.92301748, 0.91773843, 0.92470041, 0.89965292,\n",
       "       0.9077952 , 0.95048575, 0.90443054, 0.91633888, 0.86788473,\n",
       "       0.91952658, 0.95940773, 0.89754654, 0.92399235, 0.87054758,\n",
       "       0.91963207, 0.91629254, 0.91135258, 0.89457429, 0.89484995,\n",
       "       0.9150784 , 0.96081365, 0.89797818, 0.97792467, 0.89351246,\n",
       "       0.86061698, 0.91526706, 0.92777785, 0.95427891, 0.96667993,\n",
       "       0.87101109, 0.8904307 , 0.87922957, 0.92914375, 0.93706795,\n",
       "       0.87476747, 0.89812816, 0.93087641, 0.82651629, 0.93049396,\n",
       "       0.91226067, 0.92527631, 0.95508883, 0.89668502, 0.95496577,\n",
       "       0.88513705, 0.92034072, 0.92180769, 0.8852695 , 0.88643532,\n",
       "       0.90119094, 0.91573772, 0.93759905, 0.87484648, 0.87350466,\n",
       "       0.89721355, 0.87195211, 0.89082159, 0.87679961, 0.88157932,\n",
       "       0.91481648, 0.88092903, 0.90862174, 0.90435617, 0.95748551,\n",
       "       0.91151083, 0.9099328 , 0.93187329, 0.92653411, 0.9042581 ,\n",
       "       0.93152023, 0.8818495 , 0.89028869, 0.91421793, 0.86338647,\n",
       "       0.86784766, 0.89386835, 0.91272979, 0.91128053, 0.93253604,\n",
       "       0.87957901, 0.89363266, 0.94797297, 0.86594232, 0.89963727,\n",
       "       0.88610496, 0.92546969, 0.90382838, 0.89541663, 0.860667  ,\n",
       "       0.89805698, 0.93456591, 0.88612221, 0.94791523, 0.89385748,\n",
       "       0.90317148, 0.92342962, 0.90390513, 0.94031376, 0.8837016 ,\n",
       "       0.93655824, 0.87464806, 0.90699026, 0.91459959, 0.88133579,\n",
       "       0.92103473, 0.95236432, 0.93900208, 0.89321956, 0.90178834,\n",
       "       0.88190649, 0.92797148, 0.9270103 , 0.88016243, 0.93859941,\n",
       "       0.92441928, 0.86155794, 0.86453311, 0.86810921, 0.87935282,\n",
       "       0.91766126, 0.90754472, 0.93108956, 0.96052061, 0.90491423,\n",
       "       0.89782663, 0.91814201, 0.92081863, 0.90568915, 0.90193584,\n",
       "       0.92118972, 0.91456696, 0.89435305, 0.8938678 , 0.91186763,\n",
       "       0.88829301, 0.90016563, 0.92810641, 0.90348366, 0.89589058,\n",
       "       0.93151428, 0.88713626, 0.92720441, 0.89355712, 0.92369525,\n",
       "       0.94051002, 0.91679035, 0.89227905, 0.928944  , 0.93574715,\n",
       "       0.89777623, 0.92007711, 0.90399432, 0.94554414, 0.87247261,\n",
       "       0.93516236, 0.88004043, 0.8896773 , 0.88158147, 0.95162305,\n",
       "       0.9043471 , 0.90520371, 0.91317444, 0.91501475, 0.94778775,\n",
       "       0.89831872, 0.95097665, 0.86483223, 0.9530903 , 0.89095832,\n",
       "       0.86524662, 0.9383417 , 0.90913065, 0.88954977, 0.92261784,\n",
       "       0.91083135, 0.8768976 , 0.88128337, 0.92586204, 0.90866673,\n",
       "       0.91184465, 0.89488567, 0.92442012, 0.90916772, 0.89537034,\n",
       "       0.9207181 , 0.92061648, 0.97433538, 0.92081657, 0.93959983,\n",
       "       0.88508017, 0.91973038, 0.90014795, 0.88958475, 0.92260878,\n",
       "       0.93810289, 0.90935624, 0.89510409, 0.92381647, 0.90147823,\n",
       "       0.90690713, 0.88486208, 0.91319843, 0.90353175, 0.93148453,\n",
       "       0.93226929, 0.90697852, 0.91480986, 0.97454093, 0.9091381 ,\n",
       "       0.89219971, 0.91190886, 0.8603725 , 0.89410317, 0.88127148,\n",
       "       0.9121668 , 0.93983204, 0.87709977, 0.90379683, 0.91943535,\n",
       "       0.90258289, 0.85042242, 0.90953302, 0.89887558, 0.92620577,\n",
       "       0.89702101, 0.91471376, 0.8504631 , 0.85674835, 0.88036725,\n",
       "       0.90538578, 0.91651983, 0.92242742, 0.92643196, 0.90063649,\n",
       "       0.89733253, 0.90720483, 0.88912061, 0.85028177, 0.93373585,\n",
       "       0.94587251, 0.85059635, 0.88727281, 0.90867505, 0.96172907,\n",
       "       0.95185279, 0.86958276, 0.90161543, 0.94918734, 0.93162564,\n",
       "       0.93656966, 0.94260581, 0.87588844, 0.88362711, 0.90926408,\n",
       "       0.90555471, 0.91641996, 0.89741368, 0.92536843, 0.90047279,\n",
       "       0.89552796, 0.9221469 , 0.89695674, 0.90893052, 0.90875426,\n",
       "       0.89849327, 0.89020399, 0.8933687 , 0.93551597, 0.91742117,\n",
       "       0.91707314, 0.87303185, 0.93424128, 0.90116713, 0.92324132,\n",
       "       0.87585859, 0.90106255, 0.85591363, 0.8910619 , 0.9049343 ,\n",
       "       0.87964354, 0.9434287 , 0.89691675, 0.92498193, 0.85253141,\n",
       "       0.92467403, 0.9027941 , 0.93240967, 0.90867983, 0.94152839,\n",
       "       0.88454631, 0.90116349, 0.92008205, 0.93583626, 0.84863259,\n",
       "       0.95299254, 0.94525175, 0.91484033, 0.88528246, 0.91786691,\n",
       "       0.89794881, 0.87488978, 0.92826991, 0.89668785, 0.87198575,\n",
       "       0.90876599, 0.90949449, 0.9301699 , 0.9183416 , 0.9218606 ,\n",
       "       0.92263463, 0.88679287, 0.86668412, 0.96964191, 0.89720442,\n",
       "       0.91055392, 0.90970813, 0.8967216 , 0.90270401, 0.88169485,\n",
       "       0.92431526, 0.89143303, 0.92979575, 0.94071398, 0.91655531,\n",
       "       0.91489637, 0.91740729, 0.89220532, 0.90978955, 0.89756947,\n",
       "       0.93297678, 0.92119413, 0.93579837, 0.93124309, 0.93622865,\n",
       "       0.85976651, 0.91566649, 0.96470662, 0.89936882, 0.88218142,\n",
       "       0.93269571, 0.95115657, 0.90316789, 0.90505278, 0.87830833,\n",
       "       0.91871687, 0.89293919, 0.89691967, 0.87395164, 0.9038663 ,\n",
       "       0.9377035 , 0.88935766, 0.83112417, 0.85346486, 0.90459904,\n",
       "       0.86896339, 0.91998641, 0.91234298, 0.92495822, 0.94698893,\n",
       "       0.92321511, 0.87779822, 0.94657303, 0.91127621, 0.91632957,\n",
       "       0.97380834, 0.93284699, 0.88010773, 0.87386212, 0.91464072,\n",
       "       0.88983554, 0.89948095, 0.93982217, 0.87233579, 0.91403668,\n",
       "       0.90230088, 0.93145909, 0.86541296, 0.90619175, 0.86354631,\n",
       "       0.92043255, 0.88144354, 0.89286761, 0.93180509, 0.87874245,\n",
       "       0.88870326, 0.91201806, 0.90012103, 0.92441838, 0.86603987,\n",
       "       0.92342014, 0.91634979, 0.8951918 , 0.90529231, 0.9055383 ,\n",
       "       0.88955021, 0.90694675, 0.90791668, 0.91848719, 0.86808248,\n",
       "       0.90546669, 0.90952617, 0.90753026, 0.87894349, 0.91164842,\n",
       "       0.94362259, 0.91446355, 0.89751981, 0.93103994, 0.89429742,\n",
       "       0.91706095, 0.90091233, 0.88541524, 0.90805712, 0.88312141,\n",
       "       0.91990198, 0.90193553, 0.91249643, 0.88412918, 0.92091349,\n",
       "       0.88271377, 0.95027378, 0.94301338, 0.88371747, 0.8977707 ,\n",
       "       0.93767973, 0.92340937, 0.94869421, 0.88720101, 0.91974343,\n",
       "       0.90091953, 0.91494932, 0.90215534, 0.94120724, 0.91309214,\n",
       "       0.92398825, 0.88037041, 0.90815884, 0.84240747, 0.88301045,\n",
       "       0.91847392, 0.87563371, 0.89898429, 0.97413995, 0.9394449 ,\n",
       "       0.89166805, 0.92948771, 0.87477143, 0.92593219, 0.85829303,\n",
       "       0.89943919, 0.93006657, 0.88764874, 0.88357782, 0.909479  ,\n",
       "       0.92891761, 0.95215456, 0.91171456, 0.86616256, 0.87979502,\n",
       "       0.86217719, 0.89786668, 0.91046119, 0.92178779, 0.92850909,\n",
       "       0.90453001, 0.89309866, 0.87616418, 0.94221957])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm_each_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla VAE but with L1 Objective added to Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l1_added(model, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        l1_norm = torch.norm(model.fc1.weight, p=1)\n",
    "        loss += l1_norm\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         '../data/first_test_results_l1_on_loss/reconstruction_' + \n",
    "                           str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(400, 20).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 633.786499\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 270.558197\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 236.259842\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 204.894302\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 193.383209\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 184.348450\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 172.598236\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 178.443192\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 161.810699\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 166.633469\n",
      "====> Epoch: 1 Average loss: 211.5114\n",
      "====> Test set loss: 158.2329\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 168.217041\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 152.905609\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 159.634338\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 149.701324\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 153.751785\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 146.476013\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 154.076920\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 151.283020\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 150.659195\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 148.323425\n",
      "====> Epoch: 2 Average loss: 154.8013\n",
      "====> Test set loss: 141.4294\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 154.842209\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 144.991074\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 142.781311\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 139.753601\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 143.106613\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 138.776291\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 139.942825\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 142.446838\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 131.676804\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 139.230179\n",
      "====> Epoch: 3 Average loss: 143.3174\n",
      "====> Test set loss: 134.1606\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 142.105179\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 143.582275\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 126.815994\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 133.333496\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 133.036911\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 129.090042\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 129.280167\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 136.788223\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 136.895386\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 130.918930\n",
      "====> Epoch: 4 Average loss: 136.6901\n",
      "====> Test set loss: 128.0622\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 140.155258\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 134.508957\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 129.116516\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 132.514511\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 133.358963\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 129.953171\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 136.036682\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 130.587555\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 125.085045\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 132.255188\n",
      "====> Epoch: 5 Average loss: 131.8622\n",
      "====> Test set loss: 124.1682\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 131.236542\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 132.508118\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 133.342163\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 129.889084\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 127.802193\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 118.199539\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 135.776474\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 127.973633\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 118.900375\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 124.918777\n",
      "====> Epoch: 6 Average loss: 127.8753\n",
      "====> Test set loss: 120.3521\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 119.500458\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 126.126305\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 133.747726\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 127.370804\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 113.487053\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 124.550713\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 124.402924\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 121.032082\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 133.578659\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 117.455437\n",
      "====> Epoch: 7 Average loss: 125.0333\n",
      "====> Test set loss: 118.4305\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 122.617439\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 128.337219\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 124.936218\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 124.623245\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 119.908142\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 121.946297\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 130.708008\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 122.792297\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 122.031570\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 119.090858\n",
      "====> Epoch: 8 Average loss: 122.9984\n",
      "====> Test set loss: 116.8117\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 133.306732\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 119.821548\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 118.758034\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 123.761047\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 122.372223\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 115.446480\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 123.081963\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 120.686386\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 126.926430\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 119.543655\n",
      "====> Epoch: 9 Average loss: 121.3887\n",
      "====> Test set loss: 115.4459\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 123.256721\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 118.353531\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 125.025040\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 122.824402\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 117.563606\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 115.627426\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 119.148743\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 118.774918\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 113.980423\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 114.784645\n",
      "====> Epoch: 10 Average loss: 120.1074\n",
      "====> Test set loss: 114.3987\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 118.900215\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 127.836021\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 119.981148\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 122.888435\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 118.834641\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 123.897102\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 122.669998\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 126.556671\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 117.374245\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 118.156387\n",
      "====> Epoch: 11 Average loss: 119.0108\n",
      "====> Test set loss: 113.4193\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 113.668205\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 117.634071\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 119.073296\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 118.966629\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 120.514191\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 118.791870\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 116.171188\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 118.153938\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 114.034019\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 120.984398\n",
      "====> Epoch: 12 Average loss: 118.1483\n",
      "====> Test set loss: 112.9944\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 119.379951\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 116.242599\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 118.525444\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 117.789810\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 119.945206\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 114.395874\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 121.426750\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 118.244461\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 117.126015\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 111.773422\n",
      "====> Epoch: 13 Average loss: 117.3438\n",
      "====> Test set loss: 111.8650\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 119.985374\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 110.899300\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 120.257248\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 114.531029\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 115.069511\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 116.488022\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 116.608749\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 117.308044\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 110.019592\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 114.176056\n",
      "====> Epoch: 14 Average loss: 116.6297\n",
      "====> Test set loss: 111.2795\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 112.096024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 119.231621\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 115.029831\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 113.881554\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 114.487076\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 117.106445\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 115.038170\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 119.269638\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 117.066551\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 112.625824\n",
      "====> Epoch: 15 Average loss: 116.0230\n",
      "====> Test set loss: 110.9871\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 121.704185\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 115.088326\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 116.009041\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 120.474525\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 120.396629\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 106.680809\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 119.237419\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 120.691338\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 118.698914\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 115.816750\n",
      "====> Epoch: 16 Average loss: 115.4868\n",
      "====> Test set loss: 110.8380\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 114.668472\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 119.953323\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 113.239319\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 117.142670\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 115.804001\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 108.575760\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 111.266518\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 113.649742\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 118.659157\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 113.419891\n",
      "====> Epoch: 17 Average loss: 114.9906\n",
      "====> Test set loss: 110.1879\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 116.000320\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 113.797714\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 109.155762\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 113.707291\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 119.107262\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 120.593079\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 121.777367\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 111.988434\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 112.652786\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 118.618362\n",
      "====> Epoch: 18 Average loss: 114.5889\n",
      "====> Test set loss: 109.8019\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 113.464348\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 108.294281\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 113.989815\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 115.283958\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 111.982521\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 110.955055\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 110.775314\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 119.972778\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 115.292244\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 114.073914\n",
      "====> Epoch: 19 Average loss: 114.1786\n",
      "====> Test set loss: 109.5619\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 115.383324\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 110.121017\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 110.784302\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 112.791199\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 111.034897\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 116.501282\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 105.869560\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 121.441772\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 110.290993\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 118.241180\n",
      "====> Epoch: 20 Average loss: 113.8478\n",
      "====> Test set loss: 109.4761\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_l1_added(model, epoch)\n",
    "        test(model, epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       '../data/first_test_results_l1_on_loss/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), BASE_PATH_DATA + \"../data/models/first_try/no_extra_layer_l1_obj.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_layer = model.fc1.weight.detach().clone().numpy()\n",
    "abs_selection_layer = np.abs(selection_layer)\n",
    "l1_norm_each_column = np.apply_along_axis(sum, 0, abs_selection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02015531, 0.02073321, 0.0202737 , 0.02091513, 0.02082473,\n",
       "       0.02232557, 0.02086559, 0.02045751, 0.02010764, 0.02137731,\n",
       "       0.02129521, 0.02181275, 0.02128266, 0.02253216, 0.02104544,\n",
       "       0.01926024, 0.0207461 , 0.02033219, 0.02171256, 0.0196468 ,\n",
       "       0.02255907, 0.02064506, 0.02192439, 0.0202828 , 0.02202513,\n",
       "       0.02042202, 0.01988448, 0.02178324, 0.02123541, 0.02149822,\n",
       "       0.02211182, 0.0208573 , 0.01930092, 0.01954004, 0.02086005,\n",
       "       0.01902962, 0.02004075, 0.01979182, 0.01903752, 0.01946038,\n",
       "       0.04180263, 0.02985107, 0.02376955, 0.02823694, 0.01865218,\n",
       "       0.01907529, 0.02003133, 0.02418498, 0.01993056, 0.02172282,\n",
       "       0.02118365, 0.02044112, 0.02016383, 0.02048921, 0.0215823 ,\n",
       "       0.02140343, 0.02232104, 0.02134913, 0.02051459, 0.01981056,\n",
       "       0.02039936, 0.01949201, 0.01927872, 0.06018926, 0.01901491,\n",
       "       0.01631769, 0.02907819, 0.04779673, 0.05318206, 0.06342205,\n",
       "       0.09382545, 0.15371182, 0.08678072, 0.09449939, 0.13401925,\n",
       "       0.08827108, 0.04821657, 0.03649843, 0.02206977, 0.01862488,\n",
       "       0.02004537, 0.02105375, 0.02091696, 0.02073159, 0.0204962 ,\n",
       "       0.02172128, 0.02120053, 0.02068746, 0.01982216, 0.01865594,\n",
       "       0.01783216, 0.06486179, 0.03686019, 0.05175039, 0.18187577,\n",
       "       0.23497876, 0.23782509, 0.21892142, 0.28096802, 0.33094258,\n",
       "       0.34244595, 0.28324538, 0.22908222, 0.26717961, 0.20453098,\n",
       "       0.10059811, 0.02360913, 0.01779731, 0.01891392, 0.01940027,\n",
       "       0.01907471, 0.02150897, 0.0214992 , 0.01966226, 0.02026648,\n",
       "       0.0196207 , 0.02105651, 0.03494409, 0.07947688, 0.13029556,\n",
       "       0.29555936, 0.30854918, 0.33004566, 0.38180255, 0.43803964,\n",
       "       0.54918785, 0.57889809, 0.62617469, 0.57951652, 0.69117045,\n",
       "       0.68369391, 0.60214712, 0.55648718, 0.38182637, 0.18275745,\n",
       "       0.08107818, 0.06710619, 0.04169664, 0.02529988, 0.02012896,\n",
       "       0.02155583, 0.02134634, 0.02016568, 0.02021545, 0.02364823,\n",
       "       0.08678306, 0.22235305, 0.29967166, 0.27275107, 0.33138   ,\n",
       "       0.40391204, 0.43007711, 0.66454995, 0.66675438, 0.60042296,\n",
       "       0.65348699, 0.64902304, 0.57898667, 0.71413648, 0.70770359,\n",
       "       0.58077375, 0.50906495, 0.4337644 , 0.29266412, 0.20152636,\n",
       "       0.0585551 , 0.05634562, 0.0202394 , 0.02088943, 0.0207562 ,\n",
       "       0.02015785, 0.01899508, 0.03782514, 0.17004347, 0.29311961,\n",
       "       0.28778562, 0.42679022, 0.47103102, 0.53415587, 0.61449716,\n",
       "       0.67305003, 0.59059932, 0.69859087, 0.77657627, 0.61768517,\n",
       "       0.5249861 , 0.53620865, 0.74286781, 0.66020289, 0.49786292,\n",
       "       0.37412509, 0.33235047, 0.28786913, 0.09073689, 0.0595704 ,\n",
       "       0.02091288, 0.01977387, 0.02071955, 0.01811678, 0.01824466,\n",
       "       0.08839194, 0.247942  , 0.28023094, 0.47407471, 0.64548691,\n",
       "       0.59989131, 0.56237029, 0.62707895, 0.72611355, 0.71565623,\n",
       "       0.64423259, 0.70378721, 0.66091847, 0.72617428, 0.63247521,\n",
       "       0.72410785, 0.70734003, 0.654123  , 0.57576828, 0.50751273,\n",
       "       0.32816803, 0.09642707, 0.04515833, 0.02076973, 0.02133148,\n",
       "       0.01969419, 0.02074096, 0.03184167, 0.24438343, 0.32615902,\n",
       "       0.37988975, 0.55193018, 0.54650723, 0.60170827, 0.65439756,\n",
       "       0.69430392, 0.69624845, 0.73791746, 0.68532923, 0.69479774,\n",
       "       0.69375944, 0.69084709, 0.7012838 , 0.70444204, 0.67080346,\n",
       "       0.62372774, 0.60170149, 0.4264864 , 0.32637241, 0.06955875,\n",
       "       0.02048189, 0.02152279, 0.02050623, 0.01977437, 0.01816321,\n",
       "       0.07137653, 0.29409917, 0.34301178, 0.40481931, 0.47301631,\n",
       "       0.51279231, 0.52075295, 0.56937093, 0.56014725, 0.68342465,\n",
       "       0.66581208, 0.63148958, 0.68651323, 0.66923269, 0.61464793,\n",
       "       0.66657279, 0.76495383, 0.55052411, 0.52292851, 0.55346971,\n",
       "       0.3711774 , 0.26070085, 0.05779469, 0.02318388, 0.0194704 ,\n",
       "       0.02143548, 0.01938205, 0.01589987, 0.03761451, 0.23684937,\n",
       "       0.35614182, 0.49439242, 0.42843332, 0.49505725, 0.62886595,\n",
       "       0.6068134 , 0.54014225, 0.7754451 , 0.66809834, 0.68402434,\n",
       "       0.7466759 , 0.67402097, 0.60015975, 0.68159884, 0.67499929,\n",
       "       0.47737013, 0.5505437 , 0.41175695, 0.39056104, 0.20558874,\n",
       "       0.03148593, 0.01921875, 0.02114495, 0.02024129, 0.01849137,\n",
       "       0.01672385, 0.0337499 , 0.25460209, 0.3703996 , 0.4608671 ,\n",
       "       0.46559441, 0.61712449, 0.66900112, 0.6882815 , 0.80551733,\n",
       "       0.73240558, 0.72153862, 0.75273238, 0.65751573, 0.73782763,\n",
       "       0.67595271, 0.64666666, 0.65793142, 0.72541897, 0.50129964,\n",
       "       0.39283779, 0.34866693, 0.11526728, 0.01808378, 0.01942007,\n",
       "       0.02058008, 0.02142353, 0.02024889, 0.01866293, 0.02884029,\n",
       "       0.21585517, 0.34324515, 0.39848953, 0.46513257, 0.64496218,\n",
       "       0.63171057, 0.73155715, 0.70555836, 0.61221856, 0.8049087 ,\n",
       "       0.75527291, 0.70262771, 0.7211527 , 0.6665239 , 0.62276941,\n",
       "       0.70078636, 0.69754543, 0.4375387 , 0.36318312, 0.21833959,\n",
       "       0.15609671, 0.07757741, 0.03408291, 0.01983084, 0.01990355,\n",
       "       0.02078545, 0.01976499, 0.03957388, 0.17048634, 0.34848455,\n",
       "       0.40134793, 0.50782688, 0.66396481, 0.61692888, 0.64461646,\n",
       "       0.54699659, 0.81735697, 0.73201794, 0.73858444, 0.79205553,\n",
       "       0.80019024, 0.63419589, 0.60548191, 0.73169912, 0.62822345,\n",
       "       0.54782443, 0.46721192, 0.28400923, 0.2227615 , 0.05254911,\n",
       "       0.02520348, 0.01966307, 0.02061888, 0.02005566, 0.03714036,\n",
       "       0.10970283, 0.16044281, 0.36329617, 0.40647959, 0.58875247,\n",
       "       0.70190705, 0.70988454, 0.64116236, 0.64142852, 0.73445409,\n",
       "       0.65594942, 0.68383595, 0.73923773, 0.67002318, 0.5579827 ,\n",
       "       0.64790027, 0.68998876, 0.66224661, 0.54964122, 0.47392647,\n",
       "       0.26335065, 0.21763818, 0.03083464, 0.01841916, 0.0192279 ,\n",
       "       0.0200051 , 0.02022356, 0.03968733, 0.09931755, 0.20486717,\n",
       "       0.30754239, 0.43312184, 0.6429557 , 0.62152096, 0.65591049,\n",
       "       0.69717822, 0.81063029, 0.78807102, 0.65124911, 0.73761357,\n",
       "       0.6538918 , 0.67572984, 0.74543312, 0.66235081, 0.51955259,\n",
       "       0.69755276, 0.65153177, 0.40608306, 0.23259804, 0.21239686,\n",
       "       0.02279971, 0.01782902, 0.02063749, 0.02076157, 0.02047399,\n",
       "       0.01886907, 0.03553464, 0.23329598, 0.27990898, 0.44155999,\n",
       "       0.50937035, 0.66525095, 0.70140631, 0.75286983, 0.79032606,\n",
       "       0.7795561 , 0.63255009, 0.73902246, 0.75194766, 0.78829178,\n",
       "       0.75628016, 0.72955995, 0.59020147, 0.6201828 , 0.67361987,\n",
       "       0.37113961, 0.2492318 , 0.2273871 , 0.0376432 , 0.02086121,\n",
       "       0.01924045, 0.01983594, 0.01951032, 0.02001603, 0.02996427,\n",
       "       0.25578736, 0.30018509, 0.47742921, 0.57416729, 0.61612536,\n",
       "       0.62236328, 0.74645566, 0.79628721, 0.73876815, 0.64818904,\n",
       "       0.83717437, 0.77098742, 0.86009269, 0.83589843, 0.82984543,\n",
       "       0.61687412, 0.58622512, 0.42433034, 0.3364623 , 0.23096059,\n",
       "       0.2017408 , 0.04967547, 0.02256082, 0.02096752, 0.02052593,\n",
       "       0.02012909, 0.02035327, 0.04991453, 0.28524181, 0.39094298,\n",
       "       0.41780629, 0.70323587, 0.59251191, 0.67427292, 0.85628209,\n",
       "       0.71713478, 0.61219422, 0.72899552, 0.74018914, 0.8005624 ,\n",
       "       0.75886438, 0.70168655, 0.76516865, 0.68497561, 0.4727465 ,\n",
       "       0.38747168, 0.37157767, 0.33179015, 0.18387228, 0.04241735,\n",
       "       0.01971679, 0.01973024, 0.02085663, 0.01928015, 0.02070692,\n",
       "       0.08044814, 0.33292016, 0.36382634, 0.51861673, 0.53083799,\n",
       "       0.66108162, 0.70410526, 0.60294229, 0.63599117, 0.77706381,\n",
       "       0.72897001, 0.58799474, 0.73646566, 0.67455537, 0.56440231,\n",
       "       0.53420604, 0.57291475, 0.48841823, 0.45930781, 0.35944813,\n",
       "       0.34952919, 0.08182226, 0.01806737, 0.02022348, 0.0185246 ,\n",
       "       0.02058652, 0.02039704, 0.01779251, 0.13063106, 0.33640155,\n",
       "       0.41218293, 0.47567076, 0.48398506, 0.6089896 , 0.6328485 ,\n",
       "       0.61217745, 0.61741235, 0.71482379, 0.69859992, 0.71312359,\n",
       "       0.54681929, 0.63638509, 0.5210929 , 0.55787802, 0.4898936 ,\n",
       "       0.49931743, 0.38602266, 0.38511782, 0.38656387, 0.09439522,\n",
       "       0.01718922, 0.02009762, 0.01990377, 0.01985151, 0.01934129,\n",
       "       0.02321125, 0.0683688 , 0.32538441, 0.33481652, 0.49095703,\n",
       "       0.53541041, 0.63008881, 0.73769243, 0.63043356, 0.63826405,\n",
       "       0.65710461, 0.69340599, 0.65066402, 0.62618951, 0.56430556,\n",
       "       0.59992062, 0.62866515, 0.47401973, 0.43132509, 0.3674274 ,\n",
       "       0.39882754, 0.26903737, 0.0721566 , 0.0176895 , 0.02054186,\n",
       "       0.02070158, 0.02062111, 0.02070167, 0.02777699, 0.03032977,\n",
       "       0.24807258, 0.32785906, 0.33338632, 0.49943939, 0.57798751,\n",
       "       0.51796398, 0.58249567, 0.61867439, 0.6546146 , 0.68761279,\n",
       "       0.67864689, 0.65111717, 0.72299447, 0.61916054, 0.64395134,\n",
       "       0.51623735, 0.46596547, 0.37136369, 0.37484784, 0.10392857,\n",
       "       0.05431444, 0.0181906 , 0.02084875, 0.01960734, 0.020608  ,\n",
       "       0.01991677, 0.02025938, 0.02247546, 0.06511096, 0.3259598 ,\n",
       "       0.35540669, 0.42368872, 0.52391581, 0.47699947, 0.68683564,\n",
       "       0.74552388, 0.56414428, 0.58462737, 0.68790443, 0.56481017,\n",
       "       0.62614947, 0.66458285, 0.51351291, 0.4057784 , 0.36583068,\n",
       "       0.2461359 , 0.12691998, 0.07335261, 0.03464657, 0.02028321,\n",
       "       0.02148804, 0.02080409, 0.02139664, 0.02177185, 0.01934449,\n",
       "       0.01837766, 0.02635286, 0.0813328 , 0.29817723, 0.37552923,\n",
       "       0.43560909, 0.51512612, 0.63188878, 0.6566972 , 0.58414784,\n",
       "       0.65560603, 0.59787424, 0.55841681, 0.46748555, 0.31924201,\n",
       "       0.26645637, 0.23920776, 0.10678926, 0.11004156, 0.05471429,\n",
       "       0.0373522 , 0.03896503, 0.01972068, 0.02034294, 0.02089224,\n",
       "       0.01898237, 0.02121204, 0.02031421, 0.01992139, 0.0217681 ,\n",
       "       0.02136513, 0.05108153, 0.08549141, 0.15093917, 0.18162497,\n",
       "       0.21192225, 0.25404283, 0.27875073, 0.30469007, 0.30326864,\n",
       "       0.3874968 , 0.26883524, 0.19977311, 0.24147324, 0.08477292,\n",
       "       0.06945484, 0.08590531, 0.03841644, 0.0189669 , 0.02059464,\n",
       "       0.02130422, 0.02124779, 0.02105374, 0.02117771, 0.02063016,\n",
       "       0.02009459, 0.02171595, 0.02029633, 0.01838687, 0.04360372,\n",
       "       0.04970916, 0.10242684, 0.12272933, 0.15708163, 0.23646086,\n",
       "       0.25017532, 0.21785017, 0.19549728, 0.11857504, 0.09770681,\n",
       "       0.07066206, 0.06178549, 0.05695416, 0.05208569, 0.03947459,\n",
       "       0.02060449, 0.020167  , 0.01957426, 0.01956932, 0.0215896 ,\n",
       "       0.02205095, 0.02074416, 0.02174341, 0.0201347 , 0.01949508,\n",
       "       0.01988188, 0.02200621, 0.02184033, 0.02025301, 0.01887188,\n",
       "       0.02581131, 0.05100572, 0.04166215, 0.07276685, 0.08106559,\n",
       "       0.05449362, 0.03168819, 0.0217618 , 0.01993653, 0.01808923,\n",
       "       0.01983094, 0.02583958, 0.02167964, 0.0212159 , 0.02117253,\n",
       "       0.02019962, 0.02072315, 0.02134698, 0.02051481])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm_each_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_l1_diag(nn.Module):\n",
    "    def __init__(self, hidden_layer_size, z_size):\n",
    "        super(VAE_l1_diag, self).__init__()\n",
    "        \n",
    "        self.diag = torch.normal(torch.zeros(784), torch.ones(784)).clone().detach().requires_grad_(True)\n",
    "        self.selection_layer = torch.diag(self.diag)\n",
    "        self.fc1 = nn.Linear(784, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h0 = torch.mm(x, self.selection_layer)\n",
    "        h1 = F.relu(self.fc1(h0))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l1_added(model, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        l1_norm = 10 * torch.norm(model.diag, p=1)\n",
    "        loss += l1_norm\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         '../data/first_test_results_l1_on_loss_diag/reconstruction_' + \n",
    "                           str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE_l1_diag(400, 20).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 644.795898\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 332.856140\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 304.049805\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 291.460541\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 274.381256\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 255.409866\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 268.645874\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 262.972595\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 255.126434\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 253.389923\n",
      "====> Epoch: 1 Average loss: 286.7495\n",
      "====> Test set loss: 150.1991\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 243.030045\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 238.883652\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 239.931686\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 235.622330\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 234.030960\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 231.092621\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 223.840302\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 226.030304\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 228.991058\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 223.689743\n",
      "====> Epoch: 2 Average loss: 234.4010\n",
      "====> Test set loss: 131.0091\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 231.669907\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 230.044785\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 225.651077\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 225.933975\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 229.114609\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 222.323059\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 220.644363\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 211.778534\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 219.014099\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 213.450943\n",
      "====> Epoch: 3 Average loss: 221.8116\n",
      "====> Test set loss: 122.7798\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 221.667557\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 212.665451\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 219.740479\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 214.413025\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 218.368729\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 216.749130\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 219.610275\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 211.059631\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 211.247620\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 212.565338\n",
      "====> Epoch: 4 Average loss: 215.6876\n",
      "====> Test set loss: 118.1470\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 207.087906\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 218.395081\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 207.771179\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 207.249466\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 215.766525\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 210.794708\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 209.925568\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 210.478973\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 213.118546\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 211.419815\n",
      "====> Epoch: 5 Average loss: 212.1979\n",
      "====> Test set loss: 115.5444\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 203.871353\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 210.752228\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 204.838226\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 212.810974\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 215.105988\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 217.217026\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 212.126190\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 201.517700\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 205.455002\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 213.706146\n",
      "====> Epoch: 6 Average loss: 209.8374\n",
      "====> Test set loss: 113.6401\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 206.346878\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 213.825699\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 214.647888\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 207.288940\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 207.541779\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 210.611298\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 211.947083\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 212.502167\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 208.086807\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 210.942444\n",
      "====> Epoch: 7 Average loss: 208.1399\n",
      "====> Test set loss: 112.3997\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 199.371094\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 208.081421\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 207.284943\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 210.441040\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 203.960266\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 209.948502\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 208.603424\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 205.484634\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 203.362976\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 203.677994\n",
      "====> Epoch: 8 Average loss: 206.8537\n",
      "====> Test set loss: 111.0348\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 205.787476\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 204.947113\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 207.919464\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 204.653671\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 211.109711\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 208.094269\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 198.044296\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 203.132797\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 209.408997\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 208.021362\n",
      "====> Epoch: 9 Average loss: 205.8192\n",
      "====> Test set loss: 110.2133\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 207.182251\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 204.405273\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 207.996613\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 203.044159\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 203.124817\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 206.725647\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 201.366333\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 206.796631\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 204.589417\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 209.899841\n",
      "====> Epoch: 10 Average loss: 204.9296\n",
      "====> Test set loss: 109.5140\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 203.142365\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 202.830902\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 203.826843\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 197.832275\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 210.795120\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 208.301346\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 202.307053\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 201.971756\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 210.369965\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 204.841156\n",
      "====> Epoch: 11 Average loss: 204.2215\n",
      "====> Test set loss: 108.8205\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 202.763397\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 202.270081\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 199.188004\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 202.703247\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 200.077377\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 202.469666\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 205.553070\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 207.214600\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 200.258270\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 202.989532\n",
      "====> Epoch: 12 Average loss: 203.6510\n",
      "====> Test set loss: 108.4205\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 204.701385\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 206.849136\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 204.270203\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 204.297501\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 211.320038\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 194.903976\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 194.327682\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 198.881073\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 207.543182\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 202.611389\n",
      "====> Epoch: 13 Average loss: 203.0992\n",
      "====> Test set loss: 107.8845\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 197.371338\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 200.938309\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 202.638474\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 205.058685\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 206.473541\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 206.713257\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 206.732422\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 198.982086\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 202.367981\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 200.903809\n",
      "====> Epoch: 14 Average loss: 202.6550\n",
      "====> Test set loss: 107.6637\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 202.583160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 205.791611\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 204.912384\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 206.347473\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 200.009705\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 203.163361\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 195.428101\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 203.810349\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 206.656281\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 199.066315\n",
      "====> Epoch: 15 Average loss: 202.2496\n",
      "====> Test set loss: 107.2704\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 201.210739\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 203.779465\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 204.356201\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 202.558273\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 200.780380\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 202.588669\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 203.737610\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 199.732880\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 202.122833\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 193.782593\n",
      "====> Epoch: 16 Average loss: 201.9176\n",
      "====> Test set loss: 106.8669\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 205.070160\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 196.019180\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 207.437042\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 203.218475\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 205.285828\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 202.245392\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 207.908905\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 200.000458\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 204.124908\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 198.185242\n",
      "====> Epoch: 17 Average loss: 201.5783\n",
      "====> Test set loss: 106.4782\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 201.469650\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 201.017395\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 205.460983\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 197.367188\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 199.773941\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 199.994598\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 205.770874\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 200.858917\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 199.391022\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 199.623993\n",
      "====> Epoch: 18 Average loss: 201.2823\n",
      "====> Test set loss: 106.2457\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 204.499542\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 201.902344\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 197.393860\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 191.444000\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 201.246994\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 201.080383\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 203.197510\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 201.383240\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 199.195114\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 201.406845\n",
      "====> Epoch: 19 Average loss: 201.0107\n",
      "====> Test set loss: 106.1793\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 201.372437\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 196.699890\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 203.174042\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 204.436386\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 197.407623\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 198.490280\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 194.140106\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 197.361938\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 202.945099\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 199.239349\n",
      "====> Epoch: 20 Average loss: 200.7666\n",
      "====> Test set loss: 105.9363\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train_l1_added(model, epoch)\n",
    "        test(model, epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       '../data/first_test_results_l1_on_loss_diag/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), BASE_PATH_DATA + \"../data/models/first_try/no_extra_layer_l1_diag.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_layer = model.selection_layer.detach().clone().numpy()\n",
    "abs_selection_layer = np.abs(selection_layer)\n",
    "l1_norm_each_column = np.apply_along_axis(sum, 0, abs_selection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.52356839e+00, 1.08170009e+00, 1.96719110e+00, 9.39557016e-01,\n",
       "       8.39012146e-01, 1.11134028e+00, 8.19181204e-01, 1.62094057e+00,\n",
       "       8.05505812e-02, 2.39829570e-01, 1.08921766e+00, 2.38948181e-01,\n",
       "       1.29866993e+00, 1.65522806e-02, 1.34561896e+00, 3.72081578e-01,\n",
       "       1.54987741e+00, 2.67038852e-01, 4.28588331e-01, 1.92137454e-02,\n",
       "       5.28924108e-01, 3.08857154e-04, 8.42303813e-01, 1.37691545e+00,\n",
       "       3.57276499e-02, 4.97702397e-02, 9.84163821e-01, 8.25465679e-01,\n",
       "       1.10408321e-01, 5.36151648e-01, 2.50806427e+00, 1.92158327e-01,\n",
       "       6.77519441e-01, 1.13175072e-01, 4.62892413e-01, 2.29048893e-01,\n",
       "       2.34253854e-01, 3.63710761e-01, 2.96998501e-01, 4.15840358e-01,\n",
       "       1.16567123e+00, 8.42772499e-02, 9.74053144e-01, 1.29100072e+00,\n",
       "       4.46270168e-01, 1.52253795e+00, 1.24962282e+00, 9.04596567e-01,\n",
       "       2.65812082e-03, 2.44100428e+00, 1.66805208e-01, 3.30642879e-01,\n",
       "       9.63173509e-01, 1.00976694e+00, 5.38245678e-01, 1.90841243e-01,\n",
       "       5.48124492e-01, 7.55122542e-01, 4.54519659e-01, 5.30356586e-01,\n",
       "       2.27328874e-02, 1.76181912e+00, 6.07135653e-01, 2.73586482e-01,\n",
       "       6.35555327e-01, 6.42488301e-02, 3.30017865e-01, 5.42114556e-01,\n",
       "       1.01877034e+00, 3.43036056e-01, 1.69539368e+00, 6.81755960e-01,\n",
       "       1.07315731e+00, 8.67493987e-01, 6.90092146e-01, 1.86718702e-01,\n",
       "       8.75927269e-01, 2.82800019e-01, 7.93055117e-01, 5.18237472e-01,\n",
       "       7.29661107e-01, 4.05687124e-01, 5.74267626e-01, 3.25403750e-01,\n",
       "       2.23072781e-03, 1.13768077e+00, 1.82838082e-01, 4.62668210e-01,\n",
       "       4.42427725e-01, 1.70937967e+00, 7.83743411e-02, 3.41309041e-01,\n",
       "       4.86750036e-01, 3.05152148e-01, 7.79450715e-01, 9.08902586e-01,\n",
       "       4.34654057e-02, 3.35138515e-02, 6.96818009e-02, 1.66561496e+00,\n",
       "       3.76043409e-01, 9.07707274e-01, 1.28163564e+00, 7.86454797e-01,\n",
       "       8.00072551e-02, 4.48377840e-02, 2.16144528e-02, 2.17711830e+00,\n",
       "       1.08825922e+00, 9.30032015e-01, 2.06351304e+00, 9.77278769e-01,\n",
       "       5.28287709e-01, 1.20171869e+00, 1.06592762e+00, 1.00177956e+00,\n",
       "       1.34723675e+00, 1.58935979e-01, 8.59006107e-01, 1.21634161e+00,\n",
       "       5.44497192e-01, 3.41107297e+00, 2.08956051e+00, 4.59308773e-02,\n",
       "       2.68433183e-01, 3.81965113e+00, 4.09227517e-03, 1.26369691e+00,\n",
       "       4.75677073e-01, 4.32488203e-01, 6.75332665e-01, 8.91123116e-01,\n",
       "       1.22023392e+00, 1.47632790e+00, 8.18934798e-01, 4.41174060e-02,\n",
       "       4.27043349e-01, 1.14306736e+00, 1.66108441e+00, 1.06836724e+00,\n",
       "       1.67370498e+00, 9.78188455e-01, 1.13410994e-01, 6.68389976e-01,\n",
       "       2.96118641e+00, 5.82361877e-01, 1.18595660e+00, 9.41433012e-01,\n",
       "       3.16823184e-01, 1.46724713e+00, 1.20340061e+00, 8.14385295e-01,\n",
       "       6.72778487e-01, 1.61476147e+00, 2.48545265e+00, 1.12244356e+00,\n",
       "       6.29905164e-01, 1.16304345e-01, 1.51030207e+00, 1.10483742e+00,\n",
       "       1.96634784e-01, 2.79347688e-01, 2.15211010e+00, 1.87985730e+00,\n",
       "       1.42299581e+00, 2.10991001e+00, 1.44632608e-01, 3.54310632e-01,\n",
       "       1.31449115e+00, 5.54238677e-01, 1.58569738e-01, 4.93795276e-01,\n",
       "       4.05541420e-01, 7.65468955e-01, 1.14272368e+00, 9.66326177e-01,\n",
       "       6.46114945e-01, 1.31581724e+00, 8.81438136e-01, 4.35984552e-01,\n",
       "       3.98014247e-01, 6.03238583e-01, 6.41275704e-01, 1.48405898e+00,\n",
       "       1.11037374e+00, 1.25708640e+00, 9.09587324e-01, 8.94643739e-03,\n",
       "       4.79170799e-01, 1.26035202e+00, 1.81179851e-01, 9.71919239e-01,\n",
       "       8.41679350e-02, 1.51119724e-01, 8.15613687e-01, 8.16051662e-01,\n",
       "       7.31514096e-01, 5.33672512e-01, 7.60603666e-01, 1.14013541e+00,\n",
       "       1.39365458e+00, 1.36537206e+00, 1.98866415e+00, 9.20713041e-03,\n",
       "       8.53208303e-01, 6.02178514e-01, 1.84729862e+00, 4.10580248e-01,\n",
       "       4.84128557e-02, 4.58154172e-01, 1.67127454e+00, 5.50259314e-02,\n",
       "       2.82767620e-02, 1.74521756e+00, 8.69988620e-01, 3.11067760e-01,\n",
       "       4.05584514e-01, 1.22212660e+00, 1.00258458e+00, 6.04417503e-01,\n",
       "       5.30019224e-01, 1.01352501e+00, 6.77740812e-01, 2.10685313e-01,\n",
       "       1.18017399e+00, 1.02732265e+00, 5.00109315e-01, 3.21717560e-01,\n",
       "       4.66246128e-01, 8.59607637e-01, 1.12287700e+00, 1.66157150e+00,\n",
       "       3.65771025e-01, 9.20940042e-02, 7.94066966e-01, 6.47703588e-01,\n",
       "       7.35682920e-02, 6.57846689e-01, 1.36896884e+00, 6.27635181e-01,\n",
       "       1.78477788e+00, 1.43945420e+00, 3.55028927e-01, 6.69108927e-01,\n",
       "       4.28186178e-01, 1.04949343e+00, 1.19448066e+00, 2.04673672e+00,\n",
       "       4.41564173e-01, 4.35455978e-01, 3.01915002e+00, 1.01204836e+00,\n",
       "       6.86377361e-02, 2.01950431e+00, 1.54316640e+00, 1.90451014e+00,\n",
       "       6.50388360e-01, 2.51042604e-01, 9.96693850e-01, 7.99068332e-01,\n",
       "       1.02694690e+00, 9.55008805e-01, 1.41385591e+00, 1.08361578e+00,\n",
       "       1.43626824e-01, 1.22154224e+00, 3.64313245e-01, 3.56299639e-01,\n",
       "       1.29136816e-01, 2.56445501e-02, 1.72155023e-01, 7.83547580e-01,\n",
       "       5.58895111e-01, 4.44605172e-01, 8.16908062e-01, 3.21993798e-01,\n",
       "       3.07820022e-01, 4.79575306e-01, 1.75723657e-01, 1.17448673e-01,\n",
       "       2.23974466e+00, 2.84813821e-01, 2.99424976e-01, 4.60553139e-01,\n",
       "       1.58503842e+00, 4.34955508e-01, 1.17646316e-02, 3.87300879e-01,\n",
       "       4.73115772e-01, 2.02423379e-01, 1.08477664e+00, 3.80481929e-01,\n",
       "       5.64260542e-01, 1.54229984e-01, 9.81937170e-01, 7.02936709e-01,\n",
       "       4.99053508e-01, 3.23096931e-01, 5.13081610e-01, 5.75855613e-01,\n",
       "       3.10794830e-01, 9.51696396e-01, 6.96001291e-01, 5.47479212e-01,\n",
       "       1.53950167e+00, 5.19895077e-01, 1.05710578e+00, 1.11953318e+00,\n",
       "       1.52058527e-01, 5.23561776e-01, 7.35997021e-01, 7.89630562e-02,\n",
       "       1.83368444e-01, 2.22242087e-01, 2.75658488e-01, 6.71358287e-01,\n",
       "       1.30047524e+00, 1.54359245e+00, 9.77648079e-01, 8.80293429e-01,\n",
       "       9.34341401e-02, 5.46277642e-01, 1.18142164e+00, 8.63451064e-02,\n",
       "       1.48587179e+00, 7.39336908e-01, 9.99992490e-01, 3.63042057e-01,\n",
       "       1.56733319e-01, 1.36398029e+00, 5.67810889e-03, 7.54779637e-01,\n",
       "       7.31786966e-01, 4.94607657e-01, 1.33566451e+00, 1.14636421e-01,\n",
       "       7.57514656e-01, 6.71147034e-02, 8.99018764e-01, 9.79416966e-01,\n",
       "       7.26867616e-01, 2.64726281e-01, 5.79301417e-01, 9.50466692e-01,\n",
       "       1.61279106e+00, 1.08371866e+00, 1.51671338e+00, 1.72998413e-01,\n",
       "       2.25349808e+00, 1.23282349e+00, 7.83329725e-01, 1.02907121e+00,\n",
       "       8.79836261e-01, 5.77262580e-01, 8.97781134e-01, 1.63956463e+00,\n",
       "       3.48822236e-01, 7.17643201e-01, 1.07806873e+00, 1.35175395e+00,\n",
       "       6.58848703e-01, 6.93931818e-01, 1.16253436e+00, 7.23361850e-01,\n",
       "       1.22971475e+00, 4.36706960e-01, 9.19022977e-01, 9.46145535e-01,\n",
       "       1.39504647e+00, 2.00628459e-01, 3.29971194e-01, 3.87841463e-01,\n",
       "       1.38158906e+00, 5.38544178e-01, 1.09215760e+00, 1.81228590e+00,\n",
       "       5.27292311e-01, 6.32884920e-01, 8.00365627e-01, 1.97662246e+00,\n",
       "       1.00505137e+00, 1.77911699e+00, 1.98878384e+00, 3.85612547e-02,\n",
       "       7.23290265e-01, 2.94408143e-01, 7.82289326e-01, 6.35772407e-01,\n",
       "       8.40544045e-01, 1.16019011e+00, 1.17808342e-01, 7.84758449e-01,\n",
       "       2.11400771e+00, 1.07237256e+00, 1.43663853e-01, 4.42097820e-02,\n",
       "       1.49923909e+00, 1.20899904e+00, 8.00100744e-01, 5.99822402e-02,\n",
       "       5.75516880e-01, 1.07967126e+00, 2.40014195e+00, 5.69735587e-01,\n",
       "       2.95274556e-01, 1.10889602e+00, 3.36636335e-01, 2.61389947e+00,\n",
       "       9.00726080e-01, 4.56762254e-01, 5.00986695e-01, 2.87406266e-01,\n",
       "       1.14947259e+00, 1.56482562e-01, 2.10308576e+00, 1.45156133e+00,\n",
       "       1.20708060e+00, 2.98935890e-01, 2.07740974e+00, 1.48626089e+00,\n",
       "       1.48197949e-01, 1.64639801e-01, 5.06235421e-01, 5.76101005e-01,\n",
       "       9.68548134e-02, 4.30929989e-01, 6.88575745e-01, 4.80757028e-01,\n",
       "       3.52768064e-01, 2.81898826e-02, 3.22896838e-01, 2.28538409e-01,\n",
       "       4.82193410e-01, 1.25121427e+00, 1.32478905e+00, 1.74469441e-01,\n",
       "       3.17690659e+00, 9.78016794e-01, 2.67785716e+00, 1.01726258e+00,\n",
       "       1.13634968e+00, 6.27933562e-01, 1.34658352e-01, 3.02322581e-02,\n",
       "       3.23924392e-01, 2.49527156e-01, 6.73762977e-01, 3.75809103e-01,\n",
       "       2.38161713e-01, 1.15257752e+00, 7.37039089e-01, 2.49162927e-01,\n",
       "       2.24237591e-01, 1.06582701e+00, 4.63912219e-01, 6.23438776e-01,\n",
       "       3.44109237e-01, 9.02130723e-01, 1.81929097e-01, 1.29463160e+00,\n",
       "       5.80758154e-01, 5.88513255e-01, 1.02273118e+00, 3.17819774e-01,\n",
       "       3.90601903e-01, 1.26807302e-01, 1.87573004e+00, 2.85181552e-01,\n",
       "       5.98753393e-01, 3.79057795e-01, 1.05722988e+00, 6.94113314e-01,\n",
       "       9.97390509e-01, 1.08555532e+00, 2.22670698e+00, 1.46886551e+00,\n",
       "       5.00228107e-01, 1.33207417e+00, 1.19184554e+00, 4.72573817e-01,\n",
       "       9.01558280e-01, 8.14065993e-01, 2.58769512e-01, 7.05161154e-01,\n",
       "       5.54340065e-01, 6.30940497e-01, 1.22626774e-01, 1.57899344e+00,\n",
       "       3.05623740e-01, 1.00577807e+00, 1.12366164e-02, 4.73939806e-01,\n",
       "       1.52831149e+00, 9.72824931e-01, 1.05693078e+00, 2.43777916e-01,\n",
       "       1.49376154e+00, 1.25102532e+00, 3.70031565e-01, 2.48620421e-01,\n",
       "       2.29749292e-01, 7.88668811e-01, 3.77377868e-01, 1.02159452e+00,\n",
       "       8.50724757e-01, 5.40452778e-01, 6.53001428e-01, 3.68563086e-01,\n",
       "       2.61692315e-01, 8.47473681e-01, 5.08518934e-01, 1.82548785e+00,\n",
       "       4.00877073e-02, 2.78831869e-01, 1.91806108e-01, 1.55369973e+00,\n",
       "       5.47994792e-01, 1.82174996e-01, 1.60883749e+00, 7.47022867e-01,\n",
       "       5.43115258e-01, 1.63346946e-01, 7.84020662e-01, 5.63823402e-01,\n",
       "       8.67739618e-02, 1.03275323e+00, 3.50005031e-01, 4.24513936e-01,\n",
       "       1.17912471e+00, 3.92142057e-01, 1.69023097e+00, 7.04160988e-01,\n",
       "       1.13127220e+00, 1.06393266e+00, 1.68343079e+00, 3.15562516e-01,\n",
       "       6.45436347e-01, 5.99814579e-02, 5.63154936e-01, 6.60448730e-01,\n",
       "       1.77309096e+00, 1.14311337e-01, 1.00937327e-02, 2.24125516e-02,\n",
       "       1.28602087e-01, 4.43994582e-01, 2.78531402e-01, 1.78347397e+00,\n",
       "       2.57204890e-01, 7.31113195e-01, 2.26810575e-01, 2.94484608e-02,\n",
       "       1.09764323e-01, 6.70542717e-01, 6.84990585e-01, 1.54146224e-01,\n",
       "       1.53085601e+00, 3.76633257e-01, 4.68583018e-01, 1.15817392e+00,\n",
       "       6.48789525e-01, 4.51333702e-01, 2.01085949e+00, 7.55966008e-01,\n",
       "       5.00914216e-01, 1.77719802e-01, 1.15632665e+00, 1.49026644e+00,\n",
       "       1.66208223e-01, 1.18030824e-01, 1.00854647e+00, 6.68591142e-01,\n",
       "       5.74511349e-01, 1.04188800e+00, 3.33044469e-01, 1.51417243e+00,\n",
       "       3.82393956e-01, 9.81577754e-01, 3.95697728e-02, 1.56177700e+00,\n",
       "       5.87662637e-01, 7.80862987e-01, 5.80529511e-01, 1.77961802e+00,\n",
       "       7.47160852e-01, 1.06496227e+00, 1.69708431e+00, 6.39600754e-01,\n",
       "       8.05658758e-01, 8.21878433e-01, 5.97181320e-01, 1.00057256e+00,\n",
       "       7.46302977e-02, 1.53402913e+00, 1.16482794e+00, 1.62320483e+00,\n",
       "       8.98154438e-01, 1.59490275e+00, 1.72102898e-01, 8.07622075e-02,\n",
       "       5.95162250e-02, 1.03664541e+00, 5.40067315e-01, 1.64617312e+00,\n",
       "       4.57108289e-01, 1.70143557e+00, 1.13103843e+00, 9.44828749e-01,\n",
       "       8.24406862e-01, 2.27002472e-01, 8.93121958e-01, 3.34919930e-01,\n",
       "       1.64193049e-01, 1.50387442e+00, 5.68482757e-01, 9.04484987e-02,\n",
       "       1.64027989e+00, 6.71249107e-02, 1.36833191e+00, 1.33445227e+00,\n",
       "       1.17629325e+00, 4.64593291e-01, 1.68575120e+00, 1.01223063e+00,\n",
       "       2.99350649e-01, 3.58959556e-01, 1.71240106e-01, 6.82725072e-01,\n",
       "       4.36674953e-01, 1.11588323e+00, 1.76256478e+00, 7.05882490e-01,\n",
       "       6.68919384e-01, 7.76600540e-01, 3.50472003e-01, 4.84061390e-01,\n",
       "       4.24698025e-01, 9.39826369e-01, 6.94399297e-01, 1.32407701e+00,\n",
       "       1.10559094e+00, 9.05061424e-01, 1.64257511e-01, 1.22105980e+00,\n",
       "       1.90901256e+00, 1.28518999e+00, 1.15453802e-01, 1.19938517e+00,\n",
       "       9.01126266e-01, 8.54073882e-01, 2.44213730e-01, 1.46431589e+00,\n",
       "       2.34478816e-01, 1.05984712e+00, 1.24344373e+00, 7.38638580e-01,\n",
       "       1.58658075e+00, 6.93365216e-01, 3.90761197e-02, 7.88668692e-01,\n",
       "       4.89267670e-02, 6.62137628e-01, 7.06152022e-01, 6.20051861e-01,\n",
       "       7.87023902e-01, 1.44769013e+00, 3.42750639e-01, 2.30854020e-01,\n",
       "       5.49424827e-01, 1.83589947e+00, 2.22786099e-01, 8.75973105e-02,\n",
       "       7.41728127e-01, 2.92492449e-01, 2.49914661e-01, 1.22779369e+00,\n",
       "       1.51518273e+00, 4.76475328e-01, 1.34211406e-01, 1.21263094e-01,\n",
       "       9.56116617e-01, 7.52801955e-01, 1.57706606e+00, 9.37219143e-01,\n",
       "       3.33378643e-01, 1.16163635e+00, 8.10350701e-02, 7.90460765e-01,\n",
       "       8.34517479e-01, 1.85769230e-01, 3.79748166e-01, 1.30185652e+00,\n",
       "       4.65716362e-01, 6.62629545e-01, 7.31815398e-01, 2.67325968e-01,\n",
       "       1.67221177e+00, 3.85597289e-01, 3.44689995e-01, 4.65459794e-01,\n",
       "       1.69495925e-01, 7.61922121e-01, 9.57779706e-01, 2.80860245e-01,\n",
       "       1.33693472e-01, 1.15516436e+00, 1.27507794e+00, 2.02432060e+00,\n",
       "       3.22070979e-02, 4.88595784e-01, 4.76662099e-01, 4.41534251e-01,\n",
       "       4.36415881e-01, 3.15542668e-01, 7.25825608e-01, 2.84414113e-01,\n",
       "       8.17896008e-01, 3.58987182e-01, 1.34776163e+00, 1.04588447e-02,\n",
       "       4.19506043e-01, 3.45975101e-01, 9.39096883e-02, 3.87322277e-01,\n",
       "       1.93654910e-01, 1.03851080e+00, 2.54532123e+00, 5.61628938e-01,\n",
       "       4.99521136e-01, 1.11303881e-01, 5.99669814e-01, 4.63284314e-01,\n",
       "       3.09313536e-01, 1.16612613e+00, 4.64387417e-01, 3.22410733e-01,\n",
       "       4.97931153e-01, 1.22605167e-01, 4.03844386e-01, 3.25664788e-01,\n",
       "       1.86530352e+00, 2.05645263e-01, 1.66290537e-01, 1.72273612e+00,\n",
       "       2.09263172e-02, 1.17967522e+00, 1.23302829e+00, 3.21117193e-01,\n",
       "       5.40397167e-01, 1.02461088e+00, 4.53469008e-01, 1.54232955e+00,\n",
       "       1.87158629e-01, 9.37413931e-01, 9.65041518e-01, 4.05365735e-01,\n",
       "       5.69393218e-01, 1.04633346e-02, 8.36044371e-01, 6.27741873e-01,\n",
       "       2.27366710e+00, 1.16153884e+00, 7.33386397e-01, 3.84510100e-01,\n",
       "       2.45862913e+00, 6.46206677e-01, 3.18340898e-01, 4.63154972e-01,\n",
       "       5.82711518e-01, 9.82380211e-01, 4.55324978e-01, 1.28698856e-01,\n",
       "       2.15821154e-02, 1.29165852e+00, 2.70779431e-01, 5.00191331e-01,\n",
       "       3.54822725e-02, 1.11017585e+00, 1.14427280e+00, 2.39874661e-01,\n",
       "       6.29767850e-02, 2.01222706e+00, 1.32442445e-01, 4.84578729e-01])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm_each_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5235684 ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  1.0817001 ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.9671911 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -2.012227  ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        -0.13244244,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        , -0.48457873]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.585748870064744"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(l1_norm_each_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([239., 210., 156.,  91.,  54.,  21.,   7.,   3.,   2.,   1.]),\n",
       " array([3.08857154e-04, 3.82243084e-01, 7.64177311e-01, 1.14611154e+00,\n",
       "        1.52804577e+00, 1.90997999e+00, 2.29191422e+00, 2.67384845e+00,\n",
       "        3.05578267e+00, 3.43771690e+00, 3.81965113e+00]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(l1_norm_each_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 784 artists>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(len(l1_norm_each_column)),l1_norm_each_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
