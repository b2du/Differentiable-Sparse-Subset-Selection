{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "img_size = 28\n",
    "channels = 1\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "z_size = 40\n",
    "\n",
    "n = 28 * 28\n",
    "\n",
    "# from running\n",
    "# EPSILON = np.finfo(tf.float32.as_numpy_dtype).tiny\n",
    "#EPSILON = 1.1754944e-38\n",
    "EPSILON = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Device\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        BASE_PATH_DATA + '/mnist/train',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor()]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        BASE_PATH_DATA + '/mnist/test', \n",
    "        train=False, \n",
    "        download = True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor()]\n",
    "        )\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_keys(w):\n",
    "    # sample some gumbels\n",
    "    uniform = (1.0 - EPSILON) * torch.rand_like(w) + EPSILON\n",
    "    z = torch.log(-torch.log(uniform))\n",
    "    w = w + z\n",
    "    return w\n",
    "\n",
    "\n",
    "def continuous_topk(w, k, t, separate=False):\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    khot_list = []\n",
    "    onehot_approx = torch.zeros_like(w, dtype = torch.float32)\n",
    "    for i in range(k):\n",
    "        ### conver the following into pytorch\n",
    "        #khot_mask = tf.maximum(1.0 - onehot_approx, EPSILON)\n",
    "        max_mask = 1 - onehot_approx < EPSILON\n",
    "        khot_mask = 1 - onehot_approx\n",
    "        khot_mask[max_mask] = EPSILON\n",
    "        \n",
    "        w += torch.log(khot_mask)\n",
    "        #onehot_approx = tf.nn.softmax(w / t, axis=-1)\n",
    "        onehot_approx = softmax(w/t)\n",
    "        khot_list.append(onehot_approx)\n",
    "    if separate:\n",
    "        return torch.stack(khot_list)\n",
    "    else:\n",
    "        return torch.sum(torch.stack(khot_list), dim = 0) \n",
    "\n",
    "\n",
    "def sample_subset(w, k, t=0.1):\n",
    "    '''\n",
    "    Args:\n",
    "        w (Tensor): Float Tensor of weights for each element. In gumbel mode\n",
    "            these are interpreted as log probabilities\n",
    "        k (int): number of elements in the subset sample\n",
    "        t (float): temperature of the softmax\n",
    "    '''\n",
    "    w = gumbel_keys(w)\n",
    "    return continuous_topk(w, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Gumbel_MNIST(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, z_size, k, t = 0.1):\n",
    "        super(VAE_Gumbel_MNIST, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        self.t = t\n",
    "        \n",
    "        self.weight_creator = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, input_size)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc21 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc22 = nn.Linear(hidden_layer_size, z_size)\n",
    "        self.fc3 = nn.Linear(z_size, hidden_layer_size)\n",
    "        self.fc4 = nn.Linear(hidden_layer_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        w = self.weight_creator(x)\n",
    "        subset_indices = sample_subset(w, self.k, self.t)\n",
    "        x = x * subset_indices\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch, k):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         '../data/gumbel_on_mnist/k_{}_reconstruction_'.format(k) + \n",
    "                           str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE_Gumbel_MNIST(784, 400, 20, k = k).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas = (b1,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 547.400330\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 208.939606\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 183.220062\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 174.540253\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 168.974152\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 166.143539\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 156.947495\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 159.339264\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 154.260208\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 155.986191\n",
      "====> Epoch: 1 Average loss: 175.4085\n",
      "====> Test set loss: 150.1431\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 152.002121\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 141.981140\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 147.464172\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 143.369217\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 154.665802\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 150.158340\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 140.129517\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 150.037109\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 146.265289\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 146.909607\n",
      "====> Epoch: 2 Average loss: 146.5927\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ab85906b764d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m#with torch.no_grad():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#    diag_model.diag.data[torch.abs(diag_model.diag) < 0.05] = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-92f8bfcc1f32>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9c17ec004212>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9c17ec004212>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msubset_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msubset_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-685e7777a87a>\u001b[0m in \u001b[0;36msample_subset\u001b[0;34m(w, k, t)\u001b[0m\n\u001b[1;32m     37\u001b[0m     '''\n\u001b[1;32m     38\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgumbel_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontinuous_topk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-685e7777a87a>\u001b[0m in \u001b[0;36mcontinuous_topk\u001b[0;34m(w, k, t, separate)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m### conver the following into pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#khot_mask = tf.maximum(1.0 - onehot_approx, EPSILON)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mmax_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0monehot_approx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mkhot_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0monehot_approx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mkhot_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nyu/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rdiv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        train(model, optimizer, epoch)\n",
    "        #with torch.no_grad():\n",
    "        #    diag_model.diag.data[torch.abs(diag_model.diag) < 0.05] = 0\n",
    "        test(model, epoch)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       '../data/gumbel_on_mnist/k_{}_sample_'.format(k) + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data, _) in enumerate(test_loader):\n",
    "    data = data.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff20448ef90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    w = model.weight_creator(data[3, :].view(-1, 784))\n",
    "    subset_indices = sample_subset(w, k=50, t=0.1)\n",
    "plt.imshow(subset_indices.reshape((28, 28)) > 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 549.401611\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 200.720215\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 205.774826\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 186.701508\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 193.055817\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 197.984131\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 179.042389\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 182.433334\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 183.756561\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 184.392181\n",
      "====> Epoch: 1 Average loss: 199.9500\n",
      "====> Test set loss: 185.1121\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 194.596848\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 179.105362\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 196.009354\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 179.727295\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 183.429153\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 182.986740\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 175.261795\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 180.290482\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 182.760056\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 176.496841\n",
      "====> Epoch: 2 Average loss: 181.6628\n",
      "====> Test set loss: 178.1468\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 172.890213\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 186.257568\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 179.852570\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 176.142670\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 193.758972\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 183.140717\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 171.299194\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 187.273056\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 169.798111\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 179.162277\n",
      "====> Epoch: 3 Average loss: 178.4807\n",
      "====> Test set loss: 173.3773\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 173.776794\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 179.544739\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 180.748734\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 185.609329\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 177.484772\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 168.602539\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 176.235352\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 156.235641\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 172.838150\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 163.222824\n",
      "====> Epoch: 4 Average loss: 174.0935\n",
      "====> Test set loss: 176.9751\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 191.665482\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 166.940018\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 167.544037\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 169.406647\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 177.887741\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 163.358856\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 174.318573\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 179.249054\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 182.804672\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 172.128235\n",
      "====> Epoch: 5 Average loss: 174.3934\n",
      "====> Test set loss: 170.0119\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.602173\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 213.462387\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 192.647903\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 180.795822\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 181.290054\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 179.922073\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 174.683487\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 165.049118\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 170.335831\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 163.095108\n",
      "====> Epoch: 1 Average loss: 185.1886\n",
      "====> Test set loss: 163.1794\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 160.562149\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 157.179901\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 160.703903\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 159.595749\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 165.121017\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 160.589188\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 157.254150\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 159.159561\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 156.820908\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 168.728882\n",
      "====> Epoch: 2 Average loss: 162.4717\n",
      "====> Test set loss: 160.7069\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 152.914795\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 165.436874\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 156.644791\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 162.792908\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 160.619797\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 154.230057\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 160.543564\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 177.548019\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 168.882004\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 149.494415\n",
      "====> Epoch: 3 Average loss: 163.7077\n",
      "====> Test set loss: 164.5434\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 169.519684\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 153.754852\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 176.481232\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 164.448196\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 159.119843\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 157.758392\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 151.269058\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 175.193588\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 157.107437\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 169.830597\n",
      "====> Epoch: 4 Average loss: 161.8108\n",
      "====> Test set loss: 160.9156\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 166.280075\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 162.489853\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 151.912415\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 156.206787\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 152.029816\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 151.068024\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 164.372635\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 147.353577\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 157.170364\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 162.345459\n",
      "====> Epoch: 5 Average loss: 158.9802\n",
      "====> Test set loss: 156.3176\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 551.010681\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 202.327026\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 181.784607\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 189.311279\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 167.854218\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 172.737411\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 163.476578\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 156.561127\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 162.113907\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 156.378036\n",
      "====> Epoch: 1 Average loss: 176.2248\n",
      "====> Test set loss: 159.6952\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 171.202576\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 147.780136\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 158.267624\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 149.709106\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 157.851822\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 156.423172\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 151.864624\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 150.865387\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 148.434647\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 139.132004\n",
      "====> Epoch: 2 Average loss: 152.3656\n",
      "====> Test set loss: 147.5828\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 155.899673\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 146.451233\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 155.391388\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 142.072693\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 149.566345\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 146.729630\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 145.725586\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 143.799103\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 145.502899\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 154.570267\n",
      "====> Epoch: 3 Average loss: 147.6012\n",
      "====> Test set loss: 145.5919\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 149.366135\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 153.058517\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 139.676315\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 142.445114\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 135.805298\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 145.174866\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 150.406555\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 145.837051\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 148.106491\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 146.635941\n",
      "====> Epoch: 4 Average loss: 148.3921\n",
      "====> Test set loss: 148.1529\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 156.899277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 145.841507\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 150.234604\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 144.747711\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 149.950638\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 154.203384\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 140.329529\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 148.329681\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 155.659760\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 162.580917\n",
      "====> Epoch: 5 Average loss: 153.2566\n",
      "====> Test set loss: 153.3160\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 548.480225\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 205.167847\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 164.541702\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 152.453781\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 144.748306\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 142.930588\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 139.976364\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 130.243927\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 127.974213\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 127.581451\n",
      "====> Epoch: 1 Average loss: 155.5499\n",
      "====> Test set loss: 128.9648\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 133.262299\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 125.961586\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 134.504044\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 127.432167\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 127.885880\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 120.235176\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 117.058647\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 124.404953\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 120.343292\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 121.424683\n",
      "====> Epoch: 2 Average loss: 125.1471\n",
      "====> Test set loss: 120.6995\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 116.787315\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 126.344513\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 126.521469\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 121.791443\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 121.131027\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 120.792328\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 119.876190\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 118.647919\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 117.518509\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 121.571304\n",
      "====> Epoch: 3 Average loss: 119.9198\n",
      "====> Test set loss: 118.1201\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 122.124474\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 110.297562\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 114.142418\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 114.477066\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 120.424278\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 119.595779\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 116.856674\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 116.001251\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 113.971176\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 112.457932\n",
      "====> Epoch: 4 Average loss: 117.4256\n",
      "====> Test set loss: 116.2352\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 119.643356\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 110.962738\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 119.056961\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 115.707260\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 115.131195\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 118.382355\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 113.576538\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 116.568985\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 118.838867\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 119.149872\n",
      "====> Epoch: 5 Average loss: 116.1580\n",
      "====> Test set loss: 114.5435\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_all = [10, 25, 50, 250]\n",
    "\n",
    "for k in k_all:\n",
    "    model = VAE_Gumbel_MNIST(784, 400, 20, k = k).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas = (b1,b2))\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train(model, optimizer, epoch)\n",
    "        #with torch.no_grad():\n",
    "        #    diag_model.diag.data[torch.abs(diag_model.diag) < 0.05] = 0\n",
    "        test(model, epoch, k)\n",
    "        with torch.no_grad():\n",
    "            sample = torch.randn(64, 20).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       '../data/gumbel_on_mnist/k_{}_sample_'.format(k) + str(epoch) + '.png')\n",
    "    with torch.no_grad():\n",
    "        w = model.weight_creator(data[0, :].view(-1, 784))\n",
    "        subset_indices = sample_subset(w, k=k, t=0.1).cpu()\n",
    "    plt.imshow(subset_indices.reshape((28, 28)) > 0.01)\n",
    "    plt.imsave('../data/gumbel_on_mnist/k_{}_example_featureselected.png'.format(k), \n",
    "               subset_indices.reshape((28, 28)) > 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nyu] *",
   "language": "python",
   "name": "conda-env-nyu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
